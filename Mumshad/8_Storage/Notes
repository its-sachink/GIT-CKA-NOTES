############# Notes ###############



	- Storage :

		- Intro to docker storage : Storage in docker

			- Filesystem of docker on local filesystem(host)

				- /var/lib/docker
					- aufs
					- containers
					- image
					- volumes

			- Dockers are build using the "layered architecture". Each layer is an instruction/command specified in the Dockerfile.

			Layer 1. Base Ubuntu Layer
			Layer 2. Changes in apt packages
			Layer 3. Changes in pip packages
			Layer 4. Source code
			Layer 5. update Entrypoint with "flask" command

			- This way the layers can be reused to build other containers with slight modifications.

				- Image layer : It is a "Read Only" layer
				- Container layer : It is a "Read Write" layer, the file from the Image layer are modified as per requirement while creating the container

				- When the container is deleted then all the data stored in the container layer is also deleted.
				- So if we want the data to persist we could add a persist volume to the container.

					# docker volume create data_volume
					# docker run -v data_volume:/var/lib/mysql  mysql

						- The data_volume is created on the host at following location, 
							/var/lib/docker/volumes/data_volume



				- What if we didn't create a "docker volume" command, in that case docker will automatically create a volume in this case.

					# docker run -v data_volume2:/var/lib/mysql  mysql
						- This is called volume mounting. i.e only from location /var/lib/docker.


				- What if we have our volumes on some external storage like NFS.

					# docker run -v /data/mysql:/var/lib/mysql  mysql
						- This is called bind mounting. i.e from any location.


				- "-v" is old style, "-mount" is new style as below

					# docker run \
					--mount type=bind,source=/data/mysql,target=/var/lib/mysql  mysql


				- So who is respondible for maintaing a layered architecture, it is storage drivers. Some of the common storage drivers are,
					- AUFS
					- ZFS
					- BTRFS
					- Device Mapper
					- Overlay
					- Overlay2


		- Volume driver plugins in Docker
			- Volume driver plugins manages the Docker volume, the default volume plugin is "local".

			- Different volume driver plugins are as follows.
				- Local
				- Convoy
				- gce-docker
				- GlusterFS
				- NetApp
				- RexRay - used for AWS EBS volumes
				- Portworx
				- VMware vSphere Storage


			- If we want to specify the plugins explicitly we can do that as below.

				# docker run -it \
					--name mysql \
					--volume-driver rexray,ebs \
					--mount src=ebs-vol,target=/var/lib/mysql \
					mysql


		- Container Storage Interface

			- Container runtime interface(CRI) : is a standard which defines how kubernetes would interact with other container runtime like docker, rocket, cri-o

			- Container networking interface(CNI) : is a standard to develop the plugin for any network vendor to interact with the kubernetes like flannel, etc.

			- Container storage interface(CSI) : You can write your own driver for your storage to work with the kubernetes. ex : islion, hpe, hitachi, amazon ebs, dell emc, etc.

				- CSI is a universal standard for kubernetes, mesos, cloudfoundary. It is used to define the operations like create, delete volumes etc.


	- Volumes :

		- Containers are transient which means, they are called upon to process data and destroyed when finished.
		- To persist data we attach volumes to the containers.

		- Pod are also transient in nature and we can attach volumes to Pods.


		----------------------------- Volumes and Mounts ----------------------------------
		apiVersion: v1
		kind: Pod
		metadata:
		  name: random-number-generator
		spec:
		  containers:
		  - image: alpine
		    name: alpine
		    command: ["/bin/sh","-c"]
		    args: ["shuf -i 0-100 -n 1 >> /opt/number.out"]
		    volumeMounts:
		    - mountPath: /opt
		      name: data-volume

		  volumes:
		  - name: data-volume
		    hostPath:
		      path: /data              <<<< This is the host path
 			  type: Directory
		-----------------------------------------------------------------------------------

		- mounting on "/data" for kubernetes which is having multiple hosts is not feasible, hence we can use the cloud volume storage like aws-volumes, glusterfs, netapp, etc.


		-------------------------------------------
		  volumes:
		  - name: data-volume
		    awsElasticBlockStore:
		    	volumeID: <volume-id>
		    	fsType: ext4
		-------------------------------------------


	- Persistent Volumes :

		- Instead of managing the storage at the Pod level like in above we would like to manage centrally in kubernetes. We would like to configure it in a way as a pool of storage and then we can carve out as and when required.

		- Persistent Volume : Is a clusterwide pool of storage volumes to be used by application deployed on a cluster.
			- The users are now select storage from this pools using persistent volumes claims.

			PVC : a volume chaked out of PV.


		----------------------------- pv-definition.yaml ----------------------------------
		apiVersion: v1
		kind: PersistentVolume
		metadata:
		  name: pv-vol1
		spec:
			accessModes:
				- ReadWriteOnce
			capacity:
				storage: 1Gi

			awsElasticBlockStore:
				volumeID: <volume-id>
				fsType: ext4
		-----------------------------------------------------------------------------------

			# kubectl create -f pv-definition.yaml

			# kubectl get persistentvolume


	- Persistent Volumes Claims :

		- To claim the volumes configured above. To make the storage available to a node.

		- PV and PVS are 2 seperate objects in a kubernetes namespace.

		- An kubernetes Administrator creates a set of PV and a user create PVC to use the storage.

		- Once the persistent volumes claims are created it binds those claims to the persistent volumes created on the kubernetes.
		- Every PVC is bound to only a single PV.

		- PVC : is at Pod/user request side
		  PV : is at a kubernetes side

		- Kubernetes tries to bind a PVC to a PV which has a sufficient capacity and any other properties that matches 


		----------------------------- pvc-definition.yaml ----------------------------------
		apiVersion: v1
		kind: PersistentVolumeClaim
		metadata:
		  name: myclaim
		spec:
			accessModes:
				- ReadWriteOnce

			resources:
				requests:
					storage: 500Mi
		-----------------------------------------------------------------------------------

			# kubectl create -f pvc-definition.yaml
			# kubectl get persistentvolumeclaim

			# kubectl delete persistentvolumeclaim myclaim

			persistentVolumeReclaimPolicy: Retain
			persistentVolumeReclaimPolicy: Delete
			persistentVolumeReclaimPolicy: Recycle		<<<< the data is scrubed



	- Using PVCs in PODs :

		- Once you create a PVC use it in a POD definition file by specifying the PVC Claim name under persistentVolumeClaim section in the volumes section like this:

		----------------------------- pod-definition.yaml ----------------------------------
		apiVersion: v1
		kind: Pod
		metadata:
		  name: mypod
		spec:
		  containers:
		    - name: myfrontend
		      image: nginx
		      volumeMounts:
		      - mountPath: "/var/www/html"
		        name: mypd
		  volumes:
		    - name: mypd
		      persistentVolumeClaim:
		        claimName: myclaim
		------------------------------------------------------------------------------------

			- The same is true for ReplicaSets or Deployments. Add this to the pod template section of a Deployment on ReplicaSet.

			- URL: https://kubernetes.io/docs/concepts/storage/persistent-volumes/#claims-as-volumes