############### Commands ##############

- Practice OS upgrades :

	- On which nodes are the applications hosted on?

		# k get pods -o wide

		NAME                    READY   STATUS    RESTARTS   AGE   IP          NODE     NOMINATED NODE   READINESS GATES
		blue-68df9fb9d9-f6spd   1/1     Running   0          66s   10.46.0.1   node02   <none>           <none>blue-68df9fb9d9-k4hkd   1/1     Running   0          66s   10.40.0.2   node01   <none>           <none>
		blue-68df9fb9d9-xln5t   1/1     Running   0          66s   10.88.0.3   node03   <none>           <none>
		red-5dc59c9654-m8fdc    1/1     Running   0          66s   10.40.0.1   node01   <none>           <none>
		red-5dc59c9654-mmtqw    1/1     Running   0          66s   10.88.0.2   node03   <none>           <none>


	- Empty the node "node01" of all applications and mark it unschedulable.
	
		#** kubectl drain node01 --ignore-daemonsets

		# kubectl drain node03 --ignore-daemonsets --force

	- Configure the node "node01" to be schedulable again.

		# k uncordon node01


	- Why are there no pods placed on the master node?

		# k describe node master


		# k cordon node02



- Practice Cluster Upgrade Process :

	- Current version of the cluster ?

		# k version --short


	- How many nodes can host workloads in this cluster?

		# k get pod -o wide


	- What is the latest stable version available for upgrade?

		# kubeadm upgrade plan


	- Drain the master node of workloads and mark it UnSchedulable

		# kubectl drain master --ignore-daemonsets


----------------------------
	- Upgrade the master components to exact version v1.18.0
			- Note: While upgrading kubelet, if you hit dependency issue while running the apt-get upgrade kubelet command, use the apt install kubelet=1.17.0-00 command instead


		- google : Upgrade kubeadm cluster
---------------------------------------------------------------

# k drain/cordon/uncordon master/node01
---------- master ------------
kubeadm upgrade plan                  <<<< Always check the cluster version and kubeadm version here
**apt-get install kubeadm=1.18.0-00

**kubeadm upgrade plan v1.18.0
**kubeadm upgrade apply v1.18.0

apt-mark hold kubeadm

**apt-get install kubelet=1.18.0-00
**systemctl restart kubelet

---------- worker ------------
**apt-get install kubeadm=1.18.0-00
**kubeadm upgrade node

**apt-get install kubelet=1.18.0-00
**systemctl restart kubelet
------------------------------



---------------------------------------------------------------
- Practise Backup and Restore method :


	- To know the version of ETCD cluster

		# kubectl logs etcd-master -n kube-system | head

		# kubectl describe pod etcd-master -n kube-system


	- At what address do you reach the ETCD cluster from your master node? What is the address of etcd-cluster

		#** kubectl describe pod etcd-master -n kube-system | grep -i listen
      --listen-client-urls=https://127.0.0.1:2379,https://172.17.0.48:2379		<<<<< Where ETCD listen
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.17.0.48:2380


    - Where is the ETCD server certificate file located?

    	# k describe pod etcd-master -n kube-system | grep -i server.crt
      --cert-file=/etc/kubernetes/pki/etcd/server.crt


    - Where is the ETCD CA Certificate file located?

    	# k describe pod etcd-master -n kube-system | grep -i "ca.crt"
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt        <<<<<<


    - The master nodes in our cluster are planned for a regular maintenance reboot tonight. While we do not anticipate anything to go wrong, we are required to take the necessary backups. Take a snapshot of the ETCD database using the built-in snapshot functionality.
     	Store the backup file at location /tmp/snapshot-pre-boot.db

     	# export ETCDCTL_API=3

     	# k describe pod etcd-master -n kube-system | egrep -i "listen|ca.crt|server.crt|server.key"
		      --cert-file=/etc/kubernetes/pki/etcd/server.crt
		      --key-file=/etc/kubernetes/pki/etcd/server.key
		      --listen-client-urls=https://127.0.0.1:2379,https://172.17.0.48:2379
		      --listen-metrics-urls=http://127.0.0.1:2381
		      --listen-peer-urls=https://172.17.0.48:2380
		      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
		      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt


      	# etcdctl \
      	 --cacert=/etc/kubernetes/pki/etcd/ca.crt \
		 --cert=/etc/kubernetes/pki/etcd/server.crt \
		 --key=/etc/kubernetes/pki/etcd/server.key \
		 --endpoint=https://127.0.0.1:2379 \
		 snapshot save /tmp/snapshot-pre-boot.db

				Snapshot saved at /tmp/snapshot-pre-boot.db


    - Restore the original state of the cluster using the backup file.


	#  etcdctl --endpoints=https://[127.0.0.1]:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt \
     --name=master \
     --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key \
     --data-dir /var/lib/etcd-from-backup \
     --initial-cluster=master=https://127.0.0.1:2380 \
     --initial-cluster-token etcd-cluster-1 \
     --initial-advertise-peer-urls=https://127.0.0.1:2380 \
     snapshot restore /tmp/snapshot-pre-boot.db


    - Then you need to edit the '/etc/kubernetes/manifests/etcd.yaml' file accordingly
    	--data-dir=/var/lib/etcd-from-backup
    	--initial-cluster-token=etcd-cluster-1

    	- also the hostPath mounts

    ------------------------
     volumeMounts:
    - mountPath: /var/lib/etcd-from-backup
      name: etcd-data
    - mountPath: /etc/kubernetes/pki/etcd
      name: etcd-certs
  hostNetwork: true
  priorityClassName: system-cluster-critical
  volumes:
  - hostPath:
      path: /var/lib/etcd-from-backup
      type: DirectoryOrCreate
    name: etcd-data
  - hostPath:
      path: /etc/kubernetes/pki/etcd
      type: DirectoryOrCreate
    name: etcd-certs
    ------------------------

    https://github.com/mmumshad/kubernetes-the-hard-way/blob/master/practice-questions-answers/cluster-maintenance/backup-etcd/etcd-backup-and-restore.md