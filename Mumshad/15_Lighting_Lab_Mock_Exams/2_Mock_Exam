########### Mock Exam 2 ##########


1) Take a backup of the etcd cluster and save it to /tmp/etcd-backup.db


------------------------------------------------
export ETCDCTL_API=3
etcdctl  \
	--cacert=/etc/kubernetes/pki/etcd/ca.crt \
	--cert=/etc/kubernetes/pki/etcd/server.crt \
	--key=/etc/kubernetes/pki/etcd/server.key  \
	--endpoints=https://127.0.0.1:2379 member list

export ETCDCTL_API=3
etcdctl \
	--cacert=/etc/kubernetes/pki/etcd/ca.crt \
	--cert=/etc/kubernetes/pki/etcd/server.crt \
	--key=/etc/kubernetes/pki/etcd/server.key  \
	--endpoints=https://127.0.0.1:2379 snapshot save /opt/etcd-backup.db

export ETCDCTL_API=3
etcdctl \
	--cacert=/etc/kubernetes/pki/etcd/ca.crt \
	--cert=/etc/kubernetes/pki/etcd/server.crt \
	--key=/etc/kubernetes/pki/etcd/server.key  \
	--endpoints=https://127.0.0.1:2379 snapshot status /opt/etcd-backup.db -w table

------------------------------------------------


2) Create a Pod called "elephant" with image: "redis" with CPU request set to 1 CPU and Memory request as "200MiB".


# k create elephant --image=redis $do > elephant.yaml

-----
metadata:
  name: elephant
spec:
  containers:
  - image: redis
    name: elephant
    resources:
      requests:
        cpu: "1"
        memory: "200Mi"
-----      


2) Create a Pod called "redis-storage" with image: "redis:alpine" with a Volume of type "emptyDir" that lasts for the life of the Pod. Specs on the right.




3) Create a new pod called super-user-pod with image busybox:1.28. Allow the pod to be able to set system_time.
The container should sleep for 4800 seconds.


-------
apiVersion: v1
kind: Pod
metadata:
  name: super-user-pod
spec:
  containers:
  - name: super-user-pod
    image: busybox:1.28
    securityContext:
      capabilities:
        add: ["SYS_TIME"]
    command: ["sleep", "4800"]
-------


4) A pod definition file is created at /root/use-pv.yaml. Make use of this manifest file and mount the persistent volume called pv-1. Ensure the pod is running and the PV is bound.

mountPath: /data 
persistentVolumeClaim Name: my-pvc.

-----
# k get pv

------------------
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: myclaim
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Mi
  storageClassName: slow
------------------

- Enter the details of volume in the yaml file



5) Create a new deployment called nginx-deploy, with image nginx:1.16 and 1 replica. Record the version. Next upgrade the deployment to version 1.17 using rolling update. Make sure that the version upgrade is recorded in the resource annotation.

---------
# k create deployment nginx-deploy --image=nginx:1.16 --replicas=1 --record

# k rollout history deployment nginx-deploy 

# k set image deployment/nginx-deploy nginx-deploy=nginx:1.17 --record

# k rollout history deployment nginx-deploy
---------



6) Create a new user called john. Grant him access to the cluster. John should have permission to create, list, get, update and delete pods in the development namespace . 
The private key exists in the location: /root/john.key and csr at /root/john.csr

CSR: john-developer
Status: approved
Role Name: developer
namespace: development
resources: Pod
Access: User 'john' has appropriate permissions


------------
- create csr

cat <<EOF | kubectl apply -f -
apiVersion: certificates.k8s.io/v1beta1
kind: CertificateSigningRequest
metadata:
  name: john-developer
spec:
  request: $(cat john.csr | base64 | tr -d '\n')
  usages:
  - digital signature
  - key encipherment
  - server auth
EOF
------------
# k get csr
# k certificate approve john-developer

- Create a role "developer" with appropriate role

# k create role developer --resource=pods --verb=create,list,get,update,delete  --namespace=development
# k describe role developer

# k create rolebinding developer-role-binding --role=developer --user=john --namespace=development
# k describe rolebinding developer-role-binding

# k auth can-i update pods --namespace=development --as=john
# k auth can-i list pods --namespace=development --as=john



7) Create an nginx pod called nginx-resolver using image nginx, expose it internally with a service called nginx-resolver-service. Test that you are able to look up the service and pod names from within the cluster. Use the image: busybox:1.28 for dns lookup. Record results in /root/nginx.svc and /root/nginx.pod

---------
# k create nginx-resolver --image=nginx
# k expose pod nginx-resolver --name=nginx-resolver-service --port=80 --target-port=80

# k run test-nslookup --image=busybox:1.28 --rm --it -- nslookup nginx-resolver-service > /root/nginx.svc
# k run test-nslookup --image=busybox:1.28 --rm --it -- nslookup 10-32-0-5.default.pod  > /root/nginx.pod




8) Create a static pod on node01 called nginx-critical with image nginx. Create this pod on node01 and make sure that it is recreated/restarted automatically in case of a failure.

Use /etc/kubernetes/manifests as the Static Pod path for example.

# ssh node01
# systectl status kubelet 				<<<< check the path of kubernetes
# cd /etc/kubernetes/manifests  		<<<< if directory does not exist then create it

-- go to master node
# k run nginx-critical --image=nginx $do > nginx-critical-node01.yaml
# scp nginx-critical-node01.yaml node01:/etc/kubernetes/manifests