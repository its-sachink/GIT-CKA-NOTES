######################## Commands ########################



1) Upgrade the current version of kubernetes from 1.17 to 1.18.0 exactly using the kubeadm utility. Make sure that the upgrade is carried out one node at a time starting with the master node. To minimize downtime, the deployment gold-nginx should be rescheduled on an alternate node before upgrading each node.

	Upgrade master node first. Drain node01 before upgrading it. Pods for gold-nginx should run on the master node subsequently.

----------------------------------------
Master Node:kubectl drain node master --ignore-daemonsets

apt-get install kubeadm=1.18.0-00
kubeadm  upgrade plan v1.18.0
kubeadm  upgrade apply v1.18.0

apt-get install kubelet=1.18.0-00
kubectl uncordon master
kubectl drain node01 --ignore-daemonsets


Node01:
apt-get install kubeadm=1.18.0-00
kubeadm upgrade node --kubelet-version v1.18.0

apt-get install kubelet=1.18.0-00


Back on Master:
kubectl uncordon node01
kubectl get pods -o wide | grep gold (make sure this is scheduled on master node)
----------------------------------------




2) Print the names of all deployments in the admin2406 namespace in the following format:

DEPLOYMENT 			CONTAINER_IMAGE 		READY_REPLICAS 		NAMESPACE
deploy0 			nginx:alpine 			1 				admin2406

The data should be sorted by the increasing order of the deployment name.

Write the result to the file /opt/admin2406_data.


----------------------------------------
kubectl -n admin2406 get deployment 
	-o custom-columns=DEPLOYMENT:.metadata.name,CONTAINER_IMAGE:.spec.template.spec.containers[].image,READY_REPLICAS:.status.readyReplicas,NAMESPACE:.metadata.namespace 
	--sort-by=.metadata.name > /opt/admin2406_data
----------------------------------------

----------- output ------------------------
DEPLOYMENT CONTAINER_IMAGE READY_REPLICAS NAMESPACE
deploy1 	nginx 			1 			admin2406
deploy2 	nginx:alpine 	1 			admin2406
deploy3 	nginx:1.16 		<none> 		admin2406
deploy4 	nginx:1.17 		1 			admin2406
deploy5 	nginx:latest 	1 			admin2406
-------------------------------------------




3) A kubeconfig file called admin.kubeconfig has been created in /root. There is something wrong with the configuration. Troubleshoot and fix it.


----------------------------------------
Make sure the port for the kube-apiserver is correct.

Change port from 2379 to 6443.


Run:
kubectl cluster-info --kubeconfig /root/admin.kubeconfig
----------------------------------------



4) Create a new deployment called nginx-deploy, with image nginx:1.16 and 1 replica. 
	Next upgrade the deployment to version 1.17 using rolling update. Make sure that the version upgrade is recorded in the resource annotation.

------------------------------------------------
# kubectl create deployment  nginx-deploy --image=nginx:1.16 --record

# k rollout history deployment nginx-deploy 

# kubectl set image deployment/nginx-deploy nginx-deploy=nginx:1.17 --record

# k rollout history deployment nginx-deploy 
------------------------------------------------

- search "--record"


5) A new deployment called alpha-mysql has been deployed in the alpha namespace. However, the pods are not running. Troubleshoot and fix the issue.

The deployment should make use of the persistent volume alpha-pv to be mounted at /var/lib/mysql 

and should use the environment variable MYSQL_ALLOW_EMPTY_PASSWORD=1 to make use of an empty root password.


------------------------------------------------
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  annotations:
  name: mysql-alpha-pvc
  namespace: alpha
spec:
  accessModes:  
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi  
  storageClassName: slow          <<<< **** important
------------------------------------------------




6) Take the backup of ETCD at the location /opt/etcd-backup.db on the master node

------------------------------------------------
export ETCDCTL_API=3
etcdctl  \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key  \
  --endpoints=https://127.0.0.1:2379 member list

export ETCDCTL_API=3
etcdctl \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key  \
  --endpoints=https://127.0.0.1:2379 snapshot save /opt/etcd-backup.db

export ETCDCTL_API=3
etcdctl \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key  \
  --endpoints=https://127.0.0.1:2379 snapshot status /opt/etcd-backup.db -w table

------------------------------------------------


7) Create a pod called secret-1401 in the admin1401 namespace using the busybox image. The container within the pod should be called secret-admin and should sleep for 4800 seconds.

The container should mount a read-only secret volume called secret-volume at the path /etc/secret-volume. The secret being mounted has already been created for you and is called dotfile-secret.

----------------------------------------------------- **********************
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: secret-1401
  name: secret-1401
  namespace: admin1401
spec:
  volumes:
  - name: secret-volume
    secret:
      secretName: dotfile-secret
  containers:
  - command:
    - sleep
    args:
    - "4800"
    image: busybox
    name: secret-admin
    volumeMounts:
    - name: secret-volume
      readOnly: true
      mountPath: "/etc/secret-volume"
-----------------------------------------------------



$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$ Points to note down $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

--sort-by

storageClassName: slow   <<<< ony if pv is slow


snapshot status /opt/etcd-backup.db -w table

----- pass seperately
  containers:
  - command:        <<<<
    - sleep
    args:           <<<<
    - "4800"
    image: busybox
    name: secret-admin
-------


  containers:
  - command: ["sleep","4800"]
    image: busybox
    name: secret-admin