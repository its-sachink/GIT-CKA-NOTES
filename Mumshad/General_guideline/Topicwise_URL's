###### Topicwise URL ######


2) --------------------------------------------- CoreConcepts ------------------------------------------------------

- Need to check regarding scale for deployments

3) --------------------------------------------- Scheduling : ------------------------------------------------------


# kp --selector env=prod,bu=finance,tier=frontend

# k taint nodes node01 spray=mortein:NoSchedule

- Tolerations : https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/

- Untaint : k taint nodes master node-role.kubernetes.io/master:NoSchedule-


# k label nodes node01 color=blue

# k describe node node01 | grep -i label -A6


- Affinity : https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/

	- While adding affinity make sure to add after the containers sections always


- Taints to node == Tolerations is set on pod
- Labels on node == Affinity is set on pod

# k run static-busybox --image=busybox $do --command -- sleep 1000 > /etc/kubernetes/manifests/static-busybox.yaml

	first $do and then --command


- Multiple Scheduler :

	Search for "Multiple Schedulers".

		- Change name and add "- --scheduler-name=my-scheduler" in custom scheduler.

		- Specify in the pod

-----------------
	metadata:
	  name: nginx
	spec:
	  containers:
	  -  image: nginx
	     name: nginx
	  schedulerName: my-scheduler 		<<< Need to Add this
-----------------




5) ------------------------------------------ ApplLifeCycleMgmt ---------------------------------------------------


---------------
	  strategy:
	    rollingUpdate:                      <<<< Need to delete below 3 lines and change "type"
	      maxSurge: 25%
	      maxUnavailable: 25%
	    type: Recreate
---------------


---------------
	spec:
	  containers:
	  - image: ubuntu
	    name: ubuntu-sleeper-2    
	    command: ["sleep","5000"]
---------------


---------------
	spec:
	  containers:
	  - name: ubuntu
	    image: ubuntu
	    command:
	      - "sleep"
	      - "1200"			<<<< Check that this should be in doubt quotes
---------------


---------------
- container:
	ENTRYPOINT ["python", "app.py"]

	CMD ["--color", "red"]

- pod:
	  containers:
	  - name: simple-webapp
	    image: kodekloud/webapp-color
	    command: ["python", "app.py"]
	    args: ["--color", "pink"]
---------------


# k create cm webapp-config-map --dry-run -o yaml \ 
			--from-literal=APP_COLOR=darkblue > webapp-config-map.yaml
---------------
	    containers:
	    - envFrom:
	        - configMapRef:
	            name: webapp-configmap
	      image: kodekloud/webapp-color
---------------



# k create secret generic db-secret \
	--from-literal=DB_Host=sql01 \
	--from-literal=DB_User=root \
	--from-literal=DB_Password=password123 \
	--dry-run -o yaml > db-secret-1.yaml			<<<< The base64 is automaticall encoded
---------------
		  containers:
		  - image: kodekloud/simple-webapp-mysql
		    name: webapp
		    envFrom:
		      - secretRef:					<<<<
		          name: app-secret
---------------



---------------
		spec:
		  containers:
		  ....
		  ....
		  initContainers:
		  - name : redbusy
		    image: busybox
		    command: ["sleep","20"]
		  dnsPolicy: ClusterFirst
		  enableServiceLinks: true
---------------





6) ------------------------------------------ ClusterMaintainance ---------------------------------------------------


# kubectl drain node01 --ignore-daemonsets

# kubectl cordon/uncordon master

---------- master ------------
kubeadm upgrade plan
apt-get install kubeadm=1.18.0-00

kubeadm upgrade plan v1.18.0
kubeadm upgrade apply v1.18.0

apt-mark hold kubeadm

apt-get install kubelet=1.18.0-00
systemctl restart kubelet

---------- worker ------------
apt-get install kubeadm=1.18.0-00
kubeadm upgrade node

apt-get install kubelet=1.18.0-00
systemctl restart kubelet
------------------------------


# ETCDCTL_API=3 etcdctl snapshot save --help
# ETCDCTL_API=3 etcdctl snapshot restore -help

# ETCDCTL_API=3 etcdctl snapshot save -h
# ETCDCTL_API=3 etcdctl snapshot restore -h


-------------------------------

-**** kubectl describe pod etcd-master -n kube-system | egrep -i "crt|listen|key"
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.17.0.71:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.17.0.71:2380
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
	  
export ECTDCTL_API=3

# ETCDCTL_API=3 etcdctl snapshot save -h
# ETCDCTL_API=3 etcdctl snapshot restore -h


ETCDCTL_API=3 etcdctl snapshot save --help

ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \
	--cacert=/etc/kubernetes/pki/etcd/ca.crt \
	--cert=/etc/kubernetes/pki/etcd/server.crt \
	--key=/etc/kubernetes/pki/etcd/server.key 
	snapshot save /tmp/snapshot-pre-boot.db



ETCDCTL_API=3 etcdctl snapshot restore --help

ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt \
     --name=master \
     --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key \
     --data-dir /var/lib/etcd-from-backup \
     --initial-cluster=master=https://127.0.0.1:2380 \
     --initial-cluster-token=etcd-cluster-1 \
     --initial-advertise-peer-urls=https://127.0.0.1:2380 \
     snapshot restore /tmp/snapshot-pre-boot.db
	 
	 
	-**** Edit the following lines in the static pod of the cluster.
	
		--data-dir=/var/lib/etcd-from-backup
		
		--initial-cluster-token=etcd-cluster-1 			<<<< Add this

		-- check "https" is there

		-- endpoints are in bracket[], do not remove bracket

		-- name "master" is same as "--initial-cluster"

	-**** Then you need to edit the '/etc/kubernetes/manifests/etcd.yaml' file accordingly
    	--data-dir=/var/lib/etcd-from-backup
    	--initial-cluster-token=etcd-cluster-1

    	- also the hostPath mounts
11) --------------------------------------- Install K8 kubeadm way -----------------------------------------------

	1. make IP table changes on both the nodes master and node01
	2. Install the kubeadm package on master and node01
	3. Master - Bootstrap master "kubeadm init" -- and run that mkdir command
	4. Node01 - join node01 using "kubeadm join command"
	5. Then install Weave only on master



7) --------------------------------------- Security -----------------------------------------------

