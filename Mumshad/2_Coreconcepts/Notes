fN########## Notes ############

Master Node : Manage, Plan, Schedule, Monitor Nodes. Master node does all of this using Control Plane components.


- (Master Node) Controller Plane components :
	- ETCD Cluster : Maintains information of different containers, which are running on which nodes. Stores Key-Value format.
	- Kube-Scheduler : Identify the right node to place a container on based on resources requirement, affinity rule, taints toleration.
	- Kube-Controller : Contains following things.
		- Node controller : It manages the nodes, like to add new, delete the non-working nodes.
		- Replication controller : Desired numober of containers are running in a replication group.
		- Controller manager :

	- Kube-API Server : It exposes Kube API to manage operations, all the internal and external communication happens through Kube API.


- Worker Nodes components :
	- Kubelet : Run on each nodes, listen for instructions from Kube API server.
	- Kube Proxy : Ensures necessary rules are in place on nodes, to allow containers from other nodes to communicatre with each other.



--------------------- ETCD Service ---------------------

- ETCD is a distributed reliable key-value store that is Simple, Secure and Fast.

	- Download binary and extract it.

	# ./etcd
	# ./etcdctl set key1 value1
	# ./etcdctl get key1



	- ETCD Cluster stores the cluster information such as.
		Nodes
		Pods
		Configs
		Secrets
		Accounts
		Roles
		Bindings

	- etcd.service
		- port on which the etcd listens
			--advertise-client-urls https://${INTERNAL_IP}:2379

		- etcd nodes knows about each other using following configuration in "etcd.service" file, it specifies the different instances of the service

			--initial-cluster controller-0=https://{CONTROLLER0_IP}:2380,controller-1=https://{CONTROLLER1_IP}:2380


	---------- ETCD Optional -----------------
	ETCDCTL is the CLI tool used to interact with ETCD.

	ETCDCTL can interact with ETCD Server using 2 API versions - Version 2 and Version 3.  By default its set to use Version 2.

	ETCDCTL version 2 supports the following commands:

		etcdctl backup
		etcdctl cluster-health
		etcdctl mk
		etcdctl mkdir
		etcdctl set

	Whereas the commands are different in version 3

		etcdctl snapshot save 
		etcdctl endpoint health
		etcdctl get
		etcdctl put

	# export ETCDCTL_API=3	, default is 2 if not set.


	- Path of certificate files

		--cacert /etc/kubernetes/pki/etcd/ca.crt     
		--cert /etc/kubernetes/pki/etcd/server.crt     
		--key /etc/kubernetes/pki/etcd/server.key


	- So the command to list down the keys from the ETCDL service would be following

		# kubectl exec etcd-master -n kube-system -- sh -c "ETCDCTL_API=3 etcdctl get / --prefix --keys-only --limit=10 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt  --key /etc/kubernetes/pki/etcd/server.key" 


	---------- ETCD Optional ENDS -----------------


- KubeAPI server

	- KubeAPI is the only server that interacts with ETCD directly.

	- How to view the kubeapi server options like what is the address of etcd service etc.

		-**** If the cluster is setup using kubeadm then

			# /etc/kubernetes/manifests/kube-apiserver.yaml

		-**** Nob kubeadmin setup

			# /etc/systemd/system/kube-apiserver.service

		-**** Also we can run following on master node

			# ps aux | grep -i kube-apiserver



- Kubernetes Controller manager

	-**** Controller : It continuously monitors the state of various components in the kubernetes system and worked towrds bringing the whole system to the desired state.

		- Node controller : monitors various Node every 5 seconds, waits for 40s to mark it as down
		- Replicaion controller : managed the pods replication

	-**** How to view the kubecontroller options
	
		# cat /etc/kubernetes/manifests/kube-controller-manager.yaml 	- kubeadm setup

		# cat /etc/systemd/system/kube-controller-manager.Service 		- non-kubeadm setup

		# ps aux | grep -i kube-controller-manager 						- by checking the options of a running process


- Kube Scheduler

	- Scheduler decides which pod to go on which node, it doesn't actually places the pod on the node. It is done by kubelet.

	- To view the kube scheduler server options

		# cat /etc/kubernetes/manifests/kube-scheduler.yaml				- if kubeadm setup

		# ps aux | grep -i Kube-Scheduler 								- by checking the options of a running process


- Kubelet

	- Responsible for managing the nodes for following things
		- Take all action to make the nodes part of the cluster
		- Sole point of contact for Nodes
		- Send back regular report of the status of the Nodes
		- It registers the node with the kubernetes cluster
		- It request docker to pull the image and run the instance and monitor the status on timely basis

	-**** Kubeadm does not deploy kubelet
		- You have to manually install the kubelet on the worker nodes

		# ps aux | grep -i Kubelet        				- for checking the options of a running kubelet process


- Kubeproxy


	- Every pod can communicate to everyother pods in a kubernetes cluster using networking solution provided by kubeproxy.

	-**** Kubeproxy is a process that run on each node in k8 cluster, its job is to look for new services and everytime a new service is created it creates the appropriate rules on each nodes to forward traffic to those services to the backend pods

	- It creates a IP tables run to manage all these things

	- You can install kubeproxy either manually

	-**** Or a kubeadm also deploy it on each nodes, it is deployed as a Daemon on each nodes.


- PODs


	- Containers are basically encapsulated within a pods.

	- A single pod can have multiple containers but those containers would be of different type, one could be belonings to appliation and other could be a helper container(might be doing some supporting task to the appln container)

	- Deploying a pod

		# kubetctl run nginx --image nginx		(--image nginx : is the name of the image)

		# kubectl get pods



- Replica & Replication Controller

	- It performs the load balancing and scaling

	- Replication controller is older, ReplicaSet is new method
		- ReplicaSet has additional paramer of "selector"
		- If already any pod with same label is running then it does not create the additional pods

			selector:
			  matchLabels:
			    type: front-end


- Deployments

	- Upgrade the newer version of docker instances not all at once but gradually.
		- Might want to upgrade it one after another.
		- You might want to undo the changes.
		- Deployment creates a replicaset automatically.
		

- Namespace

	- namespace is used to segregate the resources in the kubernetes.
	- Kubernetes have following ns,
		- default
		- kube-system
		- kube-public

	- Each namespace can have a quota of resources and different policies.

	- Resources within a same name space can reach directly
	- To reach to resource in different namespace like say dev environment use,

			- service-name.namespace.svc.cluster.local
			- db-service.dev.svc.cluster.local

		# kubectl get pods --namespace=kube-system


- Service

	- Node-port service : Lister for a request on the nodes port and forward that request to the Pod running the appropriate applications.

		- targetPort: port on the POD where application is listening within a POD
		- port: port on the service itself
		- nodePort: port on the node itself which we use to access the webserver externally

		- for nodePort:
		-------------------------------------
			spec:
			  type: NodePort
			  ports:
			    - targetPort: 80		# if not specified, taken same as a port
			      port: 80				# this is mandatory field
			      nodePort: 30008		# if not specified a arbitrary value is taken
			  selector:					# link the service to the pod
			    app: myapp	
			    type: front-end
		-------------------------------------
		- you can use IP of any node to access the nodePort service

	- Cluster-ip service : The service creates a virtual IP inside a cluster to enable a communication between different services such a front-end to backend servers. It provides a single interface to communicate with other pods.
		- It is a default type.
		-------------------------------------
			spec:
			  type: ClusterIP
			  ports:
			    - targetPort: 80			# port where the pod is exposed
 				  port: 80					# port where the service is exposed
 			  selector:
 			    app: myapp
 			    type: back-end
		-------------------------------------


	- Loadbalancer service : It is done in supported cloud providers. Like using ALB with public IP address.

