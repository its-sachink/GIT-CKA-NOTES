---------- 14 OCt 2019 ----------

1) Redis issue : 2 hours

----------
redis-cli -a QL1d5bD -p 6381

redis-cli -a QL1d5bD -p 26381

redis-cli -a QL1d5bD -p 6381 info replication

redis-server /etc/redis/6381.conf
redis-sentinel /etc/redis/sentinel_26381.conf

	- Restarted the server multiple times. Got the slave not joining.
	
	- Finally after going through the logs and multiple troubleshooting steps, found that there is a stale database between master and slave because of which the slave and master not getting synced.
	- Cleaned the stale data and restart the cluster, cluster came up fine.

llbpal55:/var/lib/redis/6381 # ls
dump.rdb  temp-4163.rdb  temp-49812.rdb
llbpal55:/var/lib/redis/6381 # rm *
llbpal55:/var/lib/redis/6381 # ls
llbpal55:/var/lib/redis/6381 # cd /var/lib/redis/sentinel_26381
llbpal55:/var/lib/redis/sentinel_26381 # ls


----------

llbpal53:/var/lib/redis/6381 # ls
dump.rdb  temp-1571047146.7824.rdb
llbpal53:/var/lib/redis/6381 # rm *
llbpal53:/var/lib/redis/6381 # ls
llbpal53:/var/lib/redis/6381 # cd /var/lib/redis/sentinel_26381
llbpal53:/var/lib/redis/sentinel_26381 # ls
llbpal53:/var/lib/redis/sentinel_26381 # cat /etc/redis/6381.conf | grep -i slave


ctoolapi-eu1.sapdigitalinterconnect.com

2) Resolving the Demo server issue : 	- 2 hour

- Found that the Demo server is not able to ping the CTOOL URL.
- But the CTOOL URL was being able to ping from IAD1 and Prod servers.

------------- Demo server ---------------
[skesarkar@iad1bastion01 ~]$ ping ctoolapi-eu1.sapdigitalinterconnect.com
PING ctoolapi-eu1.sapdigitalinterconnect.com (178.248.228.135) 56(84) bytes of data.
64 bytes from 178.248.228.135: icmp_seq=1 ttl=244 time=84.2 ms
64 bytes from 178.248.228.135: icmp_seq=2 ttl=244 time=84.1 ms
64 bytes from 178.248.228.135: icmp_seq=3 ttl=244 time=84.0 ms
^C
--- ctoolapi-eu1.sapdigitalinterconnect.com ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2308ms
rtt min/avg/max/mdev = 84.041/84.118/84.207/0.246 ms

[skesarkar@iad1bastion01 ~]$ curl -v https://ctoolapi-eu1.sapdigitalinterconnect.com
* About to connect() to ctoolapi-eu1.sapdigitalinterconnect.com port 443 (#0)
*   Trying 178.248.228.135... connected
* Connected to ctoolapi-eu1.sapdigitalinterconnect.com (178.248.228.135) port 443 (#0)
--------------------------------------------

- Investigated with Augusto, and raised a ticket with NetEng as we found that there are couple of services which were not workig on Demo because of some change in the Network firewall configuration.

------------- us4cidemo002 -------------------------

- Opened a NGIS ticket with the NetEng team for 
	- Opening a firewall connection from us4cidemo002 to ctoolapi-eu1.sapdigitalinterconnect.com on port 443.

=================================================
Source IP Address: 10.240.2.42
Destination IP Address: 178.248.228.135
Protocol: TCP 
Dest Port: 443
Direction: Bi-Direction
==================================================

- We found the same issue with the below URL as well. So opened a request for that as well.

------------ us4cidemo002 ----------------------
Firewall Whitelist HTTPS port 443 : us4cidemo002 to authentication.sapmobileservices.com

Hi NetEng Team,

Kindly please open a HTTPS port 443 from host "us4cidemo002.bd.trust" to "authentication.sapmobileservices.com".

=================================================
Source IP Address: 10.240.2.42
Destination IP Address: 74.117.14.229
Protocol: TCP 
Dest Port: 443
Direction: Bi-Direction
==================================================

- Send a mail to Debelina regarding that.

Hi Debelina,

We have one more request. Kindly please have a look at it.

Also wanted to know was there any network changes because  some connectivity that were working before aren’t working anymore.

Thank you.

Regards,
~ Sachin. K.
DevOps.



1) Updated the appropriate ENV variables for DEV, QA, DEMO, US2&4, PROD Environment.
		- As per Ravi request updated the env variables and performed the build.
	- Performed the build for that.
	
2) Worked on fixing the issue reported by Surbhi for MFA, 
- Worked on resolving the Surbhi issue of MFA connectivity.

-------------------------------------------------------------------
Error Log for MFA
Oct 14 10:49:31 us4cidemo002 docker/bbf92f9ce792[10153]: 2019-10-14 10:49:31.479 ERROR 1 --- [io-15200-exec-1]
Oct 14 10:49:31 us4cidemo002 docker/bbf92f9ce792[10153]: #011#011#011#011c.s.s.l.m.services.impl.MfaServiceImpl   :  Error generating OAuth token I/O error on POST request for "https://authentication.sapmobileservices.com/authorization/getAccessToken": Connect to authentication.sapmobileservices.com:443 [authentication.sapmobileservices.com/74.117.14.229] failed: connect timed out; nested exception is org.apache.http.conn.ConnectTimeoutException: Connect to authentication.sapmobileservices.com:443 [authentication.sapmobileservices.com/74.117.14.229] failed: connect timed out, [org.springframework.web.client.RestTemplate.doExecute(RestTemplate.java:732), org.springframework.web.client.RestTemplate.execute(RestTemplate.java:680), org.springframework.web.client.RestTemplate.exchange(RestTemplate.java:600), com.sap.smcs.livelink.mfa.services.impl.MfaServiceImpl.generateAccessTokenForMfa(MfaServiceImpl.java:179), com.sap.smcs.livelink.mfa.LiveLinkMfaPlugin.generateAccessTokenForMfa(LiveLinkMfaPlugin.java:37), com.sap.smcs.livelink.apigateway.controllers.MfaController.generateToken(MfaController.java:152), sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method), sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.lang.reflect.Method.invoke(Method.java:498)]
Oct 14 10:49:31 us4cidemo002 docker/bbf92f9ce792[10153]: 2019-10-14 10:49:31.480 ERROR 1 --- [io-15200-exec-1]
Oct 14 10:49:31 us4cidemo002 docker/bbf92f9ce792[10153]: #011#011#011#011c.s.s.l.a.controllers.MfaController      :  Error in getting access token for mfa using client=1
-------------------------------------------------------------------

3) Acknowledged to NOC team mail regarding, [ESCALATION]|SEV2||fr1webhooks001.bd.trust||SOLUTION_DYNATRACE: \"Solution (Live Link) - ProblemDetails\":\"OPEN Problem 794 i|TT#300137733
	

5) Worked on CF Whatsapp upgradation from v2.5 to v2.6	- 2 Hours



=================================== 15 Oct 2019 ===================================

15-Oct-2019

- Went through the provided document by Pablo.

- Updated the variables provided by Nidhi.

- Worked on the build error of 2GEN, https://lvpal408.pal.sap.corp:9445/chain/result/viewChainLog.action?buildKey=LIV-LLKPOR0&buildNumber=42  .
	- Issue resolved.

- Worked on the Build log issue provided by Nidhi.
	- Resolve the issue, https://lvpal408.pal.sap.corp:9445/browse/LIV-LLKAP0-68/log
	
- Worked on sudying the Whitesource and setting it up.
15 Oct 2019

Abhilekh :	3.5
- Helped Abhilekh with the Build issue,
	https://lvpal408.pal.sap.corp:9445/browse/LIV-LLKPOR0-42/
- Worked on issue, [Bamboo] LLK > llk-portal-ui-2geng > 2GENG > #42 has FAILED.
- Helped to expose the docker as per the github, Port 15910 was exposed in your Dockerfile, https://github.wdf.sap.corp/SMCS/livelink-portal-ui/blob/2GENG/Dockerfile
	
	
SD Team : .5 hours
- Responsed to SD team mail.
- Resolved the file system usage issue.


Nidhi :
https://lvpal408.pal.sap.corp:9445/browse/LIV-LLKAP0-68/log


================================= 16 Oct 2019 =============================================
2 hours :

- Resolved nidhi issue of Java version.
	- She was facing some issue with the Java version on the build environment and locally that she was building.
	
- Reset BDTrust password for rahul.
	
- Checked the issue of Javeriah, https://lvpal408.pal.sap.corp:9445/deploy/viewDeploymentResult.action?deploymentResultId=117080776 .

- Troubleshooted the reason as to why the WhiteSource code is failing to Build. Made some changes to the GIT POM.XML file and rebuild but it didn't worked.
		https://lvpal408.pal.sap.corp:9445/browse/LIV-LLKAC-127
		
6 hours :

1) Reviewed all the security Group setting, made some appropriate changes to test the working.
	- Found that it was missing 443 connection for dmz-public-infra .
	- Then tried adding instances to the WhatsApp-LB but they were not getting detected.

2) Tried changing the LoadBalancer to internal-only on the current stack but found below issue.

{noformat}
2019-10-16 12:28:44 UTC+0530	WAEntLB	UPDATE_FAILED	CloudFormation cannot update a stack when a custom-named resource requires replacing. Rename WhatsApp-LB and update the stack again.
{noformat}

3) Created a different stack called WhatsApp-Dev with setting the LoadBalancer as a internal one, still the instances were not getting registered to the loadbalancer. 


4) Created a different stack call WhatsAppDev with setting the LoadBalancer as a external, where the instances were not getting registered again.

{noformat}	
	- Found below error in the LoadBalancer status,
		- us-east-1a
		subnet-08cc69af0d4b122ed
		198.19.0.0/25
		0
		No (Availability Zone contains no healthy targets)
	Remove from Load Balancer
		us-east-1b
		subnet-0dbd7383b4131d784
		198.19.0.128/25
		0
		No (Availability Zone contains no healthy targets)
	Remove from Load Balancer
{noformat}
	
	- Further tried attaching ELB to the private subnet at the same time when it was attached to the public subnet, got following error.
	{noformat}
			ELB cannot be attached to multiple subnets in the same AZ. (Service: AmazonElasticLoadBalancing; Status Code: 409; Error Code: InvalidConfigurationRequest; Request ID: b041c4b8-1bef-4d3e-9846-98d4b9f68185)
	{noformat}
	
		- Analysed the architecture as below.

{noformat}	
- Whatsapp AutoScaling group :

Availability Zone(s) 
us-east-1a, us-east-1b
Subnet(s) 
subnet-00773f93a24810958 (awsdev-app-2, 198.19.8.0/24)
subnet-0879d8c0539d74341 (awsdev-app-1, 198.19.7.0/24)

Security Group : awsdev-app-sg, sg-06bb7fc6339218e37

-----------------

WhatsApp-LB :

Availability Zones
subnet-08cc69af0d4b122ed (198.19.0.0/25)	- No (Availability Zone contains no healthy targets)
subnet-0dbd7383b4131d784 (198.19.0.128/25)	- No (Availability Zone contains no healthy targets)

Security Group : sg-0af54daa8be87d311, awsdev-dmz-public-whatsapp-sg										
{noformat}		

			
			
- Solution :
		
		- Checked the deetails as to how to attach a instanced from private subnet to ELB.
			https://aws.amazon.com/premiumsupport/knowledge-center/public-load-balancer-private-ec2/
			

		- Made appropriate changes to the awsdev-dmz-public-whatsapp-sg, the instances got detected by the ELB.
		
		- Raised a request with NetEng to open appropriate ports, https://lvpal408.pal.sap.corp:9443/browse/AMK-126.
		
			- Used below template for that.

{noformat}			
LB -> Instances

------------------------------
for : awsdev-dmz-public-whatsapp-sg  Outbound

outbound rule : 
HTTPS
TCP
443
sg-06bb7fc6339218e37 (awsdev-app-sg)
Allow HTTPS health check for the whatsapp instances.

-------------------------- Template -----------------------------
VPC: AWSDEV
Source IP(s): WhatsAppDev-LB-1225407380.us-east-1.elb.amazonaws.com Load Balancer, so you will have to edit the outbound rule of awsdev-dmz-public-whatsapp-sg
Destination IP(s): 198.19.8.0/24, 198.19.7.0/24
Protocol (TCP, UDP, or ICMP): TCP
Port: 443
Description: Allow EC2 instances from awsdev-app-1, awsdev-app-2 to get probed by LoadBalancer in awsdev-dmz-public-1, awsdev-dmz-public-2 for Health Check.
-----------------------------------------------------------------

Note : Simple way to do this would be, Edit the outbound rule of "awsdev-dmz-public-whatsapp-sg" sg-0af54daa8be87d311 securitygroup. Specify below things as a input.
	- Type : HTTPS
	- Protocol : TCP
	- Port : 443
	- Destination : custom(from drop down menu), sg-06bb7fc6339218e37 (awsdev-app-sg)
	- Description : Allow EC2 instances to get probed by LoadBalancer for HealthCheck.
{noformat}
	
	- Further Checked the reason as to why the EC2 instances are being continuously added and removed from the ELB.
	
	- Went through the CF template to see the details of SG but that SG was created manually by NetEng.



================================= 22 Oct 2019 =============================================

- Monitor the 499 errors to capture the required information from the logs and dynatrace monitoring tool.
- Worked on RCA for 499 errors.
- Enabled the debug logs on llk-nginx containers.

- Fixed the Redis issue for 2GEN project.		2 
- Changed the value of the attribute for 2GEN. Build and compiled the 2GEN on Dev and QA.


============================ 23 Oct 2018 =====================
- Check and setup build plan for master branch for all the projects and perform the Build, change/set the value of MAVEN_GOALS variable and check for any possible errors. This is a part of ongoing Whitesource implementation.

	llk_admin_portal
	llk-accounts-service-v2
	llk-analytics
	llk-apigateway
	llk-archival-service
	llk-callback-manager
	llk-docs
	llk-eventlog-loader
	llk-integrate-portal
	llk-SMSEventStore
	llk-ui-gateway
	ppc-aat-client
	ppc-hr-interface

- 2 hours call with Pablo.	- 2

- Setup debug logs and analysing the RCA for 499 errors, worked with Dipti on this.	- 3

================= 24 Oct =============
- 1 Hour call with Abhilekh

- Modified the build and deploy plan for llk-portal-ui-2gen so that it can point to the required port. 2 hours
	- Had to the understand the requirement and make appropriate changes.
	- Had to make changes to the environment variables and the Ansible playbook for this. Created a new folder llk-ui-portal-2gen and made appropriate changes to the ports 16501.
	- Added this line to the main docker container playbook, { src: "{{ env_file_path }}", dest: "/opt/{{user}}/{{ service }}/", name: "llk-portal-ui-2geng" }
	- https://lvpal408.pal.sap.corp:9445/browse/LIV-LLKPOR0-51
	- Performed the deployment and sanity testing.
	- Got confirmation from the developer for the same.
	
														2 hours.	

- Respond to SD team.
- Work on Suhwei reported issue of callback manager not showing status.
- Fixed Pankaj Wani Jira login issue.

- Troublehsooting 499 errors for the Paypal client.

- 725898eb35b6, 2f39bd6f0afc, 7468c21149b9, cfe7f1ef3e6a


============== 25 Oct ===============

- Had a call with Sourav from SD team, explained them little bit about the Dynatrace tracking thing.
- Worked on disk full issue on us4ciapi04 server.
- Worked on MFA compiling issue, its been reported to the Jeries via mail.



============= 11 Nov 2019 =============
1 hour : Surbhi KT
1 hour : Barani whitesource
1 hour : NOC, Worked on resolving the Authentication issue with the IT team.
1 hour : Yogita Pandit, anmol gorver
4 hour : Reading doc and implementing the CF for JIRA.


============= 12 Nov 2019 ==============

5 - Worked on testing and rectifying the Terraform teamplate.
1 - Worked on deployment of SGE with different port.
1 - Worked on resolving the issue of Redis cluster on QA.

1 - Worked on Mike request to identify the customer with the wrong email body.
Worked on resolving the issue of Pankaj wani.
Worked with the testing team to identify the App Code "7mjdPvs8AQ2B2tb8"


============ 13 Nov 2019 ==============

- Worked with Rukshana on detailing her about the SGE deployment and its environment.	1
- Resolved the login issue for Javehriah.

- Analysed the bearer token failure for livelink.

https://apm.cf.us10.hana.ondemand.com/e/f95fd477-ba09-47d0-ba91-d6f35a4e2f22/#failureanalysis;sci=SERVICE-166A78BA4824E1C0;eventid=-8960241961126666548_1573635900000;pid=-8960241961126666548_1573635900000V2;timeframe=custom1573635900000to1573638329524;servicefilter=0%1E10%11SERVICE_METHOD_GROUP-7B370CB463C9E87D%103%110;gf=all

- Nikita 	1

Requesting you to please provide me (C# is C5300646) access to below tools:

•	Github, 
•	Slack, 
•	Jira, 
•	Bamboo, 
•	Jump Server (adtrust and bdtrust)

Kindly let me know, if you require more details. Thanks!

- Worked on remainig Whitesource.	2
- Worked on terraform JIRA request.	2



============= 14 Nov 2019 ================
[ESCALTION]|SEV2||fr1webhooks001.bd.trust : OPEN Problem 439  .5 hrs
	- follow up with the developers and NOC team.
	
- Worked on resolving rest of the Whitesource issues. - 2.5
- Worked on fixing the JIRA template. - 2.5
- Had a call with Pablo Silva regarding JIRA Terraform template. - 2

============= 15 Nov 2019 ================
#(200294003)MO message faling with error 500 as live link URL not working - log analysis - 2



============= 18 Nov 2019 ============
- Worked on resolving the issue of prashasti build issues. 1.5hr 

https://lvpal408.pal.sap.corp:9445/deploy/viewDeploymentResult.action?deploymentResultId=117081520

https://lvpal408.pal.sap.corp:9445/deploy/viewDeploymentResult.action?deploymentResultId=117081366


- Spoke with dipti regarding the issue of 499.
- Anaylsed the Dynatrace logs.
- Went through the current log files to keep check on 499.
- Further found that the logs have been overridden.
- The issue didn't occured again.

- Resolved the log rotate issue on production hosts.	- 1

- Resolved issue of Nidhi build, https://lvpal408.pal.sap.corp:9445/browse/LIV-LLKAC8-4 - 1

- Had a call with PAblo regarding CF JIRA Template - 2 hrs.


============= 19 Nov 2019 ============
- Worked on reviewing Whitesource.	3h
- Worked on callby_retry issue. 30m

- Worked on JIRA Cf Template. 3h

- Had a call with Pablo. 1.5h


============= 20 Nov 2019 ============
- Worked on TTL of callback_retry .5h

- Had a call with Ravi. .5h

- Had a call with Nidhi regarding				.5h
	CALLBACK_MANAGER_MANAGEMENT_PORT=15001
	on performance env

- Worked on CF.		2h

- Worked on Abhilekh request - 2GENG qa deployment failing 	: 2hr

- Raised PAR for ADtrust accounts. .5h

- Worked on Surbhi request to resolved the QA issue for sms-channel.	2 hr


Worked on TTL of callback_retry monitoring : 30 minutes
Had a call with Ravi from message manager team : 30 minutes
Had a call with Nidhi regarding : 30 minutes
CALLBACK_MANAGER_MANAGEMENT_PORT=15001
redeployed the services on on performance env.
Raised PAR for ADtrust accounts, for "Anmol" and for "Jai" : 30 minutes.

============= 21 Nov 2019 ============
- Done with restarting the docker daemon on llbpal55
- Fixed the sms-channel docker issue.		2h

- AAT system was down because of REDIS issue, fixed the Redis issue.
- Brought up the AAT server on QA.		2h

- Worked on monitoring callback_retry queue exceeding the limit.
- Fixed the llbpal55 memory issue, set the crontjob to clear it periodically.	1h

- Worked on build and deploy pipeline for llk-engagement-callback.	2h

- Had a call with Pablo regarding troubleshooting of JIRA ALB. 1h


=========== 22 Nov 2019 ===============
- Worked on monitoring the callback_retry queue exceeding the limit.


=========== 23 Nov 2019 ===============
- Worked on fixing the CF for JIRA.



=========== 25 Nov 2019 ==============
- Responded to IN365 email.
- Helped prashasti on performance environment.
- Restarted the Bamboo issue.
- Worked on VPC flow logs. AWS Athena.
- Resolved the Build issue of Ravi, https://lvpal408.pal.sap.corp:9445/admin/viewError.action?buildKey=LIV-LLKAC0-BWM&error=1
- Checked AWS4 prod for IN365 and JIRA CF. Didn't found anything.


=========== 26 Nov 2019 ===============
- Fixed the issue of Build for ui-2gen, ankush.
- Fixed issue of ui-2gen build issue again.
- Analysed the CF template for IN365.
- 


=========== 2 Dec 2019 =============
- Resolved problem : Low disk space on host us4cipapi02. https://apm.cf.us10.hana.ondemand.com/e/f95fd477-ba09-47d0-ba91-d6f35a4e2f22/#problems/problemdetails;pid=-1293177268142584018_1575045000000V2;gf=all 

30mins

- Worked on issue realted to NBSP for MO processing.  1.5 h
US4CI Email Alert : OPEN Problem 451: Failure rate increase on Web request service [PRD_WEB_HOSTS] nginx - livelink.sapmobileservices.com:9999

- Worked on issue related to livelink failing for IN365 while sending the mails.
https://apm.cf.us10.hana.ondemand.com/e/f95fd477-ba09-47d0-ba91-d6f35a4e2f22/#failureanalysis;sci=SERVICE-7BFD98C9EEE33736;pid=7523264397298957620_1575266460000V2;servicefilter=0%1E10%11SERVICE_METHOD_GROUP-96D060D954E111A1%102%11500-599%103%110;timeframe=custom1575266331246to1575267028753;gf=all  
- Worked on IN365 issue, send a follow up mail to Iqbal.
- Had a call with Iqbal regarding this.
- As requested by Iqbal from IN365 team, coordinated with Testing team to perform the required testing.
- Testing was successful, seems to be a temporary issue as discussed with Dev team.
- Closing the issue. 

	2h

- llk-frontend-reverse-proxy error resolved : https://apm.cf.us10.hana.ondemand.com/e/f95fd477-ba09-47d0-ba91-d6f35a4e2f22/#problems/problemdetails;pid=7950391790110345545_1575264960000V2	


1h



- Helped Amol and Prashasti with the DB connectivity issues on Production and Performance hosts. 30 mins.

- Resolved issue of Nikita github. 30 mins.

- Worked with NOC for issue of  [ESCALATION] | SEV2 | SOLUTION_DYNATRACE: \"Solution (Live Link) - ProblemDetails\":\"OPEN Problem 488 in environment | TT# 300139670

 1h

- 1 hour call with Pablo. We discussed ingress, egress doubts related to kubernetes.

============ 3 Dec 2019 ============
- Checked Demo system, prashasti reported it as a down.
- Updated ARCHIVAL_SERVICE_URL=${PARENTHOST}:15650 on Nidhi request.
- Performed the respective build and deployment, confirmed it with the Developer.
- Chat logs are present on livelink_devops channel.

2h

- Attended the call with Khalid Ebdulla.	1h
	
- Created a HowTo Article. https://wiki.wdf.sap.corp/wiki/display/CI365/How+to+TroubleShoot+Failing+MO+messages		1h


- Slowness of llbpal55 server, Slowness Issues on QA-llbpal55 server. 1h

- ASE DB Creation on Aws Dev Env for 2Gen.	1h

- Worked with NOC on [ESCALATION] | SEV2 | SOLUTION_DYNATRACE: \"Solution (Live Link) - ProblemDetails\":\"OPEN Problem 888 in environment SAPDI | TT# 300139773.
	- Was a issue of failure to reach Paypal API.

	2h


=========== 4 Dev 2019 ==============
- Checking the callback_retry queue.		1h
	- cat messages | grep -i "Dec  4 06:" | grep -i "Message will be removed from the callback queue


- Working on nidhi build request,	2h

 could you please check this: https://lvpal408.pal.sap.corp:9445/browse/LIV-LLKAC8-6/log
Nidhi Nigam 10:38 AM
Hi Sachin
could you please check this: https://lvpal408.pal.sap.corp:9445/browse/LIV-LLKAC8-6/log
Sachin Kesarkar 12:11 PM
This is done, https://lvpal408.pal.sap.corp:9445/browse/LIV-LLKAC8-9

skesarkar@us4cidemo002 $ docker login https://dbs-docker-repository.docker.repositories.sap.ondemand.com
Username (dbs-docker-user):
Error response from daemon: invalid registry endpoint https://dbs-docker-repository.docker.repositories.sap.ondemand.com/v0/: unable to ping registry endpoint https://dbs-docker-repository.docker.repositories.sap.ondemand.com/v0/
v2 ping attempt failed with error: Get https://dbs-docker-repository.docker.repositories.sap.ondemand.com/v2/: x509: certificate signed by unknown authority
 v1 ping attempt failed with error: Get https://dbs-docker-repository.docker.repositories.sap.ondemand.com/v1/_ping: x509: certificate signed by unknown authority. If this private registry supports only HTTP or HTTPS with an unknown CA certificate, please add `--insecure-registry dbs-docker-repository.docker.repositories.sap.ondemand.com` to the daemon's arguments. In the case of HTTPS, if you have access to the registry's CA certificate, no need for the flag; simply place the CA certificate at /etc/docker/certs.d/dbs-docker-repository.docker.repositories.sap.ondemand.com/ca.crt
skesarkar@us4cidemo002 $ docker login https://dbs-docker-repository.docker.repositories.sap.ondemand.com
Username (dbs-docker-user):
Error response from daemon: invalid registry endpoint https://dbs-docker-repository.docker.repositories.sap.ondemand.com/v0/: unable to ping registry endpoint https://dbs-docker-repository.docker.repositories.sap.ondemand.com/v0/
v2 ping attempt failed with error: Get https://dbs-docker-repository.docker.repositories.sap.ondemand.com/v2/: x509: certificate signed by unknown authority
 v1 ping attempt failed with error: Get https://dbs-docker-repository.docker.repositories.sap.ondemand.com/v1/_ping: x509: certificate signed by unknown authority. If this private registry supports only HTTP or HTTPS with an unknown CA certificate, please add `--insecure-registry dbs-docker-repository.docker.repositories.sap.ondemand.com` to the daemon's arguments. In the case of HTTPS, if you have access to the registry's CA certificate, no need for the flag; simply place the CA certificate at /etc/docker/certs.d/dbs-docker-repository.docker.repositories.sap.ondemand.com/ca.crt

- Responded to RAMA query.	30mins

- Reported llbpal53 got down to ITDirect. ITdirect SR Ticket: 7004320491. Tried to check by taking the ILO Console as well. Finally reported to ITDirect.	1h

- Deployed the 2gen ui forwarding to 16501. Ankush : https://lvpal408.pal.sap.corp:9445/deploy/viewDeploymentResult.action?deploymentResultId=119046862 

30mins

- Brought up AAT, Redis on llbpal55.	1.5 h

	1h

- Resolved the issue of Paypal email report on us4llkapi003 server.	1h
[ESCALATION] | SEV2 | SOLUTION_DYNATRACE: \"Solution (Live Link) - ProblemDetails\":\"OPEN Problem 888 in environment SAPDI | TT# 300139773

- LLK Report - US4 geo environment: 2019-12-03 , resolving the reporting script.	1h


- Resolved the issue of Paypal email report on us4llkapi003 server.		3.5h
[ESCALATION] | SEV2 | SOLUTION_DYNATRACE: \"Solution (Live Link) - ProblemDetails\":\"OPEN Problem 888 in environment SAPDI | TT# 300139773
	- Helped Nidhi to make confirm the callback_retry and callback_timeout TTL timings.
	- Coordinated with NOC team to communicate with the Paypal.
	- Kept on watching for the messages in the logs just to make sure none of the Paypal messages are deleted.
			- Checking the callback_retry queue.
			- cat messages | grep -i "Dec  4 06:" | grep -i "Message will be removed from the callback queue
	- Confirmed to Steve on the mail, that none of the messages have been deleted from the callback_retry queue.
	- Make sure all the messages are drained successfully and processed successfully from the callback_queue.

=========== 5 Dev 2019 ============== 	1h

- Helped Nidhi to resolve the redis issue.
	Issue was that the Data sync was not happening between master and slave redis server.

root     60742  8.3  0.0 345936 304912 ?       Ssl  11:37  52:45 redis-server 0.0.0.0:6381
root     67443  0.0  0.0  16176  1424 pts/1    S+   21:58   0:00 redis-cli -a QL1d5bD -p 6381
root     75783  400  0.0 346072 304252 ?       R    22:07   0:00 redis-rdb-bgsave 0.0.0.0:6381


- Worked on the failing job raised by Evelio, https://lvpal408.pal.sap.corp:9445/deploy/viewDeploymentResult.action?deploymentResultId=119046954
- Raised an issue of Ansible server.
	"Failed to retried schedules. GET returned status: 500 A server error has occured."
	- I have raised a SysEng Ticket for that. https://jira.di-infra.sap.corp/browse/SYSENG-1035
- Worked on Rukshana Deployment issues, chat logged on Slack.	
	
-  [ESCALATION] | SEV2|| fr1webhooks001.bd.trust | SOLUTION_DYNATRACE: \"Solution (Live Link) - ProblemDetails\":\"OPEN Problem 867  |TT#300139844 - Worked with NOC on this. Production issue.		3.5hr
	- Consul server getting full.
	- Production server getting down
	- Attended Mike call.
	- Attended NOC call.
	- Later on when found that the container depends on consul server to get started, the issue was resolved.
	
	
- Had a call with Iqbal, analysed the AWS deployment further.
- Worked with Rukshana.


========= 6 Dec 2019 ===============
- Resolved issue of Radhika.
	- Make changes to application-dev.yml for QA environment.
		"DATABASE_URL": "jdbc:sybase:Tds:llbpal58.pal.sap.corp:9000/eng_db",
		"DATABASE_USER": "eng_admin_db_user",
		"DATABASE_PASSWORD": "Ng2u8HvW",
		
- Resolved issue of build and deployment, https://lvpal408.pal.sap.corp:9445/browse/KWM-SEC-239 second-gen. Set proper trigger for devstable branch.
https://lvpal408.pal.sap.corp:9445/browse/KWM-SEC2-10

- Done with the deployment. https://lvpal408.pal.sap.corp:9445/deploy/viewDeploymentResult.action?deploymentResultId=119047078
- Worked on resolving the US4 geo cronjob for paypal report mails.
- Update variable SYSTEM_URL on llbpal53 SGE.

- Done with creating the RCA document, https://wiki.wdf.sap.corp/wiki/display/CI365/LLK+365+Root+Cause+Analysis+-+Containers+healthcheck+endpoints+failing	1h

- Had a call with Pablo.
	- Had a 1.5 hours call with Pablo, regarding JIRA deployment.
	
- Worked on resolving the US4 geo cronjob for paypal report mails.
- Update variable SYSTEM_URL on llbpal53 SGE
- SGE - Deployment Issue.
- Checked regarding the JIRA Template.


======== 9 Dec 2019 ================
- Worked on Bamboo build plan. Cannot connect to Bamboo Server. Restarted the Bamboo service.
- Gave a required docker accesses to prashasti on performance environment.
- Worked on resolving the logs issue of performance issue. Logs were not populating. Restarted the rsyslog on the performance hosts.
- Helped ravi to access the logs for springboot-dev container.
- Issue of performance nodes.
	- 20 5 * * * [ -f /opt/appdata/logs/old_logs/messages-$(date +%Y-%m-%d).gz ] && /usr/bin/docker run --rm -v /opt/appdata/logs/old_logs/messages-$(date +%Y-%m-%d).gz:/messages.gz dbs-docker-repository.docker.repositories.sap.ondemand.com/llk-email-report:v1 > /opt/llk/report/mailBody.txt || /usr/bin/echo "DevOps, please check the custom report script of the container running at us4llkapi003" > /opt/llk/report/mailBody.txt
	- Replaced the cronjob with a script.

- JIRA, EFS not working, restoring the EFS config.	3h
	- Trying to connect to JIRA db using UI.
	- Resolved the EFS issue.
	- Resolving the DB connectivity issue.	
	
- Had a call with Pablo. 1h

- Resolved Nidhi issue of docker cp command.	1h
	



========== 10 Dec 2019 ================
- Was again facing an issue of no mysql driver for JIRA, so copied the mysql dump from lvpal408 to ec2-user.
	- Reset the ansible playbook.
	- reset with empty database.
	- then forwarded the configuration in dbconfig.xml to backed up database.
- Followed this approach to resolve the issue. https://community.atlassian.com/t5/Answers-Developer-Questions/How-to-connect-jira-with-non-empty-database/qaq-p/466790
- Generated the Trial license.
- Performed the reindexing of the Database.
- Was able to login with success. 

- Pankaj redis configuration files being deleted. refer to mail for details. Redis server is down on 53.
	- Could see the configuration issues.
	- The configuration was totally spoiled by manual editing.
llbpal55:/etc/redis # history | grep -i 26379
 1009  2019-12-10 01:25:18 redis-sentinel /etc/redis/sentinel_26379.conf
 1012  2019-12-10 01:25:50 history | grep -i 26379
llbpal55:/etc/redis # ls
6379.conf  6381.conf  6383.conf  sentinel_26380.conf  sentinel_26382.conf  sentinel_26386.conf
6380.conf  6382.conf  6386.conf  sentinel_26381.conf  sentinel_26383.conf
llbpal55:/etc/redis # ls 6379*
6379.conf
llbpal55:/etc/redis # vim sentinel_26379.conf
llbpal55:/etc/redis # ps aux | grep -i 6379
root     49425  0.1  0.0  33252  2044 ?        Ssl  01:25   0:00 redis-server 0.0.0.0:6379
root     50651  0.0  0.0   9280   952 pts/11   S+   01:27   0:00 grep --color=auto -i 6379
llbpal55:/etc/redis # redis-sentinel /etc/redis/sentinel_26379.conf
50758:X 10 Dec 01:27:14.421 # Can't chdir to '/var/lib/redis/sentinel_26379': No such file or directory
llbpal55:/etc/redis # mkdir /var/lib/redis/sentinel_26379
llbpal55:/etc/redis # chown redis.root /var/lib/redis/sentinel_26379
llbpal55:/etc/redis # chmod 755 /var/lib/redis/sentinel_26379
	----
llbpal53:/etc/redis # /opt/redis/bin/redis-sentinel sentinel_26379.conf
80820:X 10 Dec 01:15:38.930 # Can't chdir to '/var/lib/redis/sentinel_26379': No such file or directory
llbpal53:/etc/redis # cd /var/lib/redis/

------- config change in 6379.conf file also --------
llbpal55:/etc/redis # cat /etc/redis/6379.conf | grep -i auth
masterauth ITiNoTmo
llbpal55:/etc/redis #
----
llbpal53:/etc/redis # cat /etc/redis/6379.conf | grep -i auth
masterauth "ReDis2AAt"
llbpal53:/etc/redis #
---------
llbpal53:/var/lib/redis/6379 # cat /etc/redis/6379.conf | grep -i slaveof
slaveof 10.48.144.173 6380
---------
llbpal53:/var/lib/redis/6379 # redis-cli -a ITiNoTmo -p 6379
127.0.0.1:6379> INFO REPLICATION
# Replication
role:slave
master_host:10.48.144.173
master_port:6380
	llbpal53:/var/lib/redis/6379 # cat /etc/redis/6379.conf | grep -i slaveof
	slaveof 10.48.144.173 6380



========== 11 Dec 2019 ================
- Worked with Amol on giving him the required priviledges.
- Resolved Prashnu issue of deployment. https://lvpal408.pal.sap.corp:9445/deploy/config/configureDeploymentProject.action?id=121798657&environmentId=121667588


- Had a call with Barani.	1h

https://jira.di-infra.sap.corp/browse/ISS-1768 1h

- Deployed the ISQL database to AWS database.	1h

- Checked for the way to deploy redis on AWS environment from Ansible Tower.  2h
- Done with installing the Redis on 2 nodes of AWS for 2GEN.

- documentation of IN365, documentation of JIRA AWS. 	2h


- Done with the DB deployment on AWSDEV for 2GEN, reported to Ravi. https://jira.di-infra.sap.corp/browse/ISS-1768


========== 12 Dec 2019 ================
- Configure REDIS on AWS.
	- Spin new REDIS instances on AWS.
	- Read the link, https://docs.ansible.com/ansible-tower/latest/html/userguide/credentials.html
	- master : 198.19.7.233	, redis-master-mm = redis-master-mm-awsdev
	  slave : 198.19.7.248, 198.19.7.128 , redis-slave-mm = redis-slave-mm-awsdev
	  Sentinel : 198.19.7.233, 198.19.7.248, 198.19.7.128 , redis-sentinel-mm = redis-sentinel-mm-awsdev
	- Configured the credentials for Tower, redis-mm-awsdev-credentials.
	- Performed the deploymnent.
	- Facing issue as below,
		[ec2-user@ip-198-19-7-233 ~]$ redis-cli -a 4hh9SmX -p 6381
		127.0.0.1:6381> set key1 val1
		(error) CLUSTERDOWN Hash slot not served
		127.0.0.1:6381> get key1
		(error) CLUSTERDOWN Hash slot not served
		127.0.0.1:6381> quit
	- Removed the Cluster mode of redis and started, it got started.
	
- Now configuring the EKS environment.
	

- Worked on Pankaj request,
	- Changed env variable for dev_Stable.
	- deployment performed, https://lvpal408.pal.sap.corp:9445/deploy/viewDeploymentResult.action?deploymentResultId=121045212
- Provided readonly access to Ravi Jagani on Production environment, raised a PAR for that. I have raised a PAR request for this : 100170713.
- Had a call with Pablo.










- Worked on YAML template to deploy the second-gen, Scheduler and 2gen-ui.
- Checked with CheYu regarding exact number of variables required for UI.
- Provided Ravi with the configuration of the DB.


========== 13 Dec 2019 ===================
- Set the JIRA login for Anmol Grover.

- Update in SVN for Pankaj. Performed the Pankaj deployment, https://lvpal408.pal.sap.corp:9445/deploy/viewDeploymentResult.action?deploymentResultId=121045293   
- DevStable and Dev deployment for Shikha, Shah.																
	https://lvpal408.pal.sap.corp:9445/deploy/viewDeploymentResult.action?deploymentResultId=121045299
	https://lvpal408.pal.sap.corp:9445/deploy/viewDeploymentResult.action?deploymentResultId=121045300
	
- Resolved the Build issue of accounts-service-v2, https://lvpal408.pal.sap.corp:9445/browse/LIV-LLKAC8-11.
	- https://lvpal408.pal.sap.corp:9445/browse/LIV-LLKAC8-12
	- Resolved the deployment issue, https://lvpal408.pal.sap.corp:9445/deploy/viewDeploymentResult.action?deploymentResultId=121045321
	
- Performance Deployment.	
	- Redis deployment.
	master : redis-master-mm-perf : us4ciapp01
	slave : redis-slave-mm-perf : us4cipapi01, us4cipapi02
	sentinel : redis-sentinel-mm-perf : us4ciapp01, us4cipapi01, us4cipapi02
	password : uu9DUkhE
	redis-cli -a uu9DUkh -p 6382
	
	- Database credentials : eng_admin_db_user, D2pB9uRC, eng_db
		tcp        0      0 10.200.2.12:4000        0.0.0.0:*               LISTEN      57062/dataserver

	- Deployment of second-gen-engagement done.
	
	587c2e281a0f        dbs-docker-repository.docker.repositories.sapcdn.io/second-gen-engagement:1.9.4.266                    "sh /opt/sge/service…"   5 minutes ago       Up 5 minutes              15740/tcp, 0.0.0.0:16500->16500/tcp                                                                                                                 second-gen-engagement
	
	- Was getting error while performing the deployment for Scheduler service. Reported to Shikha Shah and worked with her to ser appropriate values on bootstrap.yml file.
	SPRING_CLOUD_CONFIG_LABEL=performance
	SPRING_CLOUD_CONFIG_PROFILE=prod
	
	- Working with Luis to sort out the Variables for llk-ui-portal.


- Followed up regarding ENV variables for 2GEN, ENV details regarding the llk-portal-ui-2gen.
	- Checked in consul GIT configuration, nothing got.
	



============= 16 Dec 2019 ===============
- Worked on NOC issue of, SAP CRM---------300140218------Sev(3)------(AM-NOC)-MO latency observed during  0th and 06th   Hours.

https://apm.cf.us10.hana.ondemand.com/e/f95fd477-ba09-47d0-ba91-d6f35a4e2f22/#problems/problemdetails;pid=-770917843923711825_1576366200000V2

The details of the error shows that it is Client Side 499 error, it’s a known issue and I am in touch with developers regarding this.
	
	- Followed up with the Dev Team on this.

- Performed deplyoment for Nidhi, https://lvpal408.pal.sap.corp:9445/deploy/viewDeploymentResult.action?deploymentResultId=121045423
- Created a diagram for IN365.
- Went through the video of IN365.
- Created database for MM ENG_DB.
- Worked on 499 error.
- Followed up with Luis on llk-ui-portal issue.

- Had a call with Pablo.

- SVN update for Pankaj, 
https://lvpal408.pal.sap.corp:9445/deploy/viewDeploymentResult.action?deploymentResultId=121045427

- Follow up on SVN variables with UI team.


============= 17 Dec 2019 ===============
- Followed up with Luis in early morning regarding ENV variables.

- Had a discussion with Shikha regarding Scheduler service.
	- Check mail for detailed errors.
	- Check mail with Handres as well.

- Performed deployment of UI service.
	- Check mails for detailed errors.
- Set the ENV VARS for UI service.
- Resolved the issue of UI by having a call with Abhilekh.
- Restarted AAT on Pablo's request.


============= 18 Dec 2019 ===============
- Worked with Shikha.
	- Unknown Host exception.
	- 
- Worked with PAvkaj.
	-	Could you please update following in svn on “stage_dev” , “development” and “stage_dev” environment of second-gen-engagement?

aat: ${SCHEDULER_AAT:http://llbpal55.pal.sap.corp:14500/oauth/token}

- Shikha request to add new variables to SVN.

Can you please add the following environmental variables for Scheduler Service:

For Dev/QA:

DATABASE_URL:jdbc:sybase:Tds:llbpal58.pal.sap.corp:8000/sch_db
DATABASE_USER:sch_admin_db_user
DATABASE_PASSWORD:HwU28h7a

For PERF:

DATABASE_URL:jdbc:sybase:Tds:us4cipapi01:4000/sch_db
DATABASE_USER: sch_admin_db_user
DATABASE_PASSWORD: RJl9SifbU

For QA we will need a different SCH_DB, as discussed, the values we will change later. 


- Worked on creating the DB getting following error on llbpal58.
Server 'QASCIDB', Line 3:
DISK INIT encountered an error while attempting to open/create the physical file for device '/sapci/servers/QASCIDB/data/sch_db_data01.dat'. Consult the Adaptive Server error log for more details. For the Cluster Edition, check the error log of each instance.
- Worked on deployment of Performance servers.
- Creating DB for SCH DB on performance server.
- Created a SCH DB on AWS Database.

-bash:1252::sybase@ip-198-19-6-159:/sapci/scripts/eng: cat sch_db.out | grep -i login
1> sp_addlogin sch_admin_db_user, SytghWMJ, sch_db
New login created.
1> sp_addlogin sch_db_user, SytghWMJ, sch_db
New login created.

- Setting the yaml for ui on aws-eks

- Had a check with Augusto, Luis.

- Worked with Mohit to create new Livelink user. Check mail.



============= 19 Dec 2019 ================
- Send mail regarding errors with performance for 2GEN. 
	- Issue on Backend side of 2GEN performance.
- Checking bootstrap.yml file for Scheduler for Dev branch for llbpal58 issue on scheduler.
- working on writing the Services for k8.

- Pankaj, Update the callback URL in stage_dev SVN file.

Can you please update scheduler AAT url as per below on QA environment of secondgenengagement?

"SCHEDULER_AAT=https://llbpal55.pal.sap.corp:14500/oauth/token"



============= 20 Dec 2019 ================
- Rukshana issue of login to putty.
- 


============= 23, 25, 26, 27 Dec 2019 ================
- On leave.


============= 30 Dec 2019 =================
- Worked on Noel issue of Bamboo.
- Worked on Nidhi request to Add parameters.
- Worked on Build/ deploy issue of Nidhi and Wani.
	https://lvpal408.pal.sap.corp:9445/deploy/viewDeploymentResult.action?deploymentResultId=121046443
	https://lvpal408.pal.sap.corp:9445/deploy/viewDeploymentResult.action?deploymentResultId=121046442
- Worked on issue of Omkar Mhatre.





============= 06 Jan 2020 =================
- 1 hour call : with Pablo
- setup build and deploy for 2gen-frontend-reverse-proxy
- Worked with Nidhi on rectifying the deployment issues

[‎06-‎01-‎2020 21:14]  Silva, Pablo:  
[root@us4cipapi01 ~]# curl -v -k https://localhost:16101 
 
[‎06-‎01-‎2020 21:19]  Silva, Pablo:  
https://172.24.227.30:15920/ 
 
[‎06-‎01-‎2020 21:24]  Silva, Pablo:  
https://demo-login.sapdigitalinterconnect.com/admin/clients 
 
[‎06-‎01-‎2020 21:24]  Silva, Pablo:  
https://us4cipapi01.bd.trust:16101/aat/response 
 

============ 07 JAn 2020 ===============
- VDI was down for 2 hours.
- Restarted demo AAT as it was giving null pointer exception.
2020-01-07 09:14:41.383 ERROR 7496 --- [http-nio-14550-exec-197] o.a.c.c.C.[.[.[/].[dispatcherServlet]    : Servlet.service() for servlet [dispatcherServlet] in context with path [] threw exception [Request processing failed; nested exception is org.springframework.transaction.CannotCreateTransactionException: Could not open JPA EntityManager for transaction; nested exception is java.lang.NullPointerException] with root cause
java.lang.NullPointerException: null
java.lang.NullPointerException: null
java.lang.IllegalStateException: java.lang.NullPointerException
Caused by: java.lang.NullPointerException: null
java.lang.NullPointerException: null
java.lang.IllegalStateException: java.lang.NullPointerException
Caused by: java.lang.NullPointerException: null




========== 08 Jan 2020 ==============
- Resolved the issue of nvpal https://lvpal408.pal.sap.corp:9445/deploy/viewDeploymentResult.action?deploymentResultId=123732166

- Nidhi build issue of smsevents_logs.
	issue : https://lvpal408.pal.sap.corp:9445/deploy/viewDeploymentResult.action?deploymentResultId=123731996
	resolved : https://lvpal408.pal.sap.corp:9445/deploy/viewDeploymentResult.action?deploymentResultId=123732225
	
- Had a late night call with Pablo for 2GEN perf.


========== 09 Jan 2020 ==============
- Gave rights to prashasti.
- Spoke with Bhomik regarding CC365 product.
- 1 hour call with Pablo.
- 30 minutes call with Khalid.


========== 10 Jan 2020 ==============
- Call with Bhomik.



========== 13 Jan 2020 ==============
- Started the Bamboo server and agents.
- Created a free space on us4cipapi01 node.
- Migrated rabbitmq on us4ciapp01, moved it to us4cipapi02 as slave. us4cipapi01 is still master.
	
	- Was getting error.
	[root@us4cipapi01 ~]# docker exec 95a436b818da rabbitmqctl stop_app
Stopping rabbit application on node rabbit@us4cipapi01 ...
[root@us4cipapi01 ~]# docker exec 95a436b818da rabbitmqctl join_cluster rabbit@us4cipapi02
Clustering node rabbit@us4cipapi01 with rabbit@us4cipapi02
Error: unable to perform an operation on node 'rabbit@us4cipapi02'. Please see diagnostics information and suggestions below.

Most common reasons for this are:

 * Target node is unreachable (e.g. due to hostname resolution, TCP connection or firewall issues)
 * CLI tool fails to authenticate with the server (e.g. due to CLI tool's Erlang cookie not matching that of the server)
 * Target node is not running

In addition to the diagnostics info below:

 * See the CLI, clustering and networking guides on https://rabbitmq.com/documentation.html to learn more
 * Consult server logs on node rabbit@us4cipapi02
 * If target node is configured to use long node names, don't forget to use --longnames with CLI tools

DIAGNOSTICS
===========

attempted to contact: [rabbit@us4cipapi02]

rabbit@us4cipapi02:
  * connected to epmd (port 4369) on us4cipapi02
  * epmd reports node 'rabbit' uses port 25672 for inter-node and CLI tool traffic
  * TCP connection succeeded but Erlang distribution failed

  * Node name (or hostname) mismatch: node "rabbit@us4ciapp01" believes its node name is not "rabbit@us4ciapp01" but something else.
    All nodes and CLI tools must refer to node "rabbit@us4ciapp01" using the same name the node itself uses (see its logs to find out what it is)


Current node details:
 * node name: 'rabbitmqcli-2674-rabbit@us4cipapi01'
 * effective user's home directory: /var/lib/rabbitmq
 * Erlang cookie hash: apdTxqz4/5ku/MYin62MQg==

[root@us4cipapi01 ~]# docker exec 95a436b818da rabbitmqctl join_cluster rabbit@us4cipapi02
Clustering node rabbit@us4cipapi01 with rabbit@us4cipapi02
[root@us4cipapi01 ~]# docker exec 95a436b818da rabbitmqctl start_app
Starting node rabbit@us4cipapi01 ...
 completed with 3 plugins.
[root@us4cipapi01 ~]# docker exec 95a436b818da rabbitmqctl set_policy ha-nodes "^nodes\." '{"ha-mode":"nodes","ha-params":["rabbit@us4cipapi02", "rabbit@us4cipapi01"]}'
Setting policy "ha-nodes" for pattern "^nodes\." to "{"ha-mode":"nodes","ha-params":["rabbit@us4cipapi02", "rabbit@us4cipapi01"]}" with priority "0" for vhost "/" ...
[root@us4cipapi01 ~]# docker exec -it 95a436b818da rabbitmqctl set_policy ha-callback "^callback" \ '{"ha-mode":"exactly","ha-params":2,"ha-sync-mode":"automatic"}'
Setting policy "ha-callback" for pattern "^callback" to " {"ha-mode":"exactly","ha-params":2,"ha-sync-mode":"automatic"}" with priority "0" for vhost "/" ...
[root@us4cipapi01 ~]#
[root@us4cipapi01 ~]#
[root@us4cipapi01 ~]# docker ps | grep -i rabbit
95a436b818da        rabbitmq:3-management                                                                               "docker-entrypoint.s…"    8 minutes ago       Up 8 minutes             0.0.0.0:4369->4369/tcp, 5671/tcp, 0.0.0.0:5672->5672/tcp, 0.0.0.0:15672->15672/tcp, 0.0.0.0:25672->25672/tcp, 0.0.0.0:35197->35197/tcp, 15671/tcp   docker-rabbit-perf-us4cipapi01

	-- on us4cipapi02 :
		[skesarkar@us4cipapi02 ~]$ docker run -d --restart=always --hostname us4cipapi02 --name docker-rabbit-perf-us4cipapi02 -p 4369:4369 -p 5672:5672 -p 15672:15672 -p 25672:25672 -p 35197:35197 -e RABBITMQ_DEFAULT_USER=admin -e RABBITMQ_DEFAULT_PASS=abcd1234 -e RABBITMQ_ERLANG_COOKIE='NSORXJLSGOOCLYMVCDQF' rabbitmq:3-management
		83166eb4309b027b27436fb52e71e69d2a49b91154ba8acc7dd0d009ccbabc60
		[skesarkar@us4cipapi02 ~]$
		[skesarkar@us4cipapi02 ~]$
		[skesarkar@us4cipapi02 ~]$ docker ps | grep -i papi02
		83166eb4309b        rabbitmq:3-management                                                                               "docker-entrypoint.s…"    8 seconds ago       Up 6 seconds             0.0.0.0:4369->4369/tcp, 5671/tcp, 0.0.0.0:5672->5672/tcp, 0.0.0.0:15672->15672/tcp, 0.0.0.0:25672->25672/tcp, 0.0.0.0:35197->35197/tcp, 15671/tcp   docker-rabbit-perf-us4cipapi02

	-- on us4cipapi01 :
	
		[root@us4cipapi01 ~]# docker exec 95a436b818da rabbitmqctl join_cluster rabbit@us4cipapi02
		Clustering node rabbit@us4cipapi01 with rabbit@us4cipapi02
		[root@us4cipapi01 ~]# docker exec 95a436b818da rabbitmqctl start_app
		Starting node rabbit@us4cipapi01 ...
		completed with 3 plugins.
		[root@us4cipapi01 ~]# docker exec 95a436b818da rabbitmqctl set_policy ha-nodes "^nodes\." '{"ha-mode":"nodes","ha-params":["rabbit@us4cipapi02", "rabbit@us4cipapi01"]}'
		Setting policy "ha-nodes" for pattern "^nodes\." to "{"ha-mode":"nodes","ha-params":["rabbit@us4cipapi02", "rabbit@us4cipapi01"]}" with priority "0" for vhost "/" ...
		[root@us4cipapi01 ~]# docker exec -it 95a436b818da rabbitmqctl set_policy ha-callback "^callback" \ '{"ha-mode":"exactly","ha-params":2,"ha-sync-mode":"automatic"}'
		Setting policy "ha-callback" for pattern "^callback" to " {"ha-mode":"exactly","ha-params":2,"ha-sync-mode":"automatic"}" with priority "0" for vhost "/" ...
		[root@us4cipapi01 ~]#
		[root@us4cipapi01 ~]#
		[root@us4cipapi01 ~]# docker ps | grep -i rabbit
		95a436b818da        rabbitmq:3-management                                                                               "docker-entrypoint.s…"    8 minutes ago       Up 8 minutes             0.0.0.0:4369->4369/tcp, 5671/tcp, 0.0.0.0:5672->5672/tcp, 0.0.0.0:15672->15672/tcp, 0.0.0.0:25672->25672/tcp, 0.0.0.0:35197->35197/tcp, 15671/tcp   docker-rabbit-perf-us4cipapi01
		[root@us4cipapi01 ~]#
		
			- Checked further able to get the admin screen.
			
- Worked with Prashasti regarding containers on production.
- Adding the token to performance of SGE.
	- Performed the testing didn't worked.
	- Getting error related to column.

- Worked with Nidhi, on DB connections for event-store on sms_db on server us4cipapi01.
	- Checked the performance for the application event-store.
	- available connections :
	   0          1355             recv sleep                               lle_admin_db_user                                                    lle_admin_db_user                                                    3821e6bf9fe1                                                0                         lle_db                   tempdb                                   AWAITING COMMAND                                                               0                                  syb_default_pool         

(363 rows affected)
(return status = 0)

	- Max connections available are : 1200
1> sp_configure 'number of user connections'
2> go
 Parameter Name                                                                                                                                                                                                                                                  Default     Memory Used Config Value                                                                                                                                                                                                                                                    Run Value                                                                                                                                                                                                                                                       Unit                 Type
 --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ----------- ----------- --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- -------------------- --------------------
 number of user connections                                                                                                                                                                                                                                               25      923262         1200                                                                                                                                                                                                                                                            1200                                                                                                                                                                                                                                                    number               dynamic

(1 row affected)
(return status = 0)

	
	
========== 14 Jan 2020 ==============
- Resolved Nidhi Build issue,
	https://lvpal408.pal.sap.corp:9445/browse/LIV-LLKAC0-BWM-351/log
	sol : https://lvpal408.pal.sap.corp:9445/browse/LIV-LLKAC0-BWM-351/log

- sms-channel on us4cipapi01 got hang.	
- Resolved the space issue on the node, us4cipapi01 and us4ciapp01.
	- Cleaned up the space but didn't reflected the correct space usage.

[root@us4cipapi02 overlay2]# cd c5b7fadf892a9f4ba78c6d2afa2858dda5eab515ff95c95d0f099d72cc791238
[root@us4cipapi02 c5b7fadf892a9f4ba78c6d2afa2858dda5eab515ff95c95d0f099d72cc791238]# du -sh *
152G    diff
4.0K    link
4.0K    lower
152G    merged
8.0K    work
[root@us4cipapi02 c5b7fadf892a9f4ba78c6d2afa2858dda5eab515ff95c95d0f099d72cc791238]# cd merged/
[root@us4cipapi02 merged]# du -sh *
784K    bin
12K     dev
1.5M    etc
16K     home
3.9M    lib
59M     LLKCallbackManager.jar
16K     media
4.0K    mnt
32K     opt
4.0K    proc
4.0K    root
4.0K    run
232K    sbin
4.0K    srv
4.0K    sys
64K     tmp
98M     usr
152G    var
4.0K    work
[root@us4cipapi02 merged]# cd var/log/
[root@us4cipapi02 log]# ls
llk
[root@us4cipapi02 log]# cd llk/
[root@us4cipapi02 llk]# ls
CallbackManager.2019-12-23.log  CallbackManager.2019-12-31.log  CallbackManager.2020-01-07.log  CallbackManager.2020-01-10.log
CallbackManager.2019-12-27.log  CallbackManager.2020-01-02.log  CallbackManager.2020-01-08.log  CallbackManager.2020-01-11.log
CallbackManager.2019-12-30.log  CallbackManager.2020-01-06.log  CallbackManager.2020-01-09.log  CallbackManager.log
[root@us4cipapi02 llk]# du -sh *
412K    CallbackManager.2019-12-23.log
43M     CallbackManager.2019-12-27.log
28M     CallbackManager.2019-12-30.log
50M     CallbackManager.2019-12-31.log
19G     CallbackManager.2020-01-02.log
13G     CallbackManager.2020-01-06.log
28M     CallbackManager.2020-01-07.log
28G     CallbackManager.2020-01-08.log
47G     CallbackManager.2020-01-09.log
29G     CallbackManager.2020-01-10.log
0       CallbackManager.2020-01-11.log
18G     CallbackManager.log
[root@us4cipapi02 llk]# find . -mtime +2 -delete
[skesarkar@us4cipapi02 ~]$ df -h
Filesystem               Size  Used Avail Use% Mounted on
/dev/mapper/datavg-root  1.4G  105M  1.2G   9% /
devtmpfs                 504G     0  504G   0% /dev
tmpfs                    504G  348K  504G   1% /dev/shm
tmpfs                    504G  4.1G  500G   1% /run
tmpfs                    504G     0  504G   0% /sys/fs/cgroup
/dev/mapper/datavg-usr   9.1G  2.8G  5.9G  33% /usr
/dev/sda2                569M  192M  378M  34% /boot
/dev/mapper/datavg-home  9.1G   37M  8.6G   1% /home
/dev/mapper/datavg-var    15G  5.6G  8.4G  41% /var
/dev/mapper/datavg-opt   252G  241G     0 100% /opt
	- Had to stop the docker and restart it.
	
	- Further sms-channel also got hung.
	

[root@us4ciapp01 ase_docker]# lsof +L1
lsof: no pwd entry for UID 9999
lsof: no pwd entry for UID 9999
lsof: no pwd entry for UID 1000
lsof: no pwd entry for UID 1000
lsof: no pwd entry for UID 9999
lsof: no pwd entry for UID 9999
lsof: no pwd entry for UID 9999
lsof: no pwd entry for UID 9999
lsof: no pwd entry for UID 9999
lsof: no pwd entry for UID 9999
lsof: no pwd entry for UID 9999
lsof: no pwd entry for UID 9999
lsof: no pwd entry for UID 9999
lsof: no pwd entry for UID 9999
COMMAND      PID      USER   FD   TYPE DEVICE   SIZE/OFF NLINK       NODE NAME
docker-co    907      root   21u  FIFO   0,20        0t0     0  958288647 /run/docker/containerd/d96d806cb2d4913b56eaa8f705eb8a8183684037a37fe852eac2ae054b964377/9d1a0e659279821de4e783d696ab510533ece744c9a53c4f6b64816cad505fe2-stdin (deleted)
docker-co    907      root   22u  FIFO   0,20        0t0     0  958948600 /run/docker/containerd/d96d806cb2d4913b56eaa8f705eb8a8183684037a37fe852eac2ae054b964377/58fe43c73f5a3929311f94c1bce0dc9d67324a3bbdcfb0700213a6551dd5bdf9-stdin (deleted)
docker-co    907      root   23u  FIFO   0,20        0t0     0  958958659 /run/docker/containerd/d96d806cb2d4913b56eaa8f705eb8a8183684037a37fe852eac2ae054b964377/88546f2414c6833653d01c9f46ab23639e2ea03ca382b499d97252f59bab0eb7-stdin (deleted)
docker-co    907      root   24r  FIFO   0,20        0t0     0  958288647 /run/docker/containerd/d96d806cb2d4913b56eaa8f705eb8a8183684037a37fe852eac2ae054b964377/9d1a0e659279821de4e783d696ab510533ece744c9a53c4f6b64816cad505fe2-stdin (deleted)
docker-co    907      root   25u  FIFO   0,20        0t0     0  958941035 /run/docker/containerd/d96d806cb2d4913b56eaa8f705eb8a8183684037a37fe852eac2ae054b964377/7fe6c3fa4f6a1edc3af3aaaa5381cea41120b30dfec57b977e6983d76eefc137-stdin (deleted)
docker-co    907      root   26r  FIFO   0,20        0t0     0  958941035 /run/docker/containerd/d96d806cb2d4913b56eaa8f705eb8a8183684037a37fe852eac2ae054b964377/7fe6c3fa4f6a1edc3af3aaaa5381cea41120b30dfec57b977e6983d76eefc137-stdin (deleted)
docker-co    907      root   27u  FIFO   0,20        0t0     0 1064150375 /run/docker/containerd/d96d806cb2d4913b56eaa8f705eb8a8183684037a37fe852eac2ae054b964377/d9e60666d1b57803b4df2744201e884504f9e97fb8409c5eabf4c4369b3d8a12-stdin (deleted)
docker-co    907      root   28r  FIFO   0,20        0t0     0  958948600 /run/docker/containerd/d96d806cb2d4913b56eaa8f705eb8a8183684037a37fe852eac2ae054b964377/58fe43c73f5a3929311f94c1bce0dc9d67324a3bbdcfb0700213a6551dd5bdf9-stdin (deleted)
docker-co    907      root   29r  FIFO   0,20        0t0     0  958958659 /run/docker/containerd/d96d806cb2d4913b56eaa8f705eb8a8183684037a37fe852eac2ae054b964377/88546f2414c6833653d01c9f46ab23639e2ea03ca382b499d97252f59bab0eb7-stdin (deleted)
docker-co    907      root   31u  FIFO   0,20        0t0     0  958871484 /run/docker/containerd/d96d806cb2d4913b56eaa8f705eb8a8183684037a37fe852eac2ae054b964377/bb98c85d3d7f0e929c7768d301f0ead408faf0d4036f9574fa17fb65e7aba9cc-stdin (deleted)
docker-co    907      root   32r  FIFO   0,20        0t0     0  958871484 /run/docker/containerd/d96d806cb2d4913b56eaa8f705eb8a8183684037a37fe852eac2ae054b964377/bb98c85d3d7f0e929c7768d301f0ead408faf0d4036f9574fa17fb65e7aba9cc-stdin (deleted)
docker-co    907      root   34r  FIFO   0,20        0t0     0 1064150375 /run/docker/containerd/d96d806cb2d4913b56eaa8f705eb8a8183684037a37fe852eac2ae054b964377/d9e60666d1b57803b4df2744201e884504f9e97fb8409c5eabf4c4369b3d8a12-stdin (deleted)
lsof: no pwd entry for UID 9999


	-- Further SMSChannel also got hung, Had to restart the while docker daemon and the respective containers.
	
[root@us4ciapp01 ase_docker]# docker ps | grep -i callback
d0a612f77d54        dbs-docker-repository.docker.repositories.sapcdn.io/llk-engagement-callback:1.9.4.68                "sh /opt/llk/service…"   4 weeks ago         Up 4 weeks                0.0.0.0:16600->16600/tcp                                       llk-engagement-callback
d96d806cb2d4        dbs-docker-repository.docker.repositories.sap.ondemand.com/llk-callback-manager:1.9.4.24-SNAPSHOT   "java -Djava.securit…"   7 weeks ago         Up 13 seconds             0.0.0.0:15000-15001->15000-15001/tcp                           llk-callback-manager
[root@us4ciapp01 ase_docker]# docker ps | grep -i sms
d0626887501f        dbs-docker-repository.docker.repositories.sapcdn.io/llk-sms-eventstore:2.0.1.18-SNAPSHOT            "sh /opt/llk/service…"   3 days ago          Up 19 hours (unhealthy)   0.0.0.0:15720-15721->15720-15721/tcp                           llk-sms-eventstore
a27a76889118        dbs-docker-repository.docker.repositories.sapcdn.io/llk-sms-channel:2.0.1.1-SNAPSHOT                "sh /opt/llk/service…"   7 days ago          Up 7 days (unhealthy)     5701/tcp, 0.0.0.0:15700-15701->15700-15701/tcp                 llk-sms-channel
10658d8b0101        dbs-docker-repository.docker.repositories.sapcdn.io/llk-sms-channel-mo-ack:1.9.4.160-SNAPSHOT       "sh /opt/llk/service…"   2 weeks ago         Up 13 days (healthy)      5701/tcp, 0.0.0.0:15710->15700/tcp, 0.0.0.0:15711->15701/tcp   llk-sms-channel-mo-ack
2de32a390c4b        dbs-docker-repository.docker.repositories.sap.ondemand.com/llk-sms-channel:1.9.3.250-SNAPSHOT       "sh /opt/llk/service…"   3 weeks ago         Up 3 weeks (unhealthy)    5701/tcp, 0.0.0.0:16700->15700/tcp, 0.0.0.0:16701->15701/tcp   lle-engagement-smschannel
[root@us4ciapp01 ase_docker]# docker restart d0626887501f  10658d8b0101 2de32a390c4b
d0626887501f
10658d8b0101
2de32a390c4b
[root@us4ciapp01 ase_docker]# docker ps | grep -i sms
d0626887501f        dbs-docker-repository.docker.repositories.sapcdn.io/llk-sms-eventstore:2.0.1.18-SNAPSHOT            "sh /opt/llk/service…"   3 days ago          Up About a minute (health: starting)   0.0.0.0:15720-15721->15720-15721/tcp                           llk-sms-eventstore
a27a76889118        dbs-docker-repository.docker.repositories.sapcdn.io/llk-sms-channel:2.0.1.1-SNAPSHOT                "sh /opt/llk/service…"   7 days ago          Up 7 days (unhealthy)                  5701/tcp, 0.0.0.0:15700-15701->15700-15701/tcp                 llk-sms-channel
10658d8b0101        dbs-docker-repository.docker.repositories.sapcdn.io/llk-sms-channel-mo-ack:1.9.4.160-SNAPSHOT       "sh /opt/llk/service…"   2 weeks ago         Up About a minute (health: starting)   5701/tcp, 0.0.0.0:15710->15700/tcp, 0.0.0.0:15711->15701/tcp   llk-sms-channel-mo-ack
2de32a390c4b        dbs-docker-repository.docker.repositories.sap.ondemand.com/llk-sms-channel:1.9.3.250-SNAPSHOT       "sh /opt/llk/service…"   3 weeks ago         Up About a minute (health: starting)   5701/tcp, 0.0.0.0:16700->15700/tcp, 0.0.0.0:16701->15701/tcp   lle-engagement-smschannel
[root@us4ciapp01 ase_docker]# docker stop a27a76889118
[root@us4ciapp01 ase_docker]# docker kill a27a76889118
^C
[root@us4ciapp01 ase_docker]# docker rm -f a27a76889118
^C
[root@us4ciapp01 ase_docker]# docker kill ----signal=SIGQUIT a27a76889118
bad flag syntax: ----signal=SIGQUIT
See 'docker kill --help'.
[root@us4ciapp01 ase_docker]# man docker
[root@us4ciapp01 ase_docker]# man docker
[root@us4ciapp01 ase_docker]# docker kill a27a76889118
[root@us4ciapp01 containers]# docker kill --signal=SIGKILL a27a76889118
^C
[root@us4ciapp01 containers]# docker kill --signal=SIGTERM a27a76889118
^C
[root@us4ciapp01 containers]# systemctl stop docker
[root@us4ciapp01 containers]# docker ps
Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
[root@us4ciapp01 containers]# systemctl start docker

^C
[root@us4ciapp01 containers]# uptime
 08:00:46 up 318 days,  9:53,  2 users,  load average: 1.26, 1.53, 1.76
[root@us4ciapp01 containers]# docker ps
^C
[root@us4ciapp01 containers]# systemctl status docker
● docker.service - Docker Application Container Engine
   Loaded: loaded (/usr/lib/systemd/system/docker.service; enabled; vendor preset: disabled)
  Drop-In: /etc/systemd/system/docker.service.d
           └─mountflags-slave.conf
   Active: activating (start) since Tue 2020-01-14 07:55:47 GMT; 5min ago
     Docs: https://docs.docker.com
 Main PID: 81468 (dockerd)
    Tasks: 75
   Memory: 1.6G
   CGroup: /system.slice/docker.service
           ├─81468 /usr/bin/dockerd
           ├─81481 docker-containerd --config /var/run/docker/containerd/containerd.toml
           └─87082 docker-containerd-shim -namespace moby -workdir /opt/appdata/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/a27a768891181060e0b4c82...

Jan 14 07:55:50 us4ciapp01.bd.trust dockerd[81468]: time="2020-01-14T07:55:50.091618192Z" level=debug msg="restoring container" container=d0a612f77d546258b...ing=false
Jan 14 07:55:50 us4ciapp01.bd.trust dockerd[81468]: time="2020-01-14T07:55:50.091678095Z" level=debug msg="restoring container" container=10658d8b0101e510f...ing=false
Jan 14 07:55:50 us4ciapp01.bd.trust dockerd[81468]: time="2020-01-14T07:55:50.091692489Z" level=debug msg="restoring container" container=d96d806cb2d4913b5...ing=false
Jan 14 07:55:50 us4ciapp01.bd.trust dockerd[81468]: time="2020-01-14T07:55:50.091816213Z" level=debug msg="restoring container" container=d656e98c3a9cd8a14...ing=false
Jan 14 07:55:50 us4ciapp01.bd.trust dockerd[81468]: time="2020-01-14T07:55:50.091937833Z" level=debug msg="restoring container" container=d0626887501f84755...ing=false
Jan 14 07:55:50 us4ciapp01.bd.trust dockerd[81468]: time="2020-01-14T07:55:50.092000473Z" level=debug msg="restoring container" container=68c60a6790ae441d2...ing=false
Jan 14 07:55:50 us4ciapp01.bd.trust dockerd[81468]: time="2020-01-14T07:55:50.092041663Z" level=debug msg="restoring container" container=2c8917872c476f10d...ing=false
Jan 14 07:55:50 us4ciapp01.bd.trust dockerd[81468]: time="2020-01-14T07:55:50.092095718Z" level=debug msg="restoring container" container=a7f14bc481867489b...ing=false
Jan 14 07:55:50 us4ciapp01.bd.trust dockerd[81468]: time="2020-01-14T07:55:50.092136554Z" level=debug msg="restoring container" container=2de32a390c4b5755c...ing=false
Jan 14 07:55:50 us4ciapp01.bd.trust dockerd[81468]: time="2020-01-14T07:55:50.092195325Z" level=debug msg="restoring container" container=10b31308cff01bfc0...ing=false
Hint: Some lines were ellipsized, use -l to show in full.
[root@us4ciapp01 containers]# systemctl start docker
^C
[root@us4ciapp01 containers]# systemctl stop docker


	- After multiple attempts of docker start and stop thinkgs for worked.
	
	- Worked with Pankaj to setup the environment of 2GEN on Performance environment.

	- Updated variables for Pankaj and Ruksana, performed the deployment. Details available on the chat and mail.
	


========== 15 Jan 2020 ==============

- Resolved issue of space on performance hosts, worked with developer on callback container to reduce the logs.

[root@us4cipapi02 overlay2]# cd ./c5b7fadf892a9f4ba78c6d2afa2858dda5eab515ff95c95d0f099d72cc791238/merged/var/log/llk
[root@us4cipapi02 llk]# ls
CallbackManager.log
[root@us4cipapi02 llk]# du -sh *
58G     CallbackManager.log
[root@us4cipapi02 llk]# > CallbackManager.log
^C


[root@us4cipapi02 llk]#
[root@us4cipapi02 llk]#
[root@us4cipapi02 llk]# du -sh *
0       CallbackManager.2020-01-14.log
9.7M    CallbackManager.log
[root@us4cipapi02 llk]# rm CallbackManager.2020-01-14.log
rm: remove regular empty file ‘CallbackManager.2020-01-14.log’? y
[root@us4cipapi02 llk]#
[root@us4cipapi02 llk]# du -sh *
11M     CallbackManager.log
[root@us4cipapi02 llk]# df -h
Filesystem               Size  Used Avail Use% Mounted on
/dev/mapper/datavg-root  1.4G  105M  1.2G   9% /
devtmpfs                 504G     0  504G   0% /dev
tmpfs                    504G  348K  504G   1% /dev/shm
tmpfs                    504G  4.1G  500G   1% /run
tmpfs                    504G     0  504G   0% /sys/fs/cgroup
/dev/mapper/datavg-usr   9.1G  2.8G  5.9G  33% /usr
/dev/sda2                569M  192M  378M  34% /boot
/dev/mapper/datavg-home  9.1G   37M  8.6G   1% /home
/dev/mapper/datavg-var    15G  6.9G  7.0G  50% /var
/dev/mapper/datavg-opt   252G  186G   54G  78% /opt

- Resolved Bamboo issue,
	- This build was gone into queue for almost 2 days.
	- https://lvpal408.pal.sap.corp:9445/browse/KWM-SCHED0-71
	- Tried to stop it manually but didn't worked, checked the bamboo process it didnt worked.
	- Had to stop the bamboo for that.
	13-Jan-2020 05:33:27	Build KWM-SCHED0-JOB1-71 had to be cancelled: it was marked as queued but was not present in the queue for (at least) the past 720 seconds.
	
- https://lvpal408.pal.sap.corp:9445/browse/LIV-LSC18-3
- Resolved the Nidhi issue of sms-channel build and deploy, changed the docker repository location to : docker login https://dbs-docker-repository.docker.repositories.sapcdn.io
	- Build the QA branch again of accounts-service-v2
	- then build the sms-channel again.
	- Done, https://lvpal408.pal.sap.corp:9445/browse/LIV-LSC18-6
- Performed deployments of Shikhs Shah, check Chat.
- Check Chat with Nidhi.

- Called to Shikha, explained her that Devops does not handle if the communication should be done on http or https.

vulnerabilities


================================ 16 Jan 2019 ===============================

[skesarkar@us4ciapi01 ~]$ docker ps | grep -i mo-ack
99520a6e1020        dbs-docker-repository.docker.repositories.sapcdn.io/llk-sms-channel-mo-ack:1.9.4.153                "sh /opt/llk/service   4 weeks ago         Up 4 hours          5701/tcp, 0.0.0.0:15710->15700/tcp, 0.0.0.0:15711->15701/tcp                                                                                        llk-sms-channel-mo-ack
[skesarkar@us4ciapi01 ~]$ docker stop 99520a6e1020
99520a6e1020
[skesarkar@us4ciapi01 ~]$ docker ps | grep -i rabbit
fa94b5df115c        rabbitmq:3-management                                                                               "docker-entrypoint.s   7 months ago        Up 5 months         0.0.0.0:4369->4369/tcp, 5671/tcp, 0.0.0.0:5672->5672/tcp, 0.0.0.0:15672->15672/tcp, 0.0.0.0:25672->25672/tcp, 15671/tcp, 0.0.0.0:35197->35197/tcp   docker-rabbit
[skesarkar@us4ciapi01 ~]$ docker restart fa94b5df115c
fa94b5df115c
[skesarkar@us4ciapi01 ~]$ docker ps | grep -i rabbit
fa94b5df115c        rabbitmq:3-management                                                                               "docker-entrypoint.s   7 months ago        Up 7 seconds        0.0.0.0:4369->4369/tcp, 5671/tcp, 0.0.0.0:5672->5672/tcp, 0.0.0.0:15672->15672/tcp, 0.0.0.0:25672->25672/tcp, 0.0.0.0:35197->35197/tcp, 15671/tcp   docker-rabbit
[skesarkar@us4ciapi01 ~]$ docker ps | grep -i rabbit
fa94b5df115c        rabbitmq:3-management                                                                               "docker-entrypoint.s   7 months ago        Up 58 seconds       0.0.0.0:4369->4369/tcp, 5671/tcp, 0.0.0.0:5672->5672/tcp, 0.0.0.0:15672->15672/tcp, 0.0.0.0:25672->25672/tcp, 0.0.0.0:35197->35197/tcp, 15671/tcp   docker-rabbit
[skesarkar@us4ciapi01 ~]$ docker start 99520a6e1020
99520a6e1020
[skesarkar@us4ciapi01 ~]$ docker ps | grep -i mo-ack
99520a6e1020        dbs-docker-repository.docker.repositories.sapcdn.io/llk-sms-channel-mo-ack:1.9.4.153                "sh /opt/llk/service   4 weeks ago         Up About a minute   5701/tcp, 0.0.0.0:15710->15700/tcp, 0.0.0.0:15711->15701/tcp                                                                                        llk-sms-channel-mo-ack
[skesarkar@us4ciapi01 ~]$ docker ps | grep -i mo-ack
99520a6e1020        dbs-docker-repository.docker.repositories.sapcdn.io/llk-sms-channel-mo-ack:1.9.4.153                "sh /opt/llk/service   4 weeks ago         Up 4 minutes        5701/tcp, 0.0.0.0:15710->15700/tcp, 0.0.0.0:15711->15701/tcp                                                                                        llk-sms-channel-mo-ack
[skesarkar@us4ciapi01 ~]$

--------

[skesarkar@us4ciapi03 ~]$ docker ps | grep -i mo-ack
183a114d87f4        dbs-docker-repository.docker.repositories.sapcdn.io/llk-sms-channel-mo-ack:1.9.4.153                "sh /opt/llk/service   4 weeks ago         Up 4 hours          5701/tcp, 0.0.0.0:15710->15700/tcp, 0.0.0.0:15711->15701/tcp                                                                                        llk-sms-channel-mo-ack
[skesarkar@us4ciapi03 ~]$ docker stop 183a114d87f4
183a114d87f4
[skesarkar@us4ciapi03 ~]$  docker ps | grep -i rabbit
0a2fd5d34778        rabbitmq:3-management                                                                               "docker-entrypoint.s   7 months ago        Up 5 months         0.0.0.0:4369->4369/tcp, 5671/tcp, 0.0.0.0:5672->5672/tcp, 0.0.0.0:15672->15672/tcp, 0.0.0.0:25672->25672/tcp, 0.0.0.0:35197->35197/tcp, 15671/tcp   docker-rabbit
[skesarkar@us4ciapi03 ~]$ docker restart 0a2fd5d34778
0a2fd5d34778
[skesarkar@us4ciapi03 ~]$  docker ps | grep -i rabbit
0a2fd5d34778        rabbitmq:3-management                                                                               "docker-entrypoint.s   7 months ago        Up 6 seconds        0.0.0.0:4369->4369/tcp, 5671/tcp, 0.0.0.0:5672->5672/tcp, 0.0.0.0:15672->15672/tcp, 0.0.0.0:25672->25672/tcp, 0.0.0.0:35197->35197/tcp, 15671/tcp   docker-rabbit
[skesarkar@us4ciapi03 ~]$  docker ps | grep -i rabbit
0a2fd5d34778        rabbitmq:3-management                                                                               "docker-entrypoint.s   7 months ago        Up 32 seconds       0.0.0.0:4369->4369/tcp, 5671/tcp, 0.0.0.0:5672->5672/tcp, 0.0.0.0:15672->15672/tcp, 0.0.0.0:25672->25672/tcp, 15671/tcp, 0.0.0.0:35197->35197/tcp   docker-rabbit
[skesarkar@us4ciapi03 ~]$ docker start 183a114d87f4
183a114d87f4
[skesarkar@us4ciapi03 ~]$ docker ps | grep -i mo-ack
183a114d87f4        dbs-docker-repository.docker.repositories.sapcdn.io/llk-sms-channel-mo-ack:1.9.4.153                "sh /opt/llk/service   4 weeks ago         Up About a minute   5701/tcp, 0.0.0.0:15710->15700/tcp, 0.0.0.0:15711->15701/tcp                                                                                        llk-sms-channel-mo-ack
[skesarkar@us4ciapi03 ~]$ docker ps | grep -i mo-ack
183a114d87f4        dbs-docker-repository.docker.repositories.sapcdn.io/llk-sms-channel-mo-ack:1.9.4.153                "sh /opt/llk/service   4 weeks ago         Up 4 minutes        5701/tcp, 0.0.0.0:15710->15700/tcp, 0.0.0.0:15711->15701/tcp                                                                                        llk-sms-channel-mo-ack
[skesarkar@us4ciapi03 ~]$


---------


[skesarkar@us4ciapi04 ~]$ docker ps | grep -i mo-ack
eaf2241f97d8        dbs-docker-repository.docker.repositories.sapcdn.io/llk-sms-channel-mo-ack:1.9.4.153                "sh /opt/llk/service   4 weeks ago         Up 4 hours          5701/tcp, 0.0.0.0:15710->15700/tcp, 0.0.0.0:15711->15701/tcp                                                                                        llk-sms-channel-mo-ack
[skesarkar@us4ciapi04 ~]$ docker stop eaf2241f97d8
eaf2241f97d8
[skesarkar@us4ciapi04 ~]$ docker ps | grep -i rabbit
42604b3f12f1        rabbitmq:3-management                                                                               "docker-entrypoint.s   7 months ago        Up 5 months         0.0.0.0:4369->4369/tcp, 5671/tcp, 0.0.0.0:5672->5672/tcp, 0.0.0.0:15672->15672/tcp, 0.0.0.0:25672->25672/tcp, 0.0.0.0:35197->35197/tcp, 15671/tcp   docker-rabbit
[skesarkar@us4ciapi04 ~]$ docker restart 42604b3f12f1
42604b3f12f1
[skesarkar@us4ciapi04 ~]$ docker ps | grep -i rabbit
42604b3f12f1        rabbitmq:3-management                                                                               "docker-entrypoint.s   7 months ago        Up 8 seconds        0.0.0.0:4369->4369/tcp, 5671/tcp, 0.0.0.0:5672->5672/tcp, 0.0.0.0:15672->15672/tcp, 0.0.0.0:25672->25672/tcp, 0.0.0.0:35197->35197/tcp, 15671/tcp   docker-rabbit
[skesarkar@us4ciapi04 ~]$ docker start eaf2241f97d8
eaf2241f97d8
[skesarkar@us4ciapi04 ~]$ docker ps | grep -i mo-ack
eaf2241f97d8        dbs-docker-repository.docker.repositories.sapcdn.io/llk-sms-channel-mo-ack:1.9.4.153                "sh /opt/llk/service   4 weeks ago         Up About a minute   5701/tcp, 0.0.0.0:15710->15700/tcp, 0.0.0.0:15711->15701/tcp                                                                                        llk-sms-channel-mo-ack
[skesarkar@us4ciapi04 ~]$ docker ps | grep -i mo-ack
eaf2241f97d8        dbs-docker-repository.docker.repositories.sapcdn.io/llk-sms-channel-mo-ack:1.9.4.153                "sh /opt/llk/service   4 weeks ago         Up 4 minutes        5701/tcp, 0.0.0.0:15710->15700/tcp, 0.0.0.0:15711->15701/tcp                                                                                        llk-sms-channel-mo-ack
[skesarkar@us4ciapi04 ~]$

- Resolved issue of ravi.
- Resolved issue of Nidhi, chat.


--------- 20 Jan 2019 ------------
- Shikh Shah issue -- check chat with Shikha Shah
fatal: [us4ciapp01]: FAILED! => {"changed": false, "msg": "Cannot have both the docker-py and docker python modules installed together as they use the same namespace and cause a corrupt installation. Please uninstall both packages, and re-install only the docker-py or docker python module. It is recommended to install the docker module if no support for Python 2.6 is required. Please note that simply uninstalling one of the modules can leave the other module in a broken state."}
	https://172.24.227.23/#/jobs/playbook/106749?job_search=page_size:20;order_by:-finished;not__launch_type:sync
		- Performed this to resolve.
			
			# history | tail -n 10
			1011  pip freeze | grep -i docker
			1012  pip list
			1013  pip list | grep -i docker
			1014  pip uninstall docker-py
			1015  pip list | grep -i docker
			1016  dockder ps
			1017  docker ps
			1018  pip install docker
			1019  pip list | grep -i docker
			1020  history | tail -n 10

		- Followed this article. https://neutrollized.blogspot.com/2018/12/cannot-have-both-docker-py-and-docker.html
		
		[root@us4ciapp01 ~]# history | tail -n 10
 1005  pip list | grep -i docker
 1006  pip remove docker
 1007  pip uninstall docker
 1008  pip list | grep -i docker
 1009  pip uninstall docker-pycreds
 1010  pip uninstall dockerpty
 1011  pip list | grep -i docker
 1012  clear
 1013  pip install docker-compose
 1014  history | tail -n 10

	- Resolved the issue, https://lvpal408.pal.sap.corp:9445/deploy/viewDeploymentResult.action?deploymentResultId=127762775
	
	
- Made deployment changes to the QA for llk-sms-channel-mock service. - Rajul Talkgaonkar.
	- Made changes to github devops repository.
	- Changed the ports from 15720, 15721 to 15730 and 15731.
	- Performed the deployments.
	
	- Done, https://lvpal408.pal.sap.corp:9445/deploy/viewDeploymentResult.action?deploymentResultId=127762785
	
	- Changed the MOCK_FLAG to True and performed the redeployment.
		https://lvpal408.pal.sap.corp:9445/deploy/viewDeploymentResult.action?deploymentResultId=127762786

	
- Worked on protecode scanning. Successfully performed the scanning.

	- Steps on creating a Protecode group and running a vulnerabilities check on a container tar file. 
		1) Self help portal, https://wiki.wdf.sap.corp/wiki/display/osssec/Self+Help+Portal
			1. Goto selfhelp portal https://go.sap.corp/oss_shp_app to register the Protecode group.
			2. For regisration added a PPMS SCV and a valid mail DL.
			3. As per wiki document, created a protecode group in the name of "SAP LIVE LINK 365 OD 1.0".
			
		2) You can check if the group has been registered or not using the following command.
		
		c5246840@llbpal53:~> curl -u "c5246840" https://protecode.c.eu-de-2.cloud.sap/api/groups/
		Enter host password for user 'c5246840':
		{"meta": {"code": 200}, "groups": [{"id": 901, "name": "SAP LIVE LINK 365 OD 1.0"}]}
		
		3) Further you can use the following script to upload the container tar file for further vulnerabilities analysis.
				- https://github.wdf.sap.corp/OS3/protecodesc-tools
				
			- After downloading the script you need to correct the address of the protecode server into that. 
			
			c5246840@llbpal53:~> wget https://github.wdf.sap.corp/OS3/protecodesc-tools/blob/master/bin/upload.sh
			c5246840@llbpal53:~> chmod a+x upload.sh
			c5246840@llbpal53:~> cat upload.sh | grep -i protecode.mo
			https://protecode.mo.sap.corp/api/upload/
			c5246840@llbpal53:~> vim upload.sh
			c5246840@llbpal53:~> cat upload.sh | grep -i protecode
			https://protecode.c.eu-de-2.cloud.sap/api/upload/

			- After editing the "upload.sh" file you can run following command to upload it to the protecode server.
			
			c5246840@llbpal53:~> ./upload.sh 901 llk-analytics.tar
			llk-analytics.tar
			Enter host password for user 'c5246840':
			{"meta": {"code": 200}, "results": {"status": "B", "sha1sum": "7db0361a206914ff58fcbfa4cd2d65717bcc99ec", "id": 208655, "product_id": 208655, "report_url": "https://protecode.c.eu-de-2.cloud.sap/products/208655/", "filename": "llk-analytics.tar", "rescan-possible": false, "stale": false, "custom_data": {}, "user": "sachin.kesarkar@sap.com", "last_updated": "2020-01-20T12:03:36"}}
			
		4) You can access the status of the vulnerabilities by accessing the following URL, https://protecode.c.eu-de-2.cloud.sap/group/901/

	- 
	
- Resolved Nidhi issue of Build, https://lvpal408.pal.sap.corp:9445/browse/LIV-LLKSMSEV4-9
	- Removed the artifact type.
	- and checked on clean the directory.
	
--------- 21 Jan 2019 ------------	
Off


--------- 22 Jan 2019 ------------
- Issue resolved with Prashasti.
- With Shikha.

- Done deployment of dd-ui on pablo request, https://bamboo.di-infra.sap.corp/deploy/viewDeploymentResult.action?deploymentResultId=127762948
- 

- Nidhi issue : https://lvpal408.pal.sap.corp:9445/browse/LIV-LLKSMSCHAN5-9/log
	'/usr/bin/docker login -u dbs-docker-user --password-stdin dbs-docker-repository.docker.repositories.sap.ondemand.com', exit code: -1

- Pablo summary of errors,

	Problem 935: Failure rate increase in environment: SAPDI	-> 404 Not found
		https://apm.cf.us10.hana.ondemand.com/e/f95fd477-ba09-47d0-ba91-d6f35a4e2f22/#problems/problemdetails;pid=5953665839314361935_1579689480000V2

	Problem 147: Failure rate increase in environment: SAPDI	
		-> 502 Bad gateway, those were almost 14 bad gateway request.
		-> com.sap.smcs.livelink.common.exceptions.LiveLinkException / null  : 462 4XX errors.
	
	Problem 504: Response time degradation in environment: SAPDI
		-> [PRD_API_HOSTS] SMSChannel.jar - SMSChannel - MT failed with "com.sap.smcs.livelink.common.exceptions.LiveLinkException":  2 requests got failed here.
		
	Problem 596: Failure rate increase in environment: SAPDI	- This issue happened twice. 11 and 14 requests failed.
		-> These were people connect 4XX errors.
		
- Docker registry.

docker run -d -v /usr/local/bamboo-agent-home-1:/root/bamboo-agent-home -v /var/run/docker.sock:/var/run/docker.sock --name="bamboo-agent-1" --net=host --init -d dbs-docker-repository.docker.repositories.sapcdn.io/atlasian-bamboo-agent:11 https://lvpal408.pal.sap.corp:9445 && docker logs -f bamboo-agent-1
	
	
	docker login -u dbs-docker-user -p n813nZw22a2eVj4N https://dbs-docker-repository.docker.repositories.sapcdn.io
	
	docker login dbs-docker-repository.docker.repositories.sap.ondemand.com -u dbs-docker-user -p n813nZw22a2eVj4N
	
- Brought down 55 and up 76(1) agent.

--------- 23 Jan 2019 ------------
- Call with Abhilekh.
- Call with Nidhi.


--------- 24 Jan 2019 ------------
- Made space on papi01.
- Worked on Build of eventstore.
- Worked with Rahul.

- Worked on analysing the dynatrace logs.


Problem 594 : Response time degradation : Reason major contributor as Network delay : https://apm.cf.us10.hana.ondemand.com/e/f95fd477-ba09-47d0-ba91-d6f35a4e2f22/#methodhotspots;entityId=SERVICE-166A78BA4824E1C0;pid=8334171521377806594_1579849740000V2;timeframe=custom1579856040000to1579856640000;servicefilter=0%1E7%11SERVICE-03DD96959737CBA3%150%150%1F10%13SERVICE_METHOD_GROUP-D96953CC279ECDB5%15SERVICE-166A78BA4824E1C0%151%150%1F%15;gf=all


- Problem 574 : Response time degradation : Major contributor is coming from Spring framework, method Thread.run
https://apm.cf.us10.hana.ondemand.com/e/f95fd477-ba09-47d0-ba91-d6f35a4e2f22/#methodhotspots;entityId=SERVICE-03DD96959737CBA3;timeframe=custom1579852560000to1579853940000;pid=8585138037107570574_1579852560000V2;servicefilter=0%1E10%11SERVICE_METHOD_GROUP-D96953CC279ECDB5;gf=all

Problem 305 : Multiple failures reason https://apm.cf.us10.hana.ondemand.com/e/f95fd477-ba09-47d0-ba91-d6f35a4e2f22/#problems/problemdetails;pid=2372315885784117305_1579851540000V2;gf=all

	1) 4XX errors from LLKAccountsService.Jar : 414 failed requests : Reason for failure "com.sap.smcs.livelink.common.exceptions.LiveLinkException / null"
	https://apm.cf.us10.hana.ondemand.com/e/f95fd477-ba09-47d0-ba91-d6f35a4e2f22/#failureanalysis;sci=SERVICE-F9537978C89A8F10;eventid=8277491140060454610_1579852620000;pid=2372315885784117305_1579851540000V2;timeframe=custom1579852620000to1579854540000;servicefilter=0%1E10%11SERVICE_METHOD_GROUP-B204392CDE709416%103%110;gf=all
	
	2) - 5XX llk-portal-ui : HTTP 502 - Bad Gateway : 16 requests failed
	
	https://apm.cf.us10.hana.ondemand.com/e/f95fd477-ba09-47d0-ba91-d6f35a4e2f22/#failureanalysis;sci=SERVICE-33CB19171A536AAE;eventid=3949845954514942833_1579852740000;pid=2372315885784117305_1579851540000V2;timeframe=custom1579852740000to1579854480000;servicefilter=0%1E10%11SERVICE_METHOD_GROUP-4930404F3D19489E%103%110;gf=all
	
	
	3) - 5XX error : HTTP 502 - Bad Gateway : 37 requests failed,  Outbound Traffic to unmonitored hosts failed with "No response".
	
	https://apm.cf.us10.hana.ondemand.com/e/f95fd477-ba09-47d0-ba91-d6f35a4e2f22/#failureanalysis;sci=SERVICE-5DE2F2F0B172AA53;eventid=5280510987325638107_1579852680000;pid=2372315885784117305_1579851540000V2;timeframe=custom1579852680000to1579854540000;servicefilter=0%1E10%11SERVICE_METHOD_GROUP-C53D958F99AD1D19%103%110;gf=all
	
	Summary : 
	
	46.4 % (+46.4 %) likely failed due to failed service calls to livelink.sapdigitalinterconnect.com:15910 with HTTP 502
	28.6 % (+28.6 %) likely failed due to failed service calls to llk-portal-ui on port * with HTTP 502
	25 % (+25 %) likely failed due to failed service calls to Outbound Traffic to unmonitored hosts with HTTP 502
	
	
Problem 836 : Related to People connect issue, 24 4XX error Bad requests.	
	
	
	
------------ 27 Jan 2020 ---------------


- Faced issue to start mongo docker on llbpal72 machine.

[root@llbpal72 moby]# docker ps -a | grep -i mongo
4ded3d1b3c35        mongo                                                                                               "docker-entrypoint.s…"    7 months ago         Exited (128) 2 days ago     0.0.0.0:28000->27017/tcp                             gracious_liskov
[root@llbpal72 moby]# docker ps -a | grep -i mongo
4ded3d1b3c35        mongo                                                                                               "docker-entrypoint.s…"    7 months ago        Exited (128) 2 days ago      0.0.0.0:28000->27017/tcp                             gracious_liskov
[root@llbpal72 moby]# docker start 4ded3d1b3c35
Error response from daemon: OCI runtime create failed: container with id exists: 4ded3d1b3c3549e45fbbf5bedd4a0a463b74a16d885187e4a70629a56e0f9267: unknown
Error: failed to start containers: 4ded3d1b3c35


----------------

[root@llbpal72 moby]# docker start b9d00f55bd7f
Error response from daemon: OCI runtime create failed: container_linux.go:346: starting container process caused "process_linux.go:297: applying cgroup configuration for process caused \"mkdir /sys/fs/cgroup/memory/kubepods/burstable/podbd8a4d6b-7045-11e9-b8a2-ac162dbc1dd0/b9d00f55bd7fe45071aa9a963581a8137c6b8e14c7baf49f29d62957f2be7297: cannot allocate memory\"": unknown
Error: failed to start containers: b9d00f55bd7f
[root@llbpal72 moby]# docker start b3a008e4f091
Error response from daemon: OCI runtime create failed: container_linux.go:346: starting container process caused "process_linux.go:297: applying cgroup configuration for process caused \"mkdir /sys/fs/cgroup/memory/kubepods/burstable/podd5f61b7f-6ce7-11e9-96e3-fc15b411265c/b3a008e4f091e7afc121924a2b2790bb82360654ef3bdf0b1cd39be680fa7cf9: cannot allocate memory\"": unknown
Error: failed to start containers: b3a008e4f091
[root@llbpal72 moby]# docker start 5ef9578d524a
Error response from daemon: OCI runtime create failed: container_linux.go:346: starting container process caused "process_linux.go:297: applying cgroup configuration for process caused \"mkdir /sys/fs/cgroup/memory/kubepods/besteffort/pod65d92ab7-7e73-11e9-b8a2-ac162dbc1dd0/5ef9578d524a09f9ade460144119576c43c6b52b93d1fffed296f36366647d73: cannot allocate memory\"": unknown
Error: failed to start containers: 5ef9578d524a
[root@llbpal72 moby]# docker start f8d92a39013a
Error response from daemon: OCI runtime create failed: container_linux.go:346: starting container process caused "process_linux.go:297: applying cgroup configuration for process caused \"mkdir /sys/fs/cgroup/memory/kubepods/besteffort/podd32fbbdf-6ce7-11e9-96e3-fc15b411265c/f8d92a39013a838680a7d395b45297f6a356b91c634e1220e08fb89ae3c37723: cannot allocate memory\"": unknown
Error: failed to start containers: f8d92a39013a
[root@llbpal72 moby]# docker start eb2904c0f01b
Error response from daemon: OCI runtime create failed: container_linux.go:346: starting container process caused "process_linux.go:297: applying cgroup configuration for process caused \"mkdir /sys/fs/cgroup/memory/kubepods/burstable/podc77c78e920f37a25771cb7f9c7108321/eb2904c0f01b9a13bb81849d7d7bc9da4adc40d4237d197313dd1f11fd8e3326: cannot allocate memory\"": unknown

	- Referred to these articles.
		https://github.com/moby/moby/issues/25270
		https://github.com/docker/for-linux/issues/412
		https://github.com/moby/moby/issues/18295
		https://github.com/moby/moby/pull/36249
		https://github.com/docker/for-linux/issues/643
		https://github.com/moby/moby/issues/38726
		
[root@llbpal72 ~]# docker run -d -p 28000:27017 -v ~/data:/home/achawla/UI/db:Z mongo --smallfiles
b6d49a8560e4f13c66d7ab5f58f9a1e72258b31a7f1f982a9462fd97ec6e8cdb
docker: Error response from daemon: OCI runtime create failed: container_linux.go:346: starting container process caused "process_linux.go:297: applying cgroup configuration for process caused \"mkdir /sys/fs/cgroup/memory/docker/b6d49a8560e4f13c66d7ab5f58f9a1e72258b31a7f1f982a9462fd97ec6e8cdb: cannot allocate memory\"": unknown.

- Analysing the logs files.


------- Worked on Rahul request to deploy CRM on production host.

		[root@ip-10-6-7-104 ~]# docker images | grep -i crm
		dbs-docker-repository.docker.repositories.sap.ondemand.com/crm-connector   1.1                 f51424f89fda        3 days ago          326MB
		dbs-docker-repository.docker.repositories.sap.ondemand.com/crm-connector   v1                  350c9891f929        2 months ago        327MB
		[root@ip-10-6-7-104 ~]# docker rmi f51424f89fda
		Error response from daemon: conflict: unable to delete f51424f89fda (must be forced) - image is being used by stopped container 1539181c993b
		[root@ip-10-6-7-104 ~]# docker rm 1539181c993b
		1539181c993b
		[root@ip-10-6-7-104 ~]# docker rmi f51424f89fda
		Untagged: dbs-docker-repository.docker.repositories.sap.ondemand.com/crm-connector:1.1
		Untagged: dbs-docker-repository.docker.repositories.sap.ondemand.com/crm-connector@sha256:2fbbc77b9ad185a1505e10e235b93fbecea3d58f69e26a8c31e6af2d5e892916
		Deleted: sha256:f51424f89fda35637040543b7d532bb6f26fcf259cdd44242f29d1aa1c5a3606
		Deleted: sha256:85a7664a79db4892d008fea84c6696a9d395ceed9918e89babe3b8baef237aa8
		[root@ip-10-6-7-104 ~]# docker login -u dbs-docker-user -p n813nZw22a2eVj4N https://dbs-docker-repository.docker.repositories.sap.ondemand.com
		WARNING! Using --password via the CLI is insecure. Use --password-stdin.
		WARNING! Your password will be stored unencrypted in /root/.docker/config.json.
		Configure a credential helper to remove this warning. See
		https://docs.docker.com/engine/reference/commandline/login/#credentials-store
		
		Login Succeeded
		[root@ip-10-6-7-104 ~]# docker run --add-host=365crm.phl.sap.corp:10.3.7.41 --name crm-connector -d -e "CRM_SECRET=VGg1c2VjOTJnYjQyJHR4MHdtMQ==" -e "CRM_USER=IT_APP_ADS1" -p 30100:8001 dbs-docker-repository.docker.repositories.sap.ondemand.com/crm-connector:1.1
		Unable to find image 'dbs-docker-repository.docker.repositories.sap.ondemand.com/crm-connector:1.1' locally
		1.1: Pulling from crm-connector
		7448db3b31eb: Already exists
		c36604fa7939: Already exists
		0c9bee0b691c: Already exists
		d3356d234a7a: Already exists
		3c6030466422: Already exists
		7b9710bca743: Already exists
		5c7280b5db98: Already exists
		2c5e2ad965af: Pull complete
		Digest: sha256:890a211d50688d2a7c60a5b29b324f11b7103bced3933d91864662d5cf6df28e
		Status: Downloaded newer image for dbs-docker-repository.docker.repositories.sap.ondemand.com/crm-connector:1.1
		992241a2413a833de49308d86db00f529853268de54244374204cd7083bb4635
		[root@ip-10-6-7-104 ~]# docker ps | grep -i crm
		992241a2413a        dbs-docker-repository.docker.repositories.sap.ondemand.com/crm-connector:1.1   "/bin/sh -c 'exec ja…"   8 seconds ago       Up 6 seconds        9001/tcp, 0.0.0.0:30100->8001/tcp   crm-connector
		
		
		-----------------------
		
		
		[ec2-user@ip-10-6-7-104 ~]$ docker ps | grep -i crm
		992241a2413a        dbs-docker-repository.docker.repositories.sap.ondemand.com/crm-connector:1.1   "/bin/sh -c 'exec ja…"   2 minutes ago       Up 2 minutes        9001/tcp, 0.0.0.0:30100->8001/tcp   crm-connector
		
		-------------------------
		
		
		2020-01-27 10:22:34.900  INFO 1 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped "{[/salesorder/checkstatus],methods=[GET],produces=[application/json]}" onto public java.util.List<com.sap.connector.xml.response.EntryStatusResponse> com.sap.connector.rest.CRMController.checkApprovalStatus(java.lang.String,java.lang.String) throws com.sap.connector.rest.exception.ConnectorException
		2020-01-27 10:22:34.902  INFO 1 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped "{[/salesorder/test],methods=[GET]}" onto public java.lang.String com.sap.connector.rest.CRMController.test()
		2020-01-27 10:22:34.908  INFO 1 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped "{[/error],produces=[text/html]}" onto public org.springframework.web.servlet.ModelAndView org.springframework.boot.autoconfigure.web.BasicErrorController.errorHtml(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)
		2020-01-27 10:22:34.915  INFO 1 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped "{[/error]}" onto public org.springframework.http.ResponseEntity<java.util.Map<java.lang.String, java.lang.Object>> org.springframework.boot.autoconfigure.web.BasicErrorController.error(javax.servlet.http.HttpServletRequest)
		2020-01-27 10:22:34.988  INFO 1 --- [           main] o.s.w.s.handler.SimpleUrlHandlerMapping  : Mapped URL path [/webjars/**] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]
		2020-01-27 10:22:34.994  INFO 1 --- [           main] o.s.w.s.handler.SimpleUrlHandlerMapping  : Mapped URL path [/**] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]
		2020-01-27 10:22:35.152  INFO 1 --- [           main] o.s.w.s.handler.SimpleUrlHandlerMapping  : Mapped URL path [/**/favicon.ico] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]
		2020-01-27 10:22:35.455  INFO 1 --- [           main] o.s.j.e.a.AnnotationMBeanExporter        : Registering beans for JMX exposure on startup
		2020-01-27 10:22:35.591  INFO 1 --- [           main] s.b.c.e.t.TomcatEmbeddedServletContainer : Tomcat started on port(s): 8001 (http)
		2020-01-27 10:22:35.601  INFO 1 --- [           main] com.sap.connector.SpringBootApplication  : Started SpringBootApplication in 8.129 seconds (JVM running for 9.099)
		2020-01-27 10:22:46.731  INFO 1 --- [nio-8001-exec-1] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring FrameworkServlet 'dispatcherServlet'
		2020-01-27 10:22:46.732  INFO 1 --- [nio-8001-exec-1] o.s.web.servlet.DispatcherServlet        : FrameworkServlet 'dispatcherServlet': initialization started
		2020-01-27 10:22:46.772  INFO 1 --- [nio-8001-exec-1] o.s.web.servlet.DispatcherServlet        : FrameworkServlet 'dispatcherServlet': initialization completed in 40 ms
		
		
--------- Fixed the github issue of Nidhi,

			STATUS		- Was getting that on Ansible Tower server.
			Failed
			EXPLANATION
			project_update failed for SAP Github with ID 108539
			
		[root@iad1ansible001 ~]# cat /etc/hosts| grep -i 160.210	- Fixed for Ansible, Nexus server.
		10.48.160.210 lsvxc001.pal.sap.corp lsvxc001
		10.48.160.210 github.wdf.sap.corp

		- on llbpal76 did following thing,
		
		[root@llbpal76 ~]# ps aux | grep -i 30739
		root     30739  0.0  0.0 183020  1424 ?        Ss   03:11   0:00 ssh -N -f -L llbpal76.pal.sap.corp:443:10.67.76.21:443 root@llbpal76
		


-----------	Worked on Protecode



------------ 28 Jan 2020 ---------------

- Worked on Protecode.
- Worked on Pankaj request of BDTrust.

- Worked on redis testing for QA team.

		master0:name=master01,status=ok,address=10.48.144.173:6381,slaves=1,sentinels=2
		127.0.0.1:26381> quit
		c5246840@llbpal55:~> redis-cli -a QL1d5bD -p 6381 DEBUG sleep 5
		OK
		c5246840@llbpal55:~> redis-cli -a QL1d5bD -p 6381 DEBUG sleep 5
		OK
		c5246840@llbpal55:~>  redis-cli -a QL1d5bD -p 6381 DEBUG sleep 120
		OK
		c5246840@llbpal55:~>

		c5246840@llbpal53:~>  redis-cli -a QL1d5bD -p 6381 DEBUG sleep 5
		OK
		c5246840@llbpal53:~>  redis-cli -a QL1d5bD -p 6381 DEBUG sleep 5
		OK
		c5246840@llbpal53:~> redis-cli -a QL1d5bD -p 6381 DEBUG sleep 120
		OK
		c5246840@llbpal53:~>
		
		
- Fixed protecode issue.
- Fixed reports us4ci report.


------------- 29 Jan 2020 -------------------
- Jan 29 06:41:52 us4llkapi002.bd.trust dbs-docker-repository.docker.repositories.sap.ondemand.com/llk-callback-manager: 1.9.4.19/llk-callback-manager/34e468b218b5[1801]:                 o.apache.http.impl.execchain.RetryExec   :  I/O exception (java.net.SocketException) caught when processing request to {s}->https://api.paypal.com:443: Connection reset
Jan 29 06:41:52 us4llkapi002.bd.trust dbs-docker-repository.docker.repositories.sap.ondemand.com/llk-callback-manager: 1.9.4.19/llk-callback-manager/34e468b218b5[1801]:                 o.apache.http.impl.execchain.RetryExec   :  I/O exception (java.net.SocketException) caught when processing request to {s}->https://api.paypal.com:443: Connection reset
Jan 29 06:41:52 us4llkapi002.bd.trust dbs-docker-repository.docker.repositories.sap.ondemand.com/llk-callback-manager: 1.9.4.19/llk-callback-manager/34e468b218b5[1801]:                 o.apache.http.impl.execchain.RetryExec   :  I/O exception (java.net.SocketException) caught when processing request to {s}->https://api.paypal.com:443: Connection reset
Jan 29 06:42:28 us4llkapi003.bd.trust llk-callback-manager/8f2d3de7061e[43066]:                 o.apache.http.impl.execchain.RetryExec   :  I/O exception (java.net.SocketException) caught when processing request to {s}->https://api.paypal.com:443: Connection reset
Jan 29 06:42:28 us4llkapi003.bd.trust llk-callback-manager/8f2d3de7061e[43066]:                 o.apache.http.impl.execchain.RetryExec   :  I/O exception (java.net.SocketException) caught when processing request to {s}->https://api.paypal.com:443: Connection reset
Jan 29 06:42:35 us4llkapi001.bd.trust dbs-docker-repository.docker.repositories.sap.ondemand.com/llk-callback-manager: 1.9.4.19/llk-callback-manager/20eddf551a10[1798]:                 o.apache.http.impl.execchain.RetryExec   :  I/O exception (java.net.SocketException) caught when processing request to {s}->https://api.paypal.com:443: Connection reset
Jan 29 06:42:35 us4llkapi001.bd.trust dbs-docker-repository.docker.repositories.sap.ondemand.com/llk-callback-manager: 1.9.4.19/llk-callback-manager/20eddf551a10[1798]:                 o.apache.http.impl.execchain.RetryExec   :  I/O exception (java.net.SocketException) caught when processing request to {s}->https://api.paypal.com:443: Connection reset
Jan 29 06:42:35 us4llkapi001.bd.trust dbs-docker-repository.docker.repositories.sap.ondemand.com/llk-callback-manager: 1.9.4.19/llk-callback-manager/20eddf551a10[1798]:                 o.apache.http.impl.execchain.RetryExec   :  I/O exception (java.net.SocketException) caught when processing request to {s}->https://api.paypal.com:443: Connection reset


Jan 29 06:45:21 us4llkapi003.bd.trust llk-callback-manager/8f2d3de7061e[43066]:                 o.apache.http.impl.execchain.RetryExec   :  I/O exception (java.net.SocketException) caught when processing request to {s}->https://api.paypal.com:443: Connection reset
Jan 29 06:45:21 us4llkapi003.bd.trust llk-callback-manager/8f2d3de7061e[43066]:                 o.apache.http.impl.execchain.RetryExec   :  I/O exception (java.net.SocketException) caught when processing request to {s}->https://api.paypal.com:443: Connection reset
Jan 29 06:45:21 us4llkapi003.bd.trust llk-callback-manager/8f2d3de7061e[43066]:                 o.apache.http.impl.execchain.RetryExec   :  I/O exception (java.net.SocketException) caught when processing request to {s}->https://api.paypal.com:443: Connection reset
Jan 29 06:45:21 us4llkapi003.bd.trust llk-callback-manager/8f2d3de7061e[43066]:                 o.apache.http.impl.execchain.RetryExec   :  I/O exception (java.net.SocketException) caught when processing request to {s}->https://api.paypal.com:443: Connection reset
Jan 29 06:45:21 us4llkapi003.bd.trust llk-callback-manager/8f2d3de7061e[43066]:                 o.apache.http.impl.execchain.RetryExec   :  I/O exception (java.net.SocketException) caught when processing request to {s}->https://api.paypal.com:443: Connection reset
Jan 29 06:45:24 us4llkapi003.bd.trust llk-callback-manager/8f2d3de7061e[43066]:                 o.apache.http.impl.execchain.RetryExec   :  I/O exception (java.net.SocketException) caught when processing request to {s}->https://api.paypal.com:443: Connection reset
Jan 29 06:45:24 us4llkapi003.bd.trust llk-callback-manager/8f2d3de7061e[43066]:                 o.apache.http.impl.execchain.RetryExec   :  I/O exception (java.net.SocketException) caught when processing request to {s}->https://api.paypal.com:443: Connection reset
Jan 29 06:45:26 us4llkapi003.bd.trust llk-callback-manager/8f2d3de7061e[43066]:                 o.apache.http.impl.execchain.RetryExec   :  I/O exception (java.net.SocketException) caught when processing request to {s}->https://api.paypal.com:443: Connection reset
Jan 29 06:45:26 us4llkapi003.bd.trust llk-callback-manager/8f2d3de7061e[43066]:                 o.apache.http.impl.execchain.RetryExec   :  I/O exception (java.net.SocketException) caught when processing request to {s}->https://api.paypal.com:443: Connection reset
Jan 29 06:45:26 us4llkapi003.bd.trust llk-callback-manager/8f2d3de7061e[43066]:                 o.apache.http.impl.execchain.RetryExec   :  I/O exception (java.net.SocketException) caught when processing request to {s}->https://api.paypal.com:443: Connection reset
Jan 29 06:45:26 us4llkapi003.bd.trust llk-callback-manager/8f2d3de7061e[43066]:                 o.apache.http.impl.execchain.RetryExec   :  I/O exception (java.net.SocketException) caught when processing request to {s}->https://api.paypal.com:443: Connection reset


---- Creating a protecode script

[root@llbpal76 protecode-docker-images]# cat protecode.sh
#!/bin/bash

echo ${bamboo.PROTECODE_DOCKER_PATH}
echo ${bamboo.service}
echo ${bamboo.version}

#curl -k -v -H "Authorization: Basic YzUyNDY4NDA6Tm92ZW1iZSFAMzQ=" -H 'Group: 901' -T "/${bamboo.PROTECODE_DOCKER_PATH}/${bamboo.service}:${bamboo.version}.tar" https://protecode.c.eu-de-2.cloud.sap/api/upload/

curl -k -v -H "Authorization: Basic YzUyNDY4NDA6Tm92ZW1iZSFAMzQ=" -H 'Group: 901' -T "./${1}:${2}.tar" https://protecode.c.eu-de-2.cloud.sap/api/upload/

		
		
########## ./protecode.sh llk-analytics 2.0.1.21-SNAPSHOT

-----------------------------------------------------------------

us4llkextweb001:9999
us4llkextweb002:9999

- Made changes in Build plan for Protecode.

ProteCode Applied Build Plans List :

1) AAT :	aat-api
			aat-frontend-reverse-proxy
			aat-ui
			
			
2) CTOOL: 	ctool-api			

3) ENG:		2gen-frontend-reverse-proxy
			keywords-manager
			llk-engagement
			llk-engagement-callback
			scheduler-service
			second-gen-engagement
			subscriber-list-manager
			
4) Livelink: livelink-springboot-admin-service
			llk-accounts-service-v2
			llk-admin-portal
			llk-analytics
			llk-apigateway
			llk-archival-frontend-reverse-proxy
			llk-archival-service
			llk-callback-manager
			llk-docs
			llk-eventlog-loader
			llk-frontend-reverse-proxy
			llk-integrate-portal
			llk-portal-ui
			llk-portal-ui-2geng
			llk-sms-channel
			llk-sms-channel-mo-ack
			llk-sms-channel-mock
			llk-sms-eventstore
			llk-ui-gateway
			

------------- 30 Jan 2020 -------------------			

- Resolved Nikita issues of Demo and Prod logs access.

5) MFA:		mfa-api
			mfa-docs
			mfa-frontend-reverse-proxy
			mfa-ui
			
			
6) PPC:		ppc-aat-client
			ppc-be-aws
			ppc-docs
			ppc-hr-interface
			ppc-ui
			
		
7) RCS:		rcs-hub
			
			
8) SMS:		sms-elections-api
			sms-elections-ui



{"access_token":"eyJhbGciOiJSUzI1NiJ9.eyJjbGFpbWFudCI6eyJpZCI6NDAsInNzb0lkIjoiQzUyNDY4NDAiLCJmaXJzdE5hbWUiOiJTYWNoaW4iLCJsYXN0TmFtZSI6Iktlc2Fya2FyIiwiZW1haWwiOiJzYWNoaW4ua2VzYXJrYXJAc2FwLmNvbSIsInNjb3BlIjoibGl2ZWxpbms6dWk6YXdzIiwicm9sZUNvZGVzIjpbIkVFX0FETUlOIiwiUENfVklFV0VSIiwiUklTS19XUklURSIsIjJGQV9BRE1JTl9ST0xFIiwiUklTS19SRUFEIiwiUENfQURNSU4iLCJMSVZFTElOS19BQ0NPVU5UX0FETUlOIiwiTElWRUxJTktfQURNSU4iLCJQQ19PUEVSQVRPUiIsIkFBX0FETUlOIiwiU0FQX0FETUlOIl19LCJpc3MiOiJTQ0lfQVVUSEVOVElDQVRJT05fQVVUSE9SSVRZIiwianRpIjoiUFlBNzU0ZFJjSTAwT0M5TkNFczBxVGJzZnBPeWRiOXciLCJleHAiOjE1ODAzNzE2MTd9.fDMrh5DXLHL035sn1UotElMJEdnA6rxpl2S7We1xcBCJ-Gb5lg9HQbMJ60XgQK6vbMpaDTFl-72_yG6S0l7eSRr4oET7NNrFECu4W5QqXIcELkmHxw_A5FYtuwazB7QyCo_DiE0WW3R_8d-HhkoOYlXsrH3-T3Fz1yecEtNKfnTiKaKRTf9u5hBzy8-d_MQLnLhb7XbxziBoll_vhVc05kZaLfTYOlDdlN3rHyk4FQ4885HZBr9v6Rxb-MahryN4Hz1E67M9UgMJEb3h9ARrNOP1Cl-rsFmGbi7ZJgOUG0BRfSgZgh1Lx3ueh0OXETRZq26zCUkOctbc0vBCi_tflQ","token_type":"Bearer","expires_in":2700000,"refresh_token":"gRS7JYekO8B51AIU","scope":"livelink:ui:aws","refresh_expires_in":3600000}

curl -X GET -H 'Cookie: LIVELINK="eyJhbGciOiJSUzI1NiJ9.eyJjbGFpbWFudCI6eyJpZCI6NDAsInNzb0lkIjoiQzUyNDY4NDAiLCJmaXJzdE5hbWUiOiJTYWNoaW4iLCJsYXN0TmFtZSI6Iktlc2Fya2FyIiwiZW1haWwiOiJzYWNoaW4ua2VzYXJrYXJAc2FwLmNvbSIsInNjb3BlIjoibGl2ZWxpbms6dWk6YXdzIiwicm9sZUNvZGVzIjpbIkVFX0FETUlOIiwiUENfVklFV0VSIiwiUklTS19XUklURSIsIjJGQV9BRE1JTl9ST0xFIiwiUklTS19SRUFEIiwiUENfQURNSU4iLCJMSVZFTElOS19BQ0NPVU5UX0FETUlOIiwiTElWRUxJTktfQURNSU4iLCJQQ19PUEVSQVRPUiIsIkFBX0FETUlOIiwiU0FQX0FETUlOIl19LCJpc3MiOiJTQ0lfQVVUSEVOVElDQVRJT05fQVVUSE9SSVRZIiwianRpIjoiUFlBNzU0ZFJjSTAwT0M5TkNFczBxVGJzZnBPeWRiOXciLCJleHAiOjE1ODAzNzE2MTd9.fDMrh5DXLHL035sn1UotElMJEdnA6rxpl2S7We1xcBCJ-Gb5lg9HQbMJ60XgQK6vbMpaDTFl-72_yG6S0l7eSRr4oET7NNrFECu4W5QqXIcELkmHxw_A5FYtuwazB7QyCo_DiE0WW3R_8d-HhkoOYlXsrH3-T3Fz1yecEtNKfnTiKaKRTf9u5hBzy8-d_MQLnLhb7XbxziBoll_vhVc05kZaLfTYOlDdlN3rHyk4FQ4885HZBr9v6Rxb-MahryN4Hz1E67M9UgMJEb3h9ARrNOP1Cl-rsFmGbi7ZJgOUG0BRfSgZgh1Lx3ueh0OXETRZq26zCUkOctbc0vBCi_tflQ"' -H 'Content-Type: application/json' http://localhost:15320/api/livelink/clients


{"access_token":"eyJhbGciOiJSUzI1NiJ9.eyJjbGFpbWFudCI6eyJpZCI6NDAsInNzb0lkIjoiQzUyNDY4NDAiLCJmaXJzdE5hbWUiOiJTYWNoaW4iLCJsYXN0TmFtZSI6Iktlc2Fya2FyIiwiZW1haWwiOiJzYWNoaW4ua2VzYXJrYXJAc2FwLmNvbSIsInNjb3BlIjoibGl2ZWxpbms6dWk6YXdzIiwicm9sZUNvZGVzIjpbIkVFX0FETUlOIiwiUENfVklFV0VSIiwiUklTS19XUklURSIsIjJGQV9BRE1JTl9ST0xFIiwiUklTS19SRUFEIiwiUENfQURNSU4iLCJMSVZFTElOS19BQ0NPVU5UX0FETUlOIiwiTElWRUxJTktfQURNSU4iLCJQQ19PUEVSQVRPUiIsIkFBX0FETUlOIiwiU0FQX0FETUlOIl19LCJpc3MiOiJTQ0lfQVVUSEVOVElDQVRJT05fQVVUSE9SSVRZIiwianRpIjoiUFlBNzU0ZFJjSTAwT0M5TkNFczBxVGJzZnBPeWRiOXciLCJleHAiOjE1ODAzNzI2NjR9.rI7gOZFCxgZCVwbz0ZQANC3H5zI8YdCFQcbT93H8CUUMo7UOVPZsxYPfeyBomQwpR26X8O50gLeUTMHdDz1qyDxIynPl5VeWWU-Qc09C4g5qmHxUL32lu3C9V6BDzcyT031Q73cx1p5ZC8vXpcYg5wL_cGE3wQuopzkfR2PDCRBhoMa68i-Vd5vR0OUWg79qI1XJ-CpyjI8Y8K7rpVeaQhR-QsS9hWxkjUiwA4D6oCTNymJkkhtB2M4DSzsKZPtPLvzcGah0Fn83_YOpVYzunJ0-0vwwy5HtIVs5Dzq9c340XD6s2ZBOvlvQtXfmh9p3blifDb31GK6lGw5T3f3SxQ","token_type":"Bearer","expires_in":1652839,"refresh_token":"gRS7JYekO8B51AIU","scope":"livelink:ui:aws","refresh_expires_in":2552839}


curl -X GET -H 'Cookie: LIVELINK=eyJhbGciOiJSUzI1NiJ9.eyJjbGFpbWFudCI6eyJpZCI6NDAsInNzb0lkIjoiQzUyNDY4NDAiLCJmaXJzdE5hbWUiOiJTYWNoaW4iLCJsYXN0TmFtZSI6Iktlc2Fya2FyIiwiZW1haWwiOiJzYWNoaW4ua2VzYXJrYXJAc2FwLmNvbSIsInNjb3BlIjoibGl2ZWxpbms6dWk6YXdzIiwicm9sZUNvZGVzIjpbIkVFX0FETUlOIiwiUENfVklFV0VSIiwiUklTS19XUklURSIsIjJGQV9BRE1JTl9ST0xFIiwiUklTS19SRUFEIiwiUENfQURNSU4iLCJMSVZFTElOS19BQ0NPVU5UX0FETUlOIiwiTElWRUxJTktfQURNSU4iLCJQQ19PUEVSQVRPUiIsIkFBX0FETUlOIiwiU0FQX0FETUlOIl19LCJpc3MiOiJTQ0lfQVVUSEVOVElDQVRJT05fQVVUSE9SSVRZIiwianRpIjoiUFlBNzU0ZFJjSTAwT0M5TkNFczBxVGJzZnBPeWRiOXciLCJleHAiOjE1ODAzNzI2NjR9.rI7gOZFCxgZCVwbz0ZQANC3H5zI8YdCFQcbT93H8CUUMo7UOVPZsxYPfeyBomQwpR26X8O50gLeUTMHdDz1qyDxIynPl5VeWWU-Qc09C4g5qmHxUL32lu3C9V6BDzcyT031Q73cx1p5ZC8vXpcYg5wL_cGE3wQuopzkfR2PDCRBhoMa68i-Vd5vR0OUWg79qI1XJ-CpyjI8Y8K7rpVeaQhR-QsS9hWxkjUiwA4D6oCTNymJkkhtB2M4DSzsKZPtPLvzcGah0Fn83_YOpVYzunJ0-0vwwy5HtIVs5Dzq9c340XD6s2ZBOvlvQtXfmh9p3blifDb31GK6lGw5T3f3SxQ' -H 'Content-Type: application/json' http://localhost:15320/api/livelink/clients

------------------

Modified the script for protecode as below : 

[root@llbpal76 protecode-docker-images]# cat protecode.sh
#!/bin/bash

curl -k -v -H "Authorization: Basic YzUyNDY4NDA6Tm92ZW1iZSFAMzQ=" -H 'Group: 901' -T "/protecode-docker-images/${1}:${2}.tar" https://protecode.c.eu-de-2.cloud.sap/api/upload/
rm -rf /protecode-docker-images/${1}:${2}.tar


Bamboo file name : 

/${bamboo.PROTECODE_DOCKER_PATH}/${bamboo.service}:${bamboo.version}.tar

rm -rf /protecode-docker-images/${bamboo.service}:${bamboo.version}.tar

master = 1
-----------------

PO1)(*2020


[ec2-user@ip-10-6-7-104 ~]$  curl -k --location --request GET "http://localhost:30100/salesorder/readByOrderId?orderId=5020554" --header 'Content-Type: application/json' --header 'Crm-Domain: https://365crm.phl.sap.corp' --header 'Authorization: Basic <TOKEN>'
{"type":"application/xml","properties":{"orderOwner":"I866915","preDecObjectId":"80007384","effectiveDate":"2020-01-31T00:00:00","orderReason":"003","orderId":"5020554","annualRevenue":"50000","guid":"3ca82a12-b4fc-1eea-88d0-9e098b98b0ac","processType":"ZSO1","description":"80007384-1","createdAt":"2019-12-19T17:28:30","createdBy":"I866915","changedAt":"2020-01-20T21:26:24","changedBy":"I866915","status":"E0001","statusTxt":"Initial Draft","preDecObjectGuid":"3ca82a12-b4fc-1eea-88d0-9942c995d07d","revenueType":"010","createdByFullName":"James Morgan","changedByFullName":"James Morgan","orderOwnerFullName":"James Morgan"}}
----
curl -k --location --request GET "https://internal-crm-connector-alb-291779376.us-east-1.elb.amazonaws.com/salesorder/readByOrderId?orderId=5020554" --header 'Content-Type: application/json' --header 'Crm-Domain: https://10.3.7.41' --header 'Authorization: Basic <TOKEN>'
curl: (7) Failed connect to internal-crm-connector-alb-291779376.us-east-1.elb.amazonaws.com:443; Connection refused
----

Call with PAblo : 40 minutes.
Call with Khalid : 1 hour.
	- Docker image.
	- livelink integration.
	- https communication for ui.

- Worked on rectifying the US4CI report.

https://wiki.wdf.sap.corp/wiki/pages/viewpage.action?pageId=2002940434





9:56
https://wiki.wdf.sap.corp/wiki/display/osssec/Notification
9:56
https://wiki.wdf.sap.corp/wiki/display/osssec/Assessment


------------- 31 Jan 2020 -------------------	

- Nidhi issue,
	https://lvpal408.pal.sap.corp:9445/browse/LIV-LLKUIP9-35/log
	
	-- Found dev build is also failing. https://lvpal408.pal.sap.corp:9445/browse/LIV-LLKUIP0 , Last 4 builds are failing

	https://lvpal408.pal.sap.corp:9445/download/LIV-LLKUIP9-JOB1/build_logs/LIV-LLKUIP9-JOB1-35.log
	
	
- PPC on QA,

	cd /opt/ppsadm/tomcat/apache-tomcat-9.0.22/bin/
	cd /opt/ppsadm/PPS-BE/people-connect/bin
	
	 /opt/appdata/java/jdk1.8.0_101/bin/java -Duser.dir=/opt/ppsadm/PPS-BE/people-connect -Dconfig.file=/opt/ppsadm/PPS-BE/people-connect/bin/../conf/environment.conf -Dpeople.connect.version=2.0.1.89-SNAPSHOT -Djava.net.preferIPv4Stack=true -Djgroups.tcp.address=10.48.144.175 10.48.144.175 -Djgroups.mping.mcast_addr=224.0.0.1 -Dpidfile.path=/dev/null -cp /opt/ppsadm/PPS-BE/people-connect/lib/* play.core.server.ProdServerStart
	 
  994  cd tomcat/apache-tomcat-9.0.22/bin/
  995  ./startup.sh
  996  history
  997  /opt/ppsadm/PPS-BE/people-connect/bin/people-connect &
  

ppsadm@llbpal55:~/PPS-BE/people-connect/bin> ps aux | grep -i people-connect
ppsadm   21998  0.0  0.0  10536   952 pts/15   R+   22:52   0:00 grep --color=auto -i people-connect
ppsadm   59560  1.5  3.2 26361172 1062816 ?    Sl   11:54  10:09 /opt/appdata/java/jdk1.8.0_101/bin/java -Duser.dir=/opt/ppsadm/PPS-BE/people-connect -Dconfig.file=/opt/ppsadm/PPS-BE/people-connect/bin/../conf/environment.conf -Dpeople.connect.version=2.0.1.89-SNAPSHOT -Djava.net.preferIPv4Stack=true -Djgroups.tcp.address=10.48.144.175 10.48.144.175 -Djgroups.mping.mcast_addr=224.0.0.1 -Dpidfile.path=/dev/null -cp /opt/ppsadm/PPS-BE/people-connect/lib/* play.core.server.ProdServerStart

llbpal55:~ # tail -f /var/log/messages | grep -i 6283f946f856
2020-01-30T23:03:05-08:00 llbpal55.pal.sap.corp ppc-ui/6283f946f856[3082]: 2020/01/31 07:03:05 [error] 11#11: *9634 connect() failed (111: Connection refused) while connecting to upstream, client: 10.97.10.133, server: , request: "GET /login HTTP/1.1", upstream: "http://10.48.144.175:11700/aat/login", host: "llbpal55.pal.sap.corp:11200"
2020-01-30T23:03:05-08:00 llbpal55.pal.sap.corp ppc-ui/6283f946f856[3082]: 10.97.10.133 - - [31/Jan/2020:07:03:05 +0000] "GET /login HTTP/1.1" 502 581 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36"
^C

llbpal55:/opt/ppsadm/tomcat/apache-tomcat-9.0.22/logs # tail -f localhost_access_log..2020-01-29.txt
172.17.0.18 - - [29/Jan/2020:03:32:36 -0800] "GET /aat/login HTTP/1.1" 302 -
172.17.0.18 - - [29/Jan/2020:03:32:39 -0800] "POST /aat/response HTTP/1.1" 302 -
172.17.0.18 - - [29/Jan/2020:03:33:13 -0800] "GET /aat/login HTTP/1.1" 302 -
172.17.0.18 - - [29/Jan/2020:03:33:17 -0800] "POST /aat/response HTTP/1.1" 302 -
172.17.0.18 - - [29/Jan/2020:03:33:49 -0800] "GET /aat/login HTTP/1.1" 302 -
172.17.0.18 - - [29/Jan/2020:03:33:53 -0800] "POST /aat/response HTTP/1.1" 302 -
172.17.0.18 - - [29/Jan/2020:03:35:02 -0800] "GET /aat/login HTTP/1.1" 302 -
172.17.0.18 - - [29/Jan/2020:03:35:06 -0800] "POST /aat/response HTTP/1.1" 302 -
172.17.0.18 - - [29/Jan/2020:03:37:36 -0800] "GET /aat/login HTTP/1.1" 302 -
172.17.0.18 - - [29/Jan/2020:03:37:40 -0800] "POST /aat/response HTTP/1.1" 302 -

 944  cd tomcat/apache-tomcat-9.0.22/bin/
  945  ./startup.sh
  946  history
  947  /opt/ppsadm/PPS-BE/people-connect/bin/people-connect &


-rw-r----- 1 ppsadm ppsadm   2156 Jan 27 23:54 localhost_access_log..2020-01-27.txt
-rwxrwxr-x 1 ppsadm ppsadm 490952 Jan 28 02:45 catalina.out
-rw-r----- 1 ppsadm ppsadm  42614 Jan 28 20:13 localhost_access_log..2020-01-28.txt
-rw-r----- 1 ppsadm ppsadm  16432 Jan 29 03:37 localhost_access_log..2020-01-29.txt



ppsadm@llbpal55:~/tomcat/apache-tomcat-9.0.22/bin> ./shutdown.sh
Using CATALINA_BASE:   /opt/ppsadm/tomcat/apache-tomcat-9.0.22
Using CATALINA_HOME:   /opt/ppsadm/tomcat/apache-tomcat-9.0.22
Using CATALINA_TMPDIR: /opt/ppsadm/tomcat/apache-tomcat-9.0.22/temp
Using JRE_HOME:        /opt/appdata/java/jdk1.8.0_101/jre/
Using CLASSPATH:       /opt/ppsadm/tomcat/apache-tomcat-9.0.22/bin/bootstrap.jar:/opt/ppsadm/tomcat/apache-tomcat-9.0.22/bin/tomcat-juli.jar
Jan 30, 2020 11:18:00 PM org.apache.catalina.startup.Catalina stopServer
SEVERE: Could not contact [localhost:11705] (base port [11705] and offset [0]). Tomcat may not be running.
Jan 30, 2020 11:18:00 PM org.apache.catalina.startup.Catalina stopServer
SEVERE: Error stopping Catalina
java.net.ConnectException: Connection refused
        at java.net.PlainSocketImpl.socketConnect(Native Method)
        at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
        at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
        at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
        at java.net.Socket.connect(Socket.java:589)
        at java.net.Socket.connect(Socket.java:538)
        at java.net.Socket.<init>(Socket.java:434)
        at java.net.Socket.<init>(Socket.java:211)
        at org.apache.catalina.startup.Catalina.stopServer(Catalina.java:513)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.catalina.startup.Bootstrap.stopServer(Bootstrap.java:390)
        at org.apache.catalina.startup.Bootstrap.main(Bootstrap.java:480)


ppsadm@llbpal55:~/tomcat/apache-tomcat-9.0.22/bin> tail -f ../logs/catalina.out
28-Jan-2020 01:48:06.687 DEBUG [PPS] HttpAccessor: Created POST request for "https://llbpal55.pal.sap.corp:14500/auth/refresh"
28-Jan-2020 01:48:06.688 DEBUG [PPS] RestTemplate$AcceptHeaderRequestCallback: Setting request Accept header to [application/xml, text/xml, application/json, application/*+xml, application/*+json]
28-Jan-2020 01:48:06.689 DEBUG [PPS] RestTemplate$HttpEntityRequestCallback: Writing [com.sap.sci365.authenticationauthority.architecture.entities.TokenRefresh@75b28358] as "application/json" using [org.springframework.http.converter.json.MappingJackson2HttpMessageConverter@1bac9783]
28-Jan-2020 01:48:06.755 DEBUG [PPS] RestTemplate: POST request for "https://llbpal55.pal.sap.corp:14500/auth/refresh" resulted in 200 (null)
28-Jan-2020 01:48:06.756 DEBUG [PPS] HttpMessageConverterExtractor: Reading [class com.sap.sci365.authenticationauthority.architecture.entities.RequestResult] as "application/json;charset=UTF-8" using [org.springframework.http.converter.json.MappingJackson2HttpMessageConverter@1bac9783]
28-Jan-2020 02:45:06.761 DEBUG [PPS] HttpAccessor: Created POST request for "https://llbpal55.pal.sap.corp:14500/auth/refresh"
28-Jan-2020 02:45:06.812 DEBUG [PPS] RestTemplate$AcceptHeaderRequestCallback: Setting request Accept header to [application/xml, text/xml, application/json, application/*+xml, application/*+json]
28-Jan-2020 02:45:06.812 DEBUG [PPS] RestTemplate$HttpEntityRequestCallback: Writing [com.sap.sci365.authenticationauthority.architecture.entities.TokenRefresh@42a4837a] as "application/json" using [org.springframework.http.converter.json.MappingJackson2HttpMessageConverter@1bac9783]
28-Jan-2020 02:45:07.068 DEBUG [PPS] RestTemplate: POST request for "https://llbpal55.pal.sap.corp:14500/auth/refresh" resulted in 200 (null)
28-Jan-2020 02:45:07.069 DEBUG [PPS] HttpMessageConverterExtractor: Reading [class com.sap.sci365.authenticationauthority.architecture.entities.RequestResult] as "application/json;charset=UTF-8" using [org.springframework.http.converter.json.MappingJackson2HttpMessageConverter@1bac9783]

Protecode password : ProtePass!@34

1) isProtecode		

Authentication, ProtecodeGrpId : 917 
Livelink,		ProtecodeGrpId : 901
PeopleConnect,	ProtecodeGrpId : 916

${bamboo.ProtecodeGrpId}

2) add script.

#!/bin/bash

echo ${bamboo.service}
echo ${bamboo.version}

if [[ "${bamboo.isProtecode}" -eq 1 ]]; then
/protecode-docker-images/protecode.sh ${bamboo.service} ${bamboo.version} ${bamboo.ProtecodeGrpId}
fi

rm -rf /protecode-docker-images/${bamboo.service}:${bamboo.version}.tar


3) override isProtecode var

curl -k -S -u bot_901@protecode-sc.local:ProtePass!@34 --max-time 10800 https://svmprodzdohvhnx0v.int.sap.eu2.hana.ondemand.com/SVM/services/xsjs/assessments/download.xsjs?format=mavenVulas >> vulas-exemptions.properties

6) PPC:		ppc-aat-client
			ppc-be-aws
			ppc-docs
			ppc-hr-interface
			ppc-ui

1) AAT :	aat-api
			aat-frontend-reverse-proxy
			aat-ui
			
			
2) CTOOL: 	ctool-api			

3) ENG:		2gen-frontend-reverse-proxy
			keywords-manager
			llk-engagement
			llk-engagement-callback
			scheduler-service
			second-gen-engagement
			subscriber-list-manager
			
4) Livelink: livelink-springboot-admin-service
			llk-accounts-service-v2
			llk-admin-portal
			llk-analytics
			llk-apigateway
			llk-archival-frontend-reverse-proxy
			llk-archival-service
			llk-callback-manager
			llk-docs
			llk-eventlog-loader
			llk-frontend-reverse-proxy
			llk-integrate-portal
			llk-portal-ui
			llk-portal-ui-2geng
			llk-sms-channel
			llk-sms-channel-mo-ack
			llk-sms-channel-mock
			llk-sms-eventstore
			llk-ui-gateway


---- AAT

  54  k get secret aat-api-secret -oyaml -n aat > decode.txt
   55  ls
   56  base64 --decode decode.txt > decode.txt
   
   61  k get cm llk-accounts-services-cm -oyaml | grep AAT
   62  k get cm llk-accounts-service-cm -oyaml | grep AAT



  AAT_PRIVATE_KEY_PATH: "/opt/aatadm/config/dev/private_key.der"
  AAT_PUBLIC_KEY_PATH: "/opt/aatadm/config/dev/public_key.der"


/opt/aatadm/config/dev $ ls
cloudIdentityAPI.jks  keystore-ssl.jks      keystore.jks          private_key.der       public_key.der        sap_metadata.xml

  AAT_TOKEN_GENERATE_URL: https://aat-service:14500/oauth/token
  AAT_TOKEN_INVALIDATE_URL: https://aat-service:14500/oauth/token/invalidate
  AAT_TOKEN_VALIDATE_URL: https://aat-service:14500/oauth/token/validate


kubectl exec -it aat-api-deployment-66454b4647-bzpjq sh -n aat


[dsilva@ip-198-19-11-212 tmp]$ openssl s_client -connect https://aa8fd7ab-aat-aatingress-837a-1461999413.us-east-1.elb.amazonaws.com
getaddrinfo: Servname not supported for ai_socktype
connect:errno=0

I later on copied the .der certificate locally from kubectl pod and tried to convert it but getting error.

[dsilva@ip-198-19-11-212 tmp]$ openssl x509 -inform der -in aat_public_key.der -out certificate.pem
unable to load certificate
139663895709584:error:0D0680A8:asn1 encoding routines:ASN1_CHECK_TLEN:wrong tag:tasn_dec.c:1220:
139663895709584:error:0D06C03A:asn1 encoding routines:ASN1_D2I_EX_PRIMITIVE:nested asn1 error:tasn_dec.c:788:
139663895709584:error:0D08303A:asn1 encoding routines:ASN1_TEMPLATE_NOEXP_D2I:nested asn1 error:tasn_dec.c:720:Field=serialNumber, Type=X509_CINF
139663895709584:error:0D08303A:asn1 encoding routines:ASN1_TEMPLATE_NOEXP_D2I:nested asn1 error:tasn_dec.c:720:Field=cert_info, Type=X509



---------- 2 Feb 2020 ---------------
- Bamboo issue.

2020-02-02 23:04:55,087 ERROR [https-jsse-nio-9445-exec-1] [BambooAuthenticator] User repository communication failed:
com.atlassian.crowd.exception.runtime.CommunicationException
        at com.atlassian.crowd.embedded.core.CrowdServiceImpl.convertOperationFailedException(CrowdServiceImpl.java:673)
        at com.atlassian.crowd.embedded.core.CrowdServiceImpl.authenticate(CrowdServiceImpl.java:76)
        at sun.reflect.GeneratedMethodAccessor3222.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:343)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:197)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
        at org.springframework.transaction.interceptor.TransactionAspectSupport.invokeWithinTransaction(TransactionAspectSupport.java:294)
        at org.springframework.transaction.interceptor.TransactionInterceptor.invoke(TransactionInterceptor.java:98)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:185)
        at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:212)
        at com.sun.proxy.$Proxy170.authenticate(Unknown Source)
        at com.atlassian.crowd.embedded.atlassianuser.EmbeddedCrowdAuthenticator.authenticate(EmbeddedCrowdAuthenticator.java:24)
        at com.atlassian.crowd.embedded.atlassianuser.EmbeddedCrowdAuthenticator$$FastClassBySpringCGLIB$$af97c51b.invoke(<generated>)
        at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:204)
        at org.springframework.aop.framework.CglibAopProxy$DynamicAdvisedInterceptor.intercept(CglibAopProxy.java:684)
        at com.atlassian.crowd.embedded.atlassianuser.EmbeddedCrowdAuthenticator$$EnhancerBySpringCGLIB$$cb218de2.authenticate(<generated>)
        at bucket.user.DefaultUserAccessor.authenticate(DefaultUserAccessor.java:711)
        at sun.reflect.GeneratedMethodAccessor3221.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:343)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:197)
		
		
		
----------- 4 Feb 2020 ---------------
- Completed the protecode code for all except, 
	- DDS, SAP Data Services 365
	- "Poduvath, Shanavas <shanavas.poduvath@sap.com>"		



----------- 5 Feb 2020 ---------------

cd /llbpal76-data-JIRA-application-data-jira/

----

2020-02-05 07:14:55,523 localq-reader-9 WARN      [c.a.j.c.distribution.localq.LocalQCacheOpReader] Runtime exception: UnrecoverableFailure occurred when processing: LocalQCacheOp{cacheName='com.tempoplugin.pgp.TeamPermissionResolverImpl.users-with-user-perms', action=REMOVE, key=c5236174, value=null, creationTimeInMillis=1580886883271} from cache replication queue: [queueId=queue_llbpal75palsapcorp_9_08bbe12e301007bfd019d91d5faa9c13, queuePath=/var/atlassian/application-data/jira/localq/queue_llbpal75palsapcorp_9_08bbe12e301007bfd019d91d5faa9c13], failuresCount: 1/10, error: java.rmi.UnknownHostException: Unknown host: d3702a942f97; nested exception is:
        java.net.UnknownHostException: d3702a942f97
2020-02-05 07:15:01,034 localq-reader-8 ERROR      [c.a.j.c.distribution.localq.LocalQCacheOpReader] Abandoning sending: LocalQCacheOp{cacheName='com.atlassian.jira.plugins.healthcheck.service.HeartBeatService.heartbeat', action=PUT, key=llbpal76.pal.sap.corp, value={time=1580886894916}, creationTimeInMillis=1580886894916} from cache replication queue: [queueId=queue_llbpal75palsapcorp_8_08bbe12e301007bfd019d91d5faa9c13, queuePath=/var/atlassian/application-data/jira/localq/queue_llbpal75palsapcorp_8_08bbe12e301007bfd019d91d5faa9c13], failuresCount: 10/10. Removing from queue. Error: java.rmi.UnknownHostException: Unknown host: d3702a942f97; nested exception is:
        java.net.UnknownHostException: d3702a942f97
com.atlassian.jira.cluster.distribution.localq.LocalQCacheOpSender$UnrecoverableFailure: java.rmi.UnknownHostException: Unknown host: d3702a942f97; nested exception is:
        java.net.UnknownHostException: d3702a942f97
        at com.atlassian.jira.cluster.distribution.localq.rmi.LocalQCacheOpRMISender.send(LocalQCacheOpRMISender.java:88)
        at com.atlassian.jira.cluster.distribution.localq.LocalQCacheOpReader.run(LocalQCacheOpReader.java:84)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.rmi.UnknownHostException: Unknown host: d3702a942f97; nested exception is:
        java.net.UnknownHostException: d3702a942f97
        at sun.rmi.transport.tcp.TCPEndpoint.newSocket(TCPEndpoint.java:616)
        at sun.rmi.transport.tcp.TCPChannel.createConnection(TCPChannel.java:216)
        at sun.rmi.transport.tcp.TCPChannel.newConnection(TCPChannel.java:202)
        at sun.rmi.server.UnicastRef.newCall(UnicastRef.java:338)
        at com.atlassian.jira.cluster.distribution.localq.rmi.BasicRMICachePeerProvider.lookupRemoteCachePeer(BasicRMICachePeerProvider.java:64)
        at com.atlassian.jira.cluster.distribution.localq.rmi.BasicRMICachePeerProvider.create(BasicRMICachePeerProvider.java:39)
        at com.atlassian.jira.cluster.distribution.localq.rmi.CachingRMICachePeerManager.getCachePeerFor(CachingRMICachePeerManager.java:58)
        at com.atlassian.jira.cluster.distribution.localq.rmi.CachingRMICachePeerManager.withCachePeer(CachingRMICachePeerManager.java:91)
        at com.atlassian.jira.cluster.distribution.localq.rmi.LocalQCacheOpRMISender.send(LocalQCacheOpRMISender.java:63)
        ... 6 more
Caused by: java.net.UnknownHostException: d3702a942f97
        at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
        at java.net.Socket.connect(Socket.java:589)
        at net.sf.ehcache.distribution.ConfigurableRMIClientSocketFactory.createSocketWithTimeout(ConfigurableRMIClientSocketFactory.java:97)
        at net.sf.ehcache.distribution.ConfigurableRMIClientSocketFactory.createSocket(ConfigurableRMIClientSocketFactory.java:87)
        ... 15 more
2020-02-05 07:15:01,643 localq-reader-9 ERROR      [c.a.j.c.distribution.localq.LocalQCacheOpReader] Abandoning sending: LocalQCacheOp{cacheName='com.tempoplugin.pgp.TeamPermissionResolverImpl.users-with-user-perms', action=REMOVE, key=c5236174, value=null, creationTimeInMillis=1580886883271} from cache replication queue: [queueId=queue_llbpal75palsapcorp_9_08bbe12e301007bfd019d91d5faa9c13, queuePath=/var/atlassian/application-data/jira/localq/queue_llbpal75palsapcorp_9_08bbe12e301007bfd019d91d5faa9c13], failuresCount: 10/10. Removing from queue. Error: java.rmi.UnknownHostException: Unknown host: d3702a942f97; nested exception is:
        java.net.UnknownHostException: d3702a942f97
com.atlassian.jira.cluster.distribution.localq.LocalQCacheOpSender$UnrecoverableFailure: java.rmi.UnknownHostException: Unknown host: d3702a942f97; nested exception is:
        java.net.UnknownHostException: d3702a942f97
        at com.atlassian.jira.cluster.distribution.localq.rmi.LocalQCacheOpRMISender.send(LocalQCacheOpRMISender.java:88)
        at com.atlassian.jira.cluster.distribution.localq.LocalQCacheOpReader.run(LocalQCacheOpReader.java:84)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.rmi.UnknownHostException: Unknown host: d3702a942f97; nested exception is:
        java.net.UnknownHostException: d3702a942f97
        at sun.rmi.transport.tcp.TCPEndpoint.newSocket(TCPEndpoint.java:616)
        at sun.rmi.transport.tcp.TCPChannel.createConnection(TCPChannel.java:216)
        at sun.rmi.transport.tcp.TCPChannel.newConnection(TCPChannel.java:202)
        at sun.rmi.server.UnicastRef.newCall(UnicastRef.java:338)
        at com.atlassian.jira.cluster.distribution.localq.rmi.BasicRMICachePeerProvider.lookupRemoteCachePeer(BasicRMICachePeerProvider.java:64)
        at com.atlassian.jira.cluster.distribution.localq.rmi.BasicRMICachePeerProvider.create(BasicRMICachePeerProvider.java:39)
        at com.atlassian.jira.cluster.distribution.localq.rmi.CachingRMICachePeerManager.getCachePeerFor(CachingRMICachePeerManager.java:58)
        at com.atlassian.jira.cluster.distribution.localq.rmi.CachingRMICachePeerManager.withCachePeer(CachingRMICachePeerManager.java:91)
        at com.atlassian.jira.cluster.distribution.localq.rmi.LocalQCacheOpRMISender.send(LocalQCacheOpRMISender.java:63)
        ... 6 more
Caused by: java.net.UnknownHostException: d3702a942f97
        at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
        at java.net.Socket.connect(Socket.java:589)
        at net.sf.ehcache.distribution.ConfigurableRMIClientSocketFactory.createSocketWithTimeout(ConfigurableRMIClientSocketFactory.java:97)
        at net.sf.ehcache.distribution.ConfigurableRMIClientSocketFactory.createSocket(ConfigurableRMIClientSocketFactory.java:87)
        ... 15 more
2020-02-05 07:15:01,644 localq-reader-9 WARN      [c.a.j.c.distribution.localq.LocalQCacheOpReader] Runtime exception: UnrecoverableFailure occurred when processing: LocalQCacheOp{cacheName='com.atlassian.gadgets.renderer.internal.cache.httpResponses', action=REMOVE, key=[{"appUrl":null},{"authType":"none"},{"instanceId":null},{"method":"GET"},{"ownerId":null},{"serviceName":null},{"tokenName":null},{"tokenOwner":null},{"url":"https://jira.di-infra.sap.corp/rest/gadgets/1.0/g/messagebundle/und/gadget.common%2Cgadget.filter.results%2Cgadget.issuetable.common"},{"viewerId":null}], value=null, creationTimeInMillis=1580886883428} from cache replication queue: [queueId=queue_llbpal75palsapcorp_9_08bbe12e301007bfd019d91d5faa9c13, queuePath=/var/atlassian/application-data/jira/localq/queue_llbpal75palsapcorp_9_08bbe12e301007bfd019d91d5faa9c13], failuresCount: 1/10, error: java.rmi.UnknownHostException: Unknown host: d3702a942f97; nested exception is:
        java.net.UnknownHostException: d3702a942f97
2020-02-05 07:15:07,771 localq-reader-9 ERROR      [c.a.j.c.distribution.localq.LocalQCacheOpReader] Abandoning sending: LocalQCacheOp{cacheName='com.atlassian.gadgets.renderer.internal.cache.httpResponses', action=REMOVE, key=[{"appUrl":null},{"authType":"none"},{"instanceId":null},{"method":"GET"},{"ownerId":null},{"serviceName":null},{"tokenName":null},{"tokenOwner":null},{"url":"https://jira.di-infra.sap.corp/rest/gadgets/1.0/g/messagebundle/und/gadget.common%2Cgadget.filter.results%2Cgadget.issuetable.common"},{"viewerId":null}], value=null, creationTimeInMillis=1580886883428} from cache replication queue: [queueId=queue_llbpal75palsapcorp_9_08bbe12e301007bfd019d91d5faa9c13, queuePath=/var/atlassian/application-data/jira/localq/queue_llbpal75palsapcorp_9_08bbe12e301007bfd019d91d5faa9c13], failuresCount: 10/10. Removing from queue. Error: java.rmi.UnknownHostException: Unknown host: d3702a942f97; nested exception is:
        java.net.UnknownHostException: d3702a942f97
com.atlassian.jira.cluster.distribution.localq.LocalQCacheOpSender$UnrecoverableFailure: java.rmi.UnknownHostException: Unknown host: d3702a942f97; nested exception is:
        java.net.UnknownHostException: d3702a942f97
        at com.atlassian.jira.cluster.distribution.localq.rmi.LocalQCacheOpRMISender.send(LocalQCacheOpRMISender.java:88)
        at com.atlassian.jira.cluster.distribution.localq.LocalQCacheOpReader.run(LocalQCacheOpReader.java:84)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.rmi.UnknownHostException: Unknown host: d3702a942f97; nested exception is:
        java.net.UnknownHostException: d3702a942f97
        at sun.rmi.transport.tcp.TCPEndpoint.newSocket(TCPEndpoint.java:616)
        at sun.rmi.transport.tcp.TCPChannel.createConnection(TCPChannel.java:216)
        at sun.rmi.transport.tcp.TCPChannel.newConnection(TCPChannel.java:202)
        at sun.rmi.server.UnicastRef.newCall(UnicastRef.java:338)
        at com.atlassian.jira.cluster.distribution.localq.rmi.BasicRMICachePeerProvider.lookupRemoteCachePeer(BasicRMICachePeerProvider.java:64)
        at com.atlassian.jira.cluster.distribution.localq.rmi.BasicRMICachePeerProvider.create(BasicRMICachePeerProvider.java:39)
        at com.atlassian.jira.cluster.distribution.localq.rmi.CachingRMICachePeerManager.getCachePeerFor(CachingRMICachePeerManager.java:58)
        at com.atlassian.jira.cluster.distribution.localq.rmi.CachingRMICachePeerManager.withCachePeer(CachingRMICachePeerManager.java:91)
        at com.atlassian.jira.cluster.distribution.localq.rmi.LocalQCacheOpRMISender.send(LocalQCacheOpRMISender.java:63)
        ... 6 more
Caused by: java.net.UnknownHostException: d3702a942f97
        at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
        at java.net.Socket.connect(Socket.java:589)
        at net.sf.ehcache.distribution.ConfigurableRMIClientSocketFactory.createSocketWithTimeout(ConfigurableRMIClientSocketFactory.java:97)
        at net.sf.ehcache.distribution.ConfigurableRMIClientSocketFactory.createSocket(ConfigurableRMIClientSocketFactory.java:87)
        ... 15 more
2020-02-05 07:15:07,772 localq-reader-9 WARN      [c.a.j.c.distribution.localq.LocalQCacheOpReader] Runtime exception: UnrecoverableFailure occurred when processing: LocalQCacheOp{cacheName='com.atlassian.gadgets.renderer.internal.cache.messageBundles', action=REMOVE, key=https://jira.di-infra.sap.corp/rest/gadgets/1.0/g/com.atlassian.jira.gadgets:filter-results-gadget/gadgets/filter-results-gadget.xml.all_ALL, value=null, creationTimeInMillis=1580886883429} from cache replication queue: [queueId=queue_llbpal75palsapcorp_9_08bbe12e301007bfd019d91d5faa9c13, queuePath=/var/atlassian/application-data/jira/localq/queue_llbpal75palsapcorp_9_08bbe12e301007bfd019d91d5faa9c13], failuresCount: 1/10, error: java.rmi.UnknownHostException: Unknown host: d3702a942f97; nested exception is:
        java.net.UnknownHostException: d3702a942f97


[root@llbpal75 ~]# docker ps | grep -i jira
d3702a942f97        dbs-docker-repository.docker.repositories.sap.ondemand.com/jira:8.5.1   "/tini -- /entrypoin…"   4 days ago          Up 6 minutes        0.0.0.0:9443->9443/tcp, 0.0.0.0:8081->8080/tcp   jira


---------------

2020-02-05 07:43:15,852 localhost-startStop-1 INFO      [c.a.jira.startup.JiraStartupLogger]

    ***********************************************************************************
    JIRA 8.5.1 build: 805001 started. You can now access JIRA through your web browser.
    ***********************************************************************************

2020-02-05 07:43:15,914 localhost-startStop-1 INFO      [c.a.jira.startup.ClusteringLauncher] Checking local index on node start
2020-02-05 07:43:17,571 localhost-startStop-1 INFO      [c.a.jira.startup.JiraStartupLogger]

    ___ Plugin System Started _________________

2020-02-05 07:43:17,677 localhost-startStop-1 INFO      [c.a.jira.upgrade.UpgradeScheduler] Scheduling upgrades to run in 1 minute(s)
2020-02-05 07:43:20,314 Modification Check:thread-1 INFO      [c.a.jira.startup.JiraStartupLogger]

    ___ Modifications ___________________________

         Modified Files                                : None
         Removed Files                                 : None


-------

llbpal53:~ # docker ps -a | grep -i 2gen
b56c5301ee22        dbs-docker-repository.docker.repositories.sapcdn.io/llk-portal-ui-2geng:2.0.1.30-SNAPSHOT                  "/bin/sh -c 'bash ..."   2 hours ago         Exited (1) 12 seconds ago                                                                                                                                                       llk-portal-ui-2geng


openssl s_client -connect IAD1BDTRUST01.bd.trust:636 -showcerts

[i871228@llbpal76 testSSL]$ sudo keytool -import -alias iad4 -keystore /opt/local/jdk/jre/lib/security/cacerts -file certificate.cer -storepass changeit -noprompt
Certificate was added to keystore
[i871228@llbpal76 testSSL]$ java SSLPoke IAD1BDTRUST01.bd.trust  636
Successfully connected

45 minutes call with Michael


------------------------- 6 Feb 2020 ----------------------------

- Worked on Nidhi request of BDTrust.
- Worked with Nidhi nigam to check the reverse proxy test from us4ciapp01 to us4ciperfweb001.

- Sending msg to SGE team to check the secure docker image.

FROM dbs-docker-repository.docker.repositories.sapcdn.io/securityapprovedbaseimages/alpine-jre8:latest
COPY HelloWorld.class HelloWorld.class
RUN ls $JAVA_HOME/lib/security
RUN keytool -list -keystore $JAVA_HOME/lib/security/cacerts -storepass changeit
# CMD whoami
# The command above shows appuser
CMD java HelloWorld

This is a SAP scanned alpine docker image and he installed JRE 8. Can someone please test this for various SGE components so that we can start using this image as per the SAP security guildelines.

- Resolved the Ansible Tower server.


----------------------- 7 Feb 2020 ---------------------------------
- 

Hi Team,
We are again facing the issue of not able to access the log directory "/aws_logs-general". When I try to access the directory it is getting hung.
[skesarkar@us4llkapi003 ~]$ cd /aws_logs-general/
[skesarkar@us4llkapi003 aws_logs-general]$ ls
^C

Further I tried to also find out the processes that might be accessing this directory, so that I can kill those processes if required. But the "lsof" command is also not responding.

[skesarkar@us4llkapi003 ~]$ sudo lsof /aws_logs-general/
[sudo] password for skesarkar:
^C
[skesarkar@us4llkapi003 ~]$
Kindly please help us to sort of this issue as soon as possible.
Thank you.


- Resolved the deployment which ran for 118 minutes, https://lvpal408.pal.sap.corp:9445/deploy/viewDeploymentResult.action?deploymentResultId=130810254

- Rahul issue to connect to host "awsdeviqdb002", 

ADS IQ server on AWS:
Servername = DLIQDM7_WR   
Port = 4203
Hostname = 198.19.5.128 (dliqdm7-wr)

awsdevops001 : 198.19.11.212
---------------------------------
[skesarkar@iad1bastion01 AWS_access]$ ssh -i awsdevops.pem ec2-user@198.19.11.212
Last login: Fri Feb  7 08:16:49 2020 from ip-172-24-227-30.ec2.internal
[ec2-user@ip-198-19-11-212 ~]$ sudo su - pablo
Last login: Fri Feb  7 08:27:07 UTC 2020 on pts/0
[pablo@ip-198-19-11-212 ~]$ cd .ssh/
[pablo@ip-198-19-11-212 .ssh]$ ls
authorized_keys  awsdevops.pem  id_rsa  id_rsa.pub  known_hosts  psilva-db-aws4.pem
[pablo@ip-198-19-11-212 .ssh]$ ssh -i id_rsa ec2-user@198.19.5.128
Last login: Fri Feb  7 08:27:19 2020 from ip-198-19-11-212.ec2.internal

       __|  __|_  )  FrontLine PCI Hardened AMI
       _|  (     /   Red Hat Enterprise 7.6 | Version: 1.19.3
      ___|\___|___|  OS Documentation: https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/
---------------------------------





[ec2-user@ip-198-19-11-212 AWS-Keys]$ history | grep -i "psilva_awsdevops001"
  263  ssh -i ~/.ssh/psilva_awsdevops001 ec2-user@awsdeviqvip004.dev.sapdi.com

llbpal72 -> bastion -> 122 -> 5.128

ssh user@host -i keyfile.pem -L 1234:127.0.0.1:2345

ssh -L 9999:localhost:9999 host1 ssh -L 9999:localhost:1234 -N host2

ssh -i awsdevops.pem -L 9999:localhost:9999 ec2-user@198.19.11.212 ssh -i psilva_awsdevops001.pem -L 9999:localhost:4203 -N ec2-user@198.19.5.128


Host iadtunnel
    Hostname 172.24.227.30
    User skesarkar

Host awstunnel
    Hostname 198.19.11.212
    LocalForward 9999 A9B64BAAC0884BB14E2B49B0B2099B1D.gr7.us-east-1.eks.amazonaws.com:443
    ProxyCommand ssh skesarkar@172.24.227.30 nc %h %p 2> /dev/null
    User ec2-user
	
awsdev-app-1 : 198.19.7.0/24
awsdev-app-2 : 198.19.8.0/24
awsdev-infra-1 : 198.19.11.0/24
awsdev-infra-2 : 198.19.12.0/24

awsdev-db-1 : 198.19.5.0/24
awsdev-db-2 : 198.19.6.0/24

----------- Allow template ----------------
VPC: AWSDEV, AWS26
Source IP(s): 198.19.11.0/24, 198.19.12.0/24, 198.19.7.0/24, 198.19.8.0/24
Destination IP(s): 198.19.5.0/24, 198.19.6.0/24
Protocol (TCP, UDP, or ICMP): TCP
Port: 4203
Description: Allow instances from Infra-Subnet and App-Subnet to access SyabseIQ on DevDB-Subnet on port 4203

Options

-------------------------------------------
	
- KnowHow about Protecode.
	1) What is a Protecode server.
	2) You need to have Protecode group.
	3) How to create a Protecode group using SVM.
	4) User management for Protecode group.
	5) Various user roles.
	6) Our approach of scanning a Docker Container with Protecode server.
	7) Vulnerabilities notification.
	
	
------------------10 Feb 2020-------------------------	
- Had a call with Pablo.
- Worked on fixing the Report.
- Had a call with Abhilekh for 2GEN.
- Worked with Debalina.
- Worked on editing the Wiki.


------------------11 Feb 2020-------------------------	
- Rollbacked the protecode for the build plans.
- Resolved Shikha Shah build issue.
- Resolved Rajul Talgaonkar build issue.

- Had a call with Abhilekh regarding Dev issues.

- Resolved Shikha issue of,

-Djgroups.tcp.address=LOOPBACK -Djava.net.preferIPv4Stack=true
~ $ sudo su - jhipster
/bin/sh: sudo: not found
~ $ su - jhipster
su: must be suid to work properly
~ $  exec java ${JAVA_OPTS} -Djava.security.egd=file:/dev/./urandom -jar "${HOME}/app.war" "$@"
Error: Unable to access jarfile /home/jhipster/app.war
c5246840@llbpal53:~> docker run --rm -ti  --entrypoint /bin/sh dbs-docker-repository.docker.repositories.sapcdn.io/scheduler-service:2.0.1.20-SNAPSHOT
~ $ cd /home/jhipster/
~ $ ls -l
total 83684
-rwxr-----    1 root     root      85582782 Feb 11 07:10 app.war
-rwxr-xr-x    2 jhipster jhipster       188 Feb 11 06:52 entrypoint.sh
-rw-r-----    3 root     root          9862 Feb 11 06:52 keystore
~ $ whoami
jhipster
~ $ file app.war
/bin/sh: file: not found
~ $ exec java ${JAVA_OPTS} -Djava.security.egd=file:/dev/./urandom -jar ./app.war
Error: Unable to access jarfile ./app.war


- 



------------------12 Feb 2020-------------------------

- failed: [llbpal53] (item={u'layer': u'be', u'name': u'scheduler-service', u'ports': [u'16400:16400']}) => {"failed": true, "item": {"layer": "be", "name": "scheduler-service", "ports": ["16400:16400"]}, "msg": "Error starting container b70efc1842394804201643fdc326312cc9713f796604a00e65448e813a523b77: 500 Server Error: Internal Server Error (\"{\"message\":\"linux spec user: unable to find user appuser: no matching entries in passwd file\"}\")"}

- When added chmod,
##### Dockerfile ####

FROM dbs-docker-repository.docker.repositories.sapcdn.io/securityapprovedbaseimages/alpine-jre8:latest
ENV SPRING_OUTPUT_ANSI_ENABLED=ALWAYS \
    JHIPSTER_SLEEP=0 \
    JAVA_OPTS="-Djgroups.tcp.address=LOOPBACK -Djava.net.preferIPv4Stack=true"
USER appuser
WORKDIR /home/appuser
ADD entrypoint.sh entrypoint.sh
ADD keystore keystore
RUN chmod 755 entrypoint.sh
ADD *.war app.war
ENTRYPOINT ["./entrypoint.sh"]
EXPOSE 9550

#####################


----

build	12-Feb-2020 11:43:27	[0mRemoving intermediate container 34c8bfb807e7
error	12-Feb-2020 11:43:27	The command '/bin/sh -c chmod 755 entrypoint.sh' returned a non-zero code: 1
simple	12-Feb-2020 11:43:27	Failing task since return code of [/usr/bin/docker build --no-cache=true --force-rm=true --tag=dbs-docker-repository.docker.repositories.sap.ondemand.com/scheduler-service:2.0.1.30-SNAPSHOT /var/atlassian/application-data/bamboo/xml-data/build-dir/132677639/KWM-SCHED0-JOB1/target] was 1 while expected 0
error	12-Feb-2020 11:43:27	Error occurred while running Task 'Docker build(6)' of type com.atlassian.bamboo.plugins.bamboo-docker-plugin:task.docker.cli.
error	12-Feb-2020 11:43:27	com.atlassian.bamboo.task.TaskException: Failed to execute task



- When removed chmod,
##### Dockerfile ####
  
FROM dbs-docker-repository.docker.repositories.sapcdn.io/securityapprovedbaseimages/alpine-jre8:latest
ENV SPRING_OUTPUT_ANSI_ENABLED=ALWAYS \
    JHIPSTER_SLEEP=0 \
    JAVA_OPTS="-Djgroups.tcp.address=LOOPBACK -Djava.net.preferIPv4Stack=true"
ADD entrypoint.sh entrypoint.sh
ADD keystore keystore
ADD *.war app.war
ENTRYPOINT ["./entrypoint.sh"]
EXPOSE 9550
#####################
----
failed: [llbpal53] (item={u'layer': u'be', u'name': u'scheduler-service', u'ports': [u'16400:16400']}) => {"failed": true, "item": {"layer": "be", "name": "scheduler-service", "ports": ["16400:16400"]}, "msg": "Error starting container cd774e9adb321d6b0a0f4deab8faef54747b96871e5c0f72d9d096e35f1260f3: 500 Server Error: Internal Server Error (\"{\"message\":\"oci runtime error: container_linux.go:247: starting container process caused \\\"exec: \\\\\\\"./entrypoint.sh\\\\\\\": permission denied\\\"\\n\"}\")"}

---------------

llbpal50:~ # cat /var/log/messages | grep -i f975307c88e8
2020-02-12T01:22:51-08:00 llbpal53.pal.sap.corp second-gen-engagement/f975307c88e8[11528]: sh: can't open '/opt/sge/services/conf/execute.sh': Permission denied
2020-02-12T01:22:54-08:00 llbpal53.pal.sap.corp second-gen-engagement/f975307c88e8[11528]: sh: can't open '/opt/sge/services/conf/execute.sh': Permission denied
2020-02-12T01:22:57-08:00 llbpal53.pal.sap.corp second-gen-engagement/f975307c88e8[11528]: sh: can't open '/opt/sge/services/conf/execute.sh': Permission denied
llbpal50:~ # cat /var/log/messages | grep -i e88fbfb25c8d
^C
You have new mail in /var/spool/mail/root
llbpal50:~ # tail -f /var/log/messages | grep -i e88fbfb25c8d
2020-02-12T04:29:16-08:00 llbpal53.pal.sap.corp scheduler-service/e88fbfb25c8d[11528]: bash: ./entrypoint.sh: Permission denied
2020-02-12T04:29:23-08:00 llbpal53.pal.sap.corp scheduler-service/e88fbfb25c8d[11528]: message repeated 2 times: [ bash: ./entrypoint.sh: Permission denied]
^C
llbpal50:~ # tail -f /var/log/messages | grep -i ff8518977c9e
2020-02-12T04:56:44-08:00 llbpal53.pal.sap.corp scheduler-service/ff8518977c9e[11528]: The application will start in 0s...
2020-02-12T04:56:44-08:00 llbpal53.pal.sap.corp scheduler-service/ff8518977c9e[11528]: Error: Unable to access jarfile /home/appuser/app.war
2020-02-12T04:56:48-08:00 llbpal53.pal.sap.corp scheduler-service/ff8518977c9e[11528]: The application will start in 0s...
2020-02-12T04:56:48-08:00 llbpal53.pal.sap.corp scheduler-service/ff8518977c9e[11528]: Error: Unable to access jarfile /home/appuser/app.war
2020-02-12T04:56:51-08:00 llbpal53.pal.sap.corp scheduler-service/ff8518977c9e[11528]: The application will start in 0s...
2020-02-12T04:56:51-08:00 llbpal53.pal.sap.corp scheduler-service/ff8518977c9e[11528]: Error: Unable to access jarfile /home/appuser/app.war


-----------------

FROM dbs-docker-repository.docker.repositories.sapcdn.io/securityapprovedbaseimages/alpine-jre8:latest
ENV SPRING_OUTPUT_ANSI_ENABLED=ALWAYS \
    JHIPSTER_SLEEP=0 \
    JAVA_OPTS="-Djgroups.tcp.address=LOOPBACK -Djava.net.preferIPv4Stack=true"
COPY --chown=appuser entrypoint.sh ~/entrypoint.sh
COPY --chown=appuser keystore ~/keystore
COPY --chown=appuser *.war ~/app.war
RUN chmod +x ~/entrypoint.sh
CMD bash ~/entrypoint.sh
EXPOSE 9550

-------------------

rror	12-Feb-2020 13:03:02	The command '/bin/sh -c chmod +x ~/entrypoint.sh' returned a non-zero code: 1
simple	12-Feb-2020 13:03:02	Failing task since return code of [/usr/bin/docker build --no-cache=true --force-rm=true --tag=dbs-docker-repository.docker.repositories.sap.ondemand.com/scheduler-service:2.0.1.42-SNAPSHOT /var/atlassian/application-data/bamboo/xml-data/build-dir/132677640/KWM-SCHED2-JOB1/target] was 1 while expected 0


------------------

FROM dbs-docker-repository.docker.repositories.sapcdn.io/securityapprovedbaseimages/alpine-jre8:latest
ENV SPRING_OUTPUT_ANSI_ENABLED=ALWAYS \
    JHIPSTER_SLEEP=0 \
    JAVA_OPTS="-Djgroups.tcp.address=LOOPBACK -Djava.net.preferIPv4Stack=true"
COPY --chown=appuser entrypoint.sh /entrypoint.sh
COPY --chown=appuser keystore /keystore
COPY --chown=appuser *.war /app.war
RUN chmod +x /entrypoint.sh
CMD bash /entrypoint.sh
EXPOSE 9550

---

This worked.

- Had a call with Khalid.
- Fixed the cronjob.




------------------ 13 Feb 2020-------------------------

- Send mail to Database team.
- Check with Shikha.
- Pankaj issue,

It was showing this error
"msg": "Error stopping container 03c19b7da23a8827f9bbd2ce3166974ac2d922cd26b93e179854fc9548324553: UnixHTTPConnectionPool(host='localhost', port=None): Read timed out. (read timeout=70)"}





1:47
I did the redeployment which resolved the issue.
1:47
https://bamboo.di-infra.sap.corp/deploy/viewDeploymentResult.action?deploymentResultId=133366133


- Checked ulimit for docker containers on prod servers
- Call with Abhilekh
	- Bamboo branch was going into disabled state automatically.
	- Resovled the above issue.
	- 1 hour call. did got resolved.
	- Resolved the issue after recreating the new Build plan.
		https://bamboo.di-infra.sap.corp/deploy/viewDeploymentResult.action?deploymentResultId=133366152
	
- Resolved Ravi Jagani issue.
	https://bamboo.di-infra.sap.corp/deploy/viewDeploymentResult.action?deploymentResultId=133366154
	
- Identified the rca ofr ulimit, chat is on pablo side.



------------------ 14 Feb 2020-------------------------


- SSH tunnel from llbpal72.pal.sap.corp

   67  vi .ssh/config
   68  pwd
   69  ls -a
   70  exit
   71  ls -trhl .ssh/
   72  cat .ssh/config
   73  ssh iadtunnel
   74  ssh-add -K
   75  ssh iadtunnel
   76  rm /sapmnt/HOME/i871228/.ssh/id_rsa
   77  ssh-keygen -t rsa
   78  ssh-copy-id iadtunnel
   79  ssh iadtunnel
   80  cat .ssh/config
   81  cat ~/.ssh/id_rsa.pub
   82  vi .ssh/config
   83  ssh -N -f awsapphost
   84  vi .ssh/config
   85  ssh -N -f awsapphost
   86  ps aux | grep apphost
   87  kill 193364 193364
   88  kill 28835 23510
   89  vi .ssh/config
   90  ssh -N -f awsapphost
   91  telnet localhost 4203
   92  telnet llbpal72.pal.sap.corp 4203
[i871228@llbpal72 ~]$ cd .ssh/
[i871228@llbpal72 .ssh]$ cat config
Host us4* us2* iad1* fr*
    ProxyCommand ssh -q psilva@172.24.227.30 -W %h:%p
    User psilva
    IdentityFile ~/.ssh/id_rsa
    ServerAliveInterval 30
    ServerAliveCountMax 4

Host iadtunnel
    Hostname 172.24.227.30
    User psilva

Host awsapphost
    Hostname 198.19.5.210
    LocalForward llbpal72.pal.sap.corp:4203 198.19.5.128:4203
    ProxyJump iadtunnel
    User ec2-user

----- iadbastion01

psilva@iad1bastion01 .ssh] ssh -i ~/.ssh/psilva_awsdevops001 ec2-user@198.19.5.210

on 11.212

-----BEGIN RSA PRIVATE KEY-----
MIIEpAIBAAKCAQEAuS9QgXwM/SyRcIdEPdvX8kj7Kc2p0dS/dTUuTjNuerEisXMC
HjFrsZW6bwUN8Y8ebqfmj0ls8s7lCtSp1A9y7deZkpggj/YexIedHLm8E8UNBrQ4
7DFErSeO42t2hGK0dQM4lUyp5FBWZYTmsnfYX0pGSkMMOFIQ8kTT3h9M77RYwNfj
nm7cagCWJXc2On5qhmA5osFGsQQgeZWEWljF0Ik/Vdms0jkxK+pXojigw7TKmvpq
WZFcdYnNeccBzljoUmROy12l9ef63XEs/Q8iY8TGyOmHugnOtl/XwvSrbGipBoh2
IrODmspnxyO0PY5boWObs0A1K9s8Px1ckjJSewIDAQABAoIBAQCbOuYpJGte7jUc
Nsclc1uZZUPkztlDYyr6fRHLy6EYh9OlG9rypEq6GeCln38Itge0ilniUpJZFsU4
Rvv1Ht4gMVIyaAMjqhQwed1hbvARQmSg8gGlO8dtvnMwf6xhXLwYOhL8U781+4Hw
p28auwtNFV3Gcl2T+HbsHnXbBDHh28T1+ESkufTNqne+twRNWLKz02LIiJE0yNeS
nCUFWtbPcpFwbHX43Wpymt1H8WuKdIJ+3D12BaQy7IXD9BXgtWveE9Pvnv9NwkqH
r/NE857uB1MqFkPwDj5HBM56UvOj8O/mm/pupFIIwA+6OpEMn1mpBbrSRzmRdpdG
buKdh7KBAoGBAO3SvCvGmf5oDrpiFih48cw2gtVMB5Fv9F+d42wMztyjm60VwV77
CbIz5YOL7aXMCUSV51UhaVqXOTxImo6RnpWVLHWhLZLJqBKYy9GJzD4aV6BNPSxN
krIGk254f7PZmNKDdZanA+ttuwJ28vFLD/lr96UDnWT8kMBFcV3vNwaTAoGBAMdW
p1GPvS/8zbWLw3VMgT9zg14rWnzJpRzsThzE+9F+AUnDvU/KpMzFhHAD4fv+RK7T
T92zBA8ste11DSn1zm7U7fNRsM6OU0EPLNr2Hk1e6Fq0l8ApUHXyekImsu99KxSy
XReTsyuOdlw00QMI7ymw25k5yQHNBVOwkylWsk15AoGAdxsRboMaYJVQuvugyUg5
IOM0Bwr6x1Xgd3qliVmoyEuQDdFVdPIY2wyup4EO1fgntBe3MaV4FAykd5KNBVXc
rGF3ULoWDuu7pwSrHxf6GCgtNnU4a9W14D72216x5irWJKf93iNrDR95hzR3W20S
ZQIzlosG6qqq/ABLVJYXYs0CgYA8N8s38mo+kuo5y8zyJHhvCKBI1GPzip7ci+5U
9pT0BlKxS/ZmolsCmTCW/jVIgNmSzrczzB6Pzvvujgu5/02rYZu4g5cb7xbrcoAk
bXFeAivDKwckv9ZUqzmPvk0T2AYprHA+7RmVMOZEmJgDiLFnMMxGD5TPfHX9gDwU
ouIeUQKBgQDDvuRYyJpbzZDbyNuMBG1/7PtBHhC+IcUKh/wrHcy0UJGu7SSMZKmK
f7WJz4hlFI3HKWq9cr1JENo4wQaYsWebNrLo1mm72M6JDFX60wZc8KWUFuDozzgG
ikbF3d2c3k13mrHK0X9NeNwb8/A5M5BN8Vseffv2Zgw7+DV8MWa+ww==
-----END RSA PRIVATE KEY-----


- Adding key to keystore.

keytool error: java.io.FileNotFoundException: /usr/lib/jvm/java-1.8-openjdk/jre/lib/security/cacerts (Permission denied)
/ $ ls -l /usr/lib/jvm/java-1.8-openjdk/jre/lib/security/cacerts
lrwxrwxrwx    1 root     root            27 Jan 20 19:25 /usr/lib/jvm/java-1.8-openjdk/jre/lib/security/cacerts -> /etc/ssl/certs/java/cacerts
/ $ ls
app.war           certfile_nat.cer  etc               lib               opt               run               sys               var
bin               dev               home              media             proc              sbin              tmp
certfile.cer      entrypoint.sh     keystore          mnt               root              srv               usr
/ $ keytool -import -alias sap_nat -keystore /usr/lib/jvm/java-1.8-openjdk/jre/lib/security/cacerts -storepass changeit -file ./certfile_nat.cer
....
....
Trust this certificate? [no]:  yes
Certificate was added to keystore
keytool error: java.io.FileNotFoundException: /usr/lib/jvm/java-1.8-openjdk/jre/lib/security/cacerts (Permission denied)


-------------------- 17 Feb 2020 ------------------------

- Worked on resolving the /aws mount point issue.


- Resolved the Bhumika Mongo issue,

[root@llbpal72 ~]# docker ps -a | grep -i mongo
d99b06a93547        mongo                                                                                               "docker-entrypoint.s…"    2 weeks ago              Up 2 weeks                 0.0.0.0:28000->27017/tcp                             funny_elgamal
5a658ab2aff7        mongo                                                                                               "docker-entrypoint.s…"    2 weeks ago              Exited (2) 2 weeks ago                                                          competent_lewin
[root@llbpal72 ~]# docker exec -it d99b06a93547 sh
# history
sh: 1: history: not found
# ps aux
USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
mongodb      1  0.6  0.0 1625444 130328 ?      Ssl  Jan27 205:13 mongod --bind_ip_all
root       316  0.2  0.0   4616   660 pts/0    Ss   06:36   0:00 sh
root       321  0.0  0.0  34388  1552 pts/0    R+   06:36   0:00 ps aux
# exit
[root@llbpal72 ~]# telnet localhost 28000
Trying 127.0.0.1...
Connected to localhost.
Escape character is '^]'.
^C
^]
telnet> quit


- Resolved the "current status pending" Bamboo build job for Shikha, Shah.

- Checking public keys.
	- Resolved "achawla" login issue.
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDKt0QV2DWSievFNn4Vwe0FudZ0YDbPJqNPvodmLPlBHHvFDOvejgeasnTQGO4v2ZZ7lroKVlbvL3YBiApn23j7ci4+hHzsohS1xuEopOa9sY0HC+CHS+7QH5alabH1hGE8Da6hOL10IsUIdU2xNCEwwK2jPAh8zqBwRVtOnD0UAMZkNds30GQduF0eoojR0IsvJIJkPIJx6S/Q0U5g9n0mzv5JblBem/JY0TtKGNGlKHYwkYrLeX9+UYufahNULcItNgpVqruvi2Hx0RYqk7EyWPqBvecPKtxe5utTtA2w0iCiXhpUxX5dyUlUUk8y02IVWzBISOuaUQMfMflyFyGN i871228@llbpal72.pal.sap.corp

ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDKt0QV2DWSievFNn4Vwe0FudZ0YDbPJqNPvodmLPlBHHvFDOvejgeasnTQGO4v2ZZ7lroKVlbvL3YBiApn23j7ci4+hHzsohS1xuEopOa9sY0HC+CHS+7QH5alabH1hGE8Da6hOL10IsUIdU2xNCEwwK2jPAh8zqBwRVtOnD0UAMZkNds30GQduF0eoojR0IsvJIJkPIJx6S/Q0U5g9n0mzv5JblBem/JY0TtKGNGlKHYwkYrLeX9+UYufahNULcItNgpVqruvi2Hx0RYqk7EyWPqBvecPKtxe5utTtA2w0iCiXhpUxX5dyUlUUk8y02IVWzBISOuaUQMfMflyFyGN i871228@llbpal72.pal.sap.corp

- Had a call with Abhilekh for performance issue.



-------------------- 18 Feb 2020 ------------------------

- Had a discussion with Rahul regarding this connectivity.

[sybase@awsdeviqdb002 ~]$ ps aux | grep -i sybase
sybase   15697  0.0  3.1 22127304 1006776 ?    Ssl  Feb17   0:35 /opt/sybase/software/iq161/IQ-16_1/bin64/iqsrv16 @//opt/sybase/servers/DIQDI7_21/conf/DIQDI7_21.cfg /opt/sybase/servers/DIQDI7_21/data/DIQDI7_21.db -iqstart 1 -c 48m -gn 105 -o /opt/sybase/servers/DIQDI7_21/log/DIQDI7_21.0001.srvlog -hn 5
sybase   16370  0.1  4.9 22147916 1582540 ?    Ssl  Feb17   1:05 /opt/sybase/software/iq161/IQ-16_1/bin64/iqsrv16 @//opt/sybase/servers/DIQDI7_22/conf/DIQDI7_22.cfg /opt/sybase/servers/DIQDI7_22/data/DIQDI7_22.db -iqstart 1 -c 48m -gn 105 -o /opt/sybase/servers/DIQDI7_22/log/DIQDI7_22.0001.srvlog -hn 5
root     19270  0.0  0.0 218644  4076 pts/0    S    06:38   0:00 sudo su - sybase
root     19272  0.0  0.0 169320  1916 pts/0    S    06:38   0:00 su - sybase
sybase   19273  0.0  0.0 115580  2152 pts/0    S    06:38   0:00 -bash
sybase   19296  0.0  0.0 155372  1860 pts/0    R+   06:38   0:00 ps aux
sybase   19297  0.0  0.0 112716  1000 pts/0    S+   06:38   0:00 grep --color=auto -i sybase
[sybase@awsdeviqdb002 ~]$ history | grep -i diqrlv7
 1004  history | grep -i diqrlv7
[sybase@awsdeviqdb002 ~]$ exit
logout
[ec2-user@awsdeviqdb002 ~]$ exit
logout
Connection to 198.19.5.128 closed.
[skesarkar@iad1bastion01 AWS_access]$ ssh -i psilva_awsdevops001.pem ec2-user@198.19.5.101
^C
[skesarkar@iad1bastion01 AWS_access]$ ssh -i psilva_awsdevops001.pem ec2-user@198.19.5.76
The authenticity of host '198.19.5.76 (198.19.5.76)' can't be established.
RSA key fingerprint is 5b:b8:5f:17:e5:28:b9:ae:00:81:45:11:d1:c6:02:0c.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '198.19.5.76' (RSA) to the list of known hosts.
Last login: Thu Feb 13 20:48:26 2020 from ip-172-24-227-31.ec2.internal

       __|  __|_  )  FrontLine PCI Hardened AMI
       _|  (     /   Red Hat Enterprise 7.6 | Version: 1.19.3
      ___|\___|___|  OS Documentation: https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/

[ec2-user@awsdeviqdb001 ~]$ sudo su - sybase
Last login: Mon Feb 17 18:58:27 UTC 2020 from awsdeviqdb002 on pts/1
Sourcing SYBASE.sh
[sybase@awsdeviqdb001 ~]$ history | grep -i diqrlv7
 1000  history | grep -i diqrlv7
[sybase@awsdeviqdb001 ~]$ exit
logout
[ec2-user@awsdeviqdb001 ~]$ exit
logout
Connection to 198.19.5.76 closed.
[skesarkar@iad1bastion01 AWS_access]$ ssh -i psilva_awsdevops001.pem ec2-user@198.19.5.38
^C
[skesarkar@iad1bastion01 AWS_access]$ ssh -i psilva_awsdevops001.pem ec2-user@198.19.5.174
The authenticity of host '198.19.5.174 (198.19.5.174)' can't be established.
RSA key fingerprint is fd:07:56:f2:b3:97:24:76:6d:0f:8e:8f:10:67:d4:a1.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '198.19.5.174' (RSA) to the list of known hosts.
Last login: Thu Jan  9 01:17:48 2020 from ip-198-19-11-212.ec2.internal

       __|  __|_  )  FrontLine PCI Hardened AMI
       _|  (     /   Red Hat Enterprise 7.6 | Version: 1.19.3
      ___|\___|___|  OS Documentation: https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/

[ec2-user@awsdeviqdb003 ~]$ sudo su - sybase
Last login: Thu Feb 13 06:40:27 UTC 2020 from ip-172-24-227-30.ec2.internal on pts/0
[sybase@awsdeviqdb003 ~]$ history | grep -i diqrlv7
 1000  history | grep -i diqrlv7
[sybase@awsdeviqdb003 ~]$ exit
logout
[ec2-user@awsdeviqdb003 ~]$ exit
logout
Connection to 198.19.5.174 closed.
[skesarkar@iad1bastion01 AWS_access]$ ssh -i psilva_awsdevops001.pem ec2-user@198.19.6.6
The authenticity of host '198.19.6.6 (198.19.6.6)' can't be established.
RSA key fingerprint is fd:07:56:f2:b3:97:24:76:6d:0f:8e:8f:10:67:d4:a1.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '198.19.6.6' (RSA) to the list of known hosts.
Last login: Thu Jan  9 01:17:55 2020 from ip-198-19-11-212.ec2.internal

       __|  __|_  )  FrontLine PCI Hardened AMI
       _|  (     /   Red Hat Enterprise 7.6 | Version: 1.19.3
      ___|\___|___|  OS Documentation: https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/

[ec2-user@awsdeviqdb004 ~]$ sudo su - sybase
Last login: Tue Jan 21 22:39:31 UTC 2020 from ip-172-24-227-30.ec2.internal on pts/0
[sybase@awsdeviqdb004 ~]$ history | grep -i diqrlv7
  569  history | grep -i diqrlv7
[sybase@awsdeviqdb004 ~]$ hostname
awsdeviqdb004
[sybase@awsdeviqdb004 ~]$ exit
logout
[ec2-user@awsdeviqdb004 ~]$ exit
logout
Connection to 198.19.6.6 closed.
[skesarkar@iad1bastion01 AWS_access]$ ssh -i psilva_awsdevops001.pem ec2-user@198.19.5.128
Last login: Tue Feb 18 06:37:25 2020 from ip-172-24-227-30.ec2.internal

       __|  __|_  )  FrontLine PCI Hardened AMI
       _|  (     /   Red Hat Enterprise 7.6 | Version: 1.19.3
      ___|\___|___|  OS Documentation: https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/

[ec2-user@awsdeviqdb002 ~]$ sudo su -
Last login: Fri Feb  7 13:58:13 UTC 2020 on pts/0
[root@awsdeviqdb002 ~]# netstat -ntulp | grep -i 4205
[root@awsdeviqdb002 ~]# su - sybase
Last login: Tue Feb 18 06:38:14 UTC 2020 on pts/0
Sourcing SYBASE.sh
[sybase@awsdeviqdb002 ~]$ history | grep -i diqrlv7
  998  history | grep -i diqrlv7
 1000  history | grep -i diqrlv7
[sybase@awsdeviqdb002 ~]$




- Had a call with Iqbal.
- Sorted the issue of Pankaj and Ruksana.
- Worked on terraform template.


- Vinay Call.

  aws4psilva
  
  rhel5 ami-id : 
  [pablo@ip-198-19-11-212 awsprdasedb001]$ aws ec2 describe-images --owners 309956199498 --filters 'Name=name,Values=RHEL-7.5_HVM_GA*' 'Name=state,Values=available' --query 'reverse(sort_by(Images, &CreationDate))[:1].ImageId' --output text
		ami-6871a115


Instance type : m5.2xlarge
Security Grp : aws6-db-sg
 (awsprdasedb001)Private IP: 10.6.5.44
	subnet : subnet-057f942ef041b44ec
	
- Added Vinay public key for the user "ec2-user" user on both the hosts.

[ec2-user@ip-10-6-5-44 ~]$ df -h
Filesystem      Size  Used Avail Use% Mounted on
devtmpfs         16G     0   16G   0% /dev
tmpfs            16G     0   16G   0% /dev/shm
tmpfs            16G   17M   16G   1% /run
tmpfs            16G     0   16G   0% /sys/fs/cgroup
/dev/nvme0n1p2  160G  2.8G  158G   2% /
/dev/nvme1n1     50G   53M   47G   1% /opt
/dev/nvme2n1     50G   53M   47G   1% /data01
tmpfs           3.1G     0  3.1G   0% /run/user/1000
[ec2-user@ip-10-6-5-44 ~]$ cat /etc/redhat-release
Red Hat Enterprise Linux Server release 7.7 (Maipo)

 
 (awsprdasedb002)Private IP: 10.6.6.140
	subnet : subnet-0fdca6ee4ae3686b2
	
[ec2-user@ip-10-6-6-140 ~]$ df -h
Filesystem      Size  Used Avail Use% Mounted on
devtmpfs         16G     0   16G   0% /dev
tmpfs            16G     0   16G   0% /dev/shm
tmpfs            16G   17M   16G   1% /run
tmpfs            16G     0   16G   0% /sys/fs/cgroup
/dev/nvme0n1p2  160G  2.8G  158G   2% /
/dev/nvme1n1     50G   53M   47G   1% /opt
/dev/nvme2n1     50G   53M   47G   1% /data01
tmpfs           3.1G     0  3.1G   0% /run/user/1000
[ec2-user@ip-10-6-6-140 ~]$ cat /etc/redhat-release
Red Hat Enterprise Linux Server release 7.7 (Maipo)


----------------- 19 Feb 2020 --------------------
- Resolved issue of Pankaj.
- Resolved issue of Ankush.
- Resolved issue of Nikita Channawar.
- Resolved issue of Nidhi.
	https://bamboo.di-infra.sap.corp/download/LIV-LSC9-BWM/build_logs/LIV-LSC9-BWM-71.log
	
	build	19-Feb-2020 08:22:11	Step 1/13 : FROM dbs-docker-repository.docker.repositories.sap.ondemand.com/openjdk-8-sapdi
	error	19-Feb-2020 08:22:15	Get https://dbs-docker-repository.artifactory-prod.sapcdn.io/v2/openjdk-8-sapdi/manifests/latest: unknown: Authentication is required
	simple	19-Feb-2020 08:22:15	Failing task since return code of [/usr/bin/docker build --no-cache=true --force-rm=true --pull --tag=dbs-docker-repository.docker.repositories.sap.ondemand.com/llk-sms-channel:2.0.1.132-SNAPSHOT /root/bamboo-agent-home/xml-data/build-dir/LIV-LSC9-BWM] was 1 while expected 0
	error	19-Feb-2020 08:22:15	Error occurred while running Task '(8)' of type com.atlassian.bamboo.plugins.bamboo-docker-plugin:task.docker.cli.
	error	19-Feb-2020 08:22:15	com.atlassian.bamboo.task.TaskException: Failed to execute task

	
[c5246840@llbpal74 ~]$ docker login https://dbs-docker-repository.docker.repositories.sap.ondemand.com
Username: dbs-docker-user
Password:
WARNING! Your password will be stored unencrypted in /sapmnt/HOME/c5246840/.docker/config.json.
Configure a credential helper to remove this warning. See
https://docs.docker.com/engine/reference/commandline/login/#credentials-store

Login Succeeded
[c5246840@llbpal74 ~]$ docker ps | grep -i bamboo
09b07a98e008        c2da060dc68c                                                              "/entrypoint.sh"          4 days ago          Up 4 days                                    bamboo-server-1
[c5246840@llbpal74 ~]$ docker exec -it 09b07a98e008 sh
sh-4.2# docker login https://dbs-docker-repository.docker.repositories.sap.ondemand.com
Username: dbs-docker-user
Password:
WARNING! Your password will be stored unencrypted in /root/.docker/config.json.
Configure a credential helper to remove this warning. See
https://docs.docker.com/engine/reference/commandline/login/#credentials-store

Login Succeeded

- Issue, https://bamboo.di-infra.sap.corp/browse/LIV-LSC9-71/log
- Issue got resolved, https://bamboo.di-infra.sap.corp/browse/LIV-LSC9-72 .

- Resolved ravi jagani issue of build, https://bamboo.di-infra.sap.corp/download/LIV-LLKAN-MBD/build_logs/LIV-LLKAN-MBD-50.log

- Resolved ravi issue, https://bamboo.di-infra.sap.corp/browse/LIV-LLKAN-52 .


------------------ 20 Feb 2020 --------------------------

- 110
SSH (22)
TCP (6)
22
172.24.224.0/20
ALLOW

----

[ec2-user@ip-10-6-6-124 .ssh]$ ssh -i aws4psilva.pem ec2-user@10.6.5.127
The authenticity of host '10.6.5.127 (10.6.5.127)' can't be established.
ECDSA key fingerprint is SHA256:y4ymsOAVTuCD89A3X0DMxqvMs8TtGAFk2UkOPq+JVxU.
ECDSA key fingerprint is MD5:69:44:36:c7:85:0d:a5:c9:a3:42:51:68:af:94:c5:20.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '10.6.5.127' (ECDSA) to the list of known hosts.
Permission denied (publickey,gssapi-keyex,gssapi-with-mic).

- Worked on ticket https://jira.di-infra.sap.corp/browse/AMK-154, recreated the instances.


----

- saved docker container locally.

[skesarkar@us4cipapi01 ~]$ docker ps | grep -i ui
d3a68a6e81df        dbs-docker-repository.docker.repositories.sapcdn.io/llk-portal-ui-2geng:2.0.1.14-SNAPSHOT           "/bin/sh -c 'bash -c…"    6 weeks ago         Up 4 weeks               0.0.0.0:7443->15910/tcp                                                                                                                             llk-portal-ui-2geng
[skesarkar@us4cipapi01 ~]$ ls
[skesarkar@us4cipapi01 ~]$
[skesarkar@us4cipapi01 ~]$ docker save d3a68a6e81df > llk-portal-ui-2geng.tar.bkp.20Feb2020
Error response from daemon: No such image: d3a68a6e81df
[skesarkar@us4cipapi01 ~]$ docker save dbs-docker-repository.docker.repositories.sapcdn.io/llk-portal-ui-2geng:2.0.1.14-SNAPSHOT > llk-portal-ui-2geng.tar.bkp.20Feb2020
[skesarkar@us4cipapi01 ~]$ ls
llk-portal-ui-2geng.tar.bkp.20Feb2020

[skesarkar@us4cipapi01 ~]$ docker ps | grep -i ui
d3a68a6e81df        dbs-docker-repository.docker.repositories.sapcdn.io/llk-portal-ui-2geng:2.0.1.14-SNAPSHOT           "/bin/sh -c 'bash -c…"    6 weeks ago         Up 4 weeks               0.0.0.0:7443->15910/tcp                                                                                                                             llk-portal-ui-2geng

- Check all communication on Slack.
- Had one hour call with 2GEN team.


---------------- 24 Feb 2020 ---------------------------
- Generate proper report for 23-Feb-2020.

	 zcat messages-2020-02-10.gz | grep -i "^Feb  9" >  messages-2020-02-09
	 gzip -c messages-2020-02-09 > messages-2020-02-09.gz
	 bash -x /opt/llk/cronscripts/cron_report.sh
	 
	- reset the cat /etc/anacrontab ---- START_HOURS_RANGE=0-3
	 
- Please reset the Unix BDTrust password for username "gpatole" and "szuha". There email address
are "gitanjali.patole@sap.com" and "sheikh.zuha@sap.com".

Thank you.	 

- Personnel access token : 104e9819c32a0fc3ff43c37c0e3c1dd9650277c9
104e9819c32a0fc3ff43c37c0e3c1dd9650277c9


Hi Team,

We are not able to create/write any file on "/aws_logs-general/" for host "us4llkapi003".

- Shikha issue resolved, https://bamboo.di-infra.sap.corp/deploy/viewDeploymentResult.action?deploymentResultId=135397996 .

- setting the kubectl.



apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRFNU1EVXlOREl3TVRneU0xb1hEVEk1TURVeU1USXdNVGd5TTFvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTlNnCjg3SXI2ZG4xa25TaTNWZmN4Z3FZRU1xS1AwZC9ldUVLRXRHajZweUdSTmVkTG1BOS9LQ1ZLUFBxYzZOUGFVODkKY28rdzRmWE9WcTVBaHEzMEtHUHRvYVdkQlJHVzl5MkRCNlFiSXRqUGIzcGhsNkMyWm1OcTNSYTdkT2V4RVZ3cgpTV1NYOGNqeWdBUGY3QzVOcFN0QWt3dVo5VklGWXpwY2VBbUREZlN5QUNWSFV0U2NUQnZRRWxTbWwvZTBrWFJpCndKcjBLU3JOZXgwY2RSOFZqY0JNdWR1SnlwMkRJN0UzcVF6V01Ib21lL1NnYnRQUFA2TEUrMm9VVy9QZk1hTkEKK0lVMWN3K3hqN2lZVzVSNGNENWJhVGxuY0FvSERWMFZSYnVrZVJzVDNxL1AzeTE2Tmo4ZlRWZEV0RDVneDg0bQpFMGJZL0hHd3h5L1RsUzlVYTg4Q0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFEN05rWlF1d0gxMnRVNXR4NmtRNHB6K2ZTY0MKRmNhajFhTlJuNm1CUExjSG5uekFvZ0lpN0J0WjNNTSt1bFZWeW10YnlUQVZVZEVJSm5nYndRcDVDZ0pCT29EVwpmS2FMOTFYT1lYaFVGWFpCSU50M05VTTVzUTlvSlJ0SHQreVY0MmEvVUc5YUsxSDc1OWQwcXZramtsUURKMHZxCndqSUNYNU5yd1NnYWVRalVDMEtENWZoK1dJaEd5OFNwTmx6Nk5XV1VxNmFBL00zb3VsWFlHcStjK0FFMmc3cXoKTEI3dW5tWnVqSVBIYzcvSWJRQkVNMXY2MlFMMnZqdlNyOUZHcUR2ZzhXWVBhb1VMbGVnRGU3M2M4a0F0cnpGYgpSK3dNd0JnZVNLTWNraUp2Sm01eFVPTU9oUmkzalZUMVhqZU9hR01oNVFOY1FWb2hDOTJuZmtOU1Z4MD0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    server: https://68AE191FA7A3DDEE75213EF94880E592.sk1.us-east-1.eks.amazonaws.com
  name: arn:aws:eks:us-east-1:079549895456:cluster/eks-dev-cluster
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJd01ERXlNekU0TVRFeE1sb1hEVE13TURFeU1ERTRNVEV4TWxvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTDYxCm1UY2xzb2xrOFBPS1lONFM1cXYzaXA5QXVpNWxkOWhLeDFBNTU5KzFIVVovbzF1M0xKbDlmQ0tSUGhUNy9qMVMKWVpxclhMUnVWaE5BKzNnQjZ4c2pRM1hWSHQyd1p6L3Ardk9Mc1Qzb09MeGJ5VG05eGdZZERacEl4blY5T1V5VQozYUppVFhQRVRhRDZVK2RNUGNBL1dpUXpoeU9VdnBYUnRSSTlNVkd4M2JMMFR0TUsrZURkY1JHaGpWaGxuNXp2CjBoVHVjQVlLTExma2pPSEVqYWExNTBiVjVYb2pveWNPTU1xY2hpSCt4Mk95d3ZWbHM3azJRUk02UFdWMTcwaXUKa3BhNnRyREdSY2gzc0h0YXlZSEdrVnF0eTV2UU5FV3lMSVJVRThQWnZ6c0xHZE92YUlFcHplSzRjaEFiYUJqbApwdkFOM1VabGJEbDYrd1VYb3E4Q0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFDYXRKaXVuTE4wUkg4SEszQ2JSYk9QaCthMFQKRDlTUFJEOU5QbUFiWXczUG1NVVFWQnlCcGdyMmwrYVBjNDErMWgyZTd0VnhWeWhlSVNWdmFTaFYxcEhPcisyaQpmZ29ENFdRakQ4dThtV2Nyc0hVVzFXS0FXYUxYeFZwaFZSQys1amdZSERsWGhNVFQvb056YzRrTVZpdldIYkFlCkQrVDl1RnhNN2FmZHkwbVUvaEtuMVVicVFHQjRFamhtbzEvNHNQcThsRnRuVVNRRFZBNmVIUSsvT2V6QndUMHYKRldTN3B5WnpSRVByOFpJRnd4SWpHUGp5Y3MxL3VnVUhjb1lLMFdhVElGV21nSGhOSWltakJKcWFidDdMeDgwVwpyYnNaUERHczlGbENhdGtUbkVOVDJXTHhWMnN5SS8xZVpoeFZoRmUxcTZBeU9SbFEvTUlxeUIzbk9Oaz0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    server: https://8E48F1EC1BA388291978C89C839A4484.gr7.us-east-1.eks.amazonaws.com
  name: arn:aws:eks:us-east-1:161952870685:cluster/eks-aws6-cluster
contexts:
- context:
    cluster: arn:aws:eks:us-east-1:079549895456:cluster/eks-dev-cluster
    namespace: llk
    user: arn:aws:eks:us-east-1:079549895456:cluster/eks-dev-cluster
  name: arn:aws:eks:us-east-1:079549895456:cluster/eks-dev-cluster
- context:
    cluster: arn:aws:eks:us-east-1:161952870685:cluster/eks-aws6-cluster
    user: arn:aws:eks:us-east-1:161952870685:cluster/eks-aws6-cluster
  name: arn:aws:eks:us-east-1:161952870685:cluster/eks-aws6-cluster
current-context: arn:aws:eks:us-east-1:079549895456:cluster/eks-dev-cluster
kind: Config
preferences: {}
users:
- name: arn:aws:eks:us-east-1:079549895456:cluster/eks-dev-cluster
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1alpha1
      args:
      - token
      - -i
      - eks-dev-cluster
      command: aws-iam-authenticator
      env: null
- name: arn:aws:eks:us-east-1:161952870685:cluster/eks-aws6-cluster
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1alpha1
      args:
      - --region
      - us-east-1
      - eks
      - get-token
      - --cluster-name
      - eks-aws6-cluster
      command: aws
      env:
      - name: AWS_PROFILE
        value: aws6
- name: sachin.kesarkar@sap.com@eks-aws6-cluster.us-east-1.eksctl.io
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1alpha1
      args:
      - token
      - -i
      - eks-aws6-cluster
      command: aws-iam-authenticator
      env: null

---- Approving pull request for JIRA, https://github.wdf.sap.corp/SMCS/2gen-frontend-reverse-proxy/pull/1
---- Rajul deployment check on Slack private.

---- Had a call with Abhilekh regarding UI and proxy changes for certificates. Check slack.

---- Checked livelink_Devops slack chat.

---- awsdev deployment, created mmg_db database.

-bash:2209::sybase@ip-198-19-6-159:~/scripts/mmg: cat mmg_db.24feb2020.out
1> /* MM Database Creation Script
2>  Create Data Device
3>  Create Log Device
4>  Create Database
5>  Create Initial User Role
6>  Assign User role to admin user
7>  Create Admin User
8> */
9> use master
1>
2> PRINT '<<<< DROP DATABASE mmg_db>>>>'
<<<< DROP DATABASE mmg_db>>>>
1>
2>
3> IF EXISTS (SELECT 1 FROM master.dbo.sysdatabases
4> WHERE name = 'mmg_db')
5>  DROP DATABASE mmg_db
1>
2>
3> IF (@@error != 0)
4>  BEGIN
5>  PRINT 'Error dropping database mmg_db'
6>  SELECT syb_quit()
7>  END
1>
2> -----------------------------------------------------------------------------
3> -- DDL for DatabaseDevice 'mmg_db_data01'
4> -----------------------------------------------------------------------------
5>
6> print '<<<<< CREATING DatabaseDevice - "mmg_db_data01" >>>>>'
<<<<< CREATING DatabaseDevice - "mmg_db_data01" >>>>>
1>
2> use master
1>
2> IF EXISTS (SELECT 1 FROM master.dbo.sysdevices WHERE name = 'mmg_db_data01')
3>  exec sp_dropdevice mmg_db_data01
4> --remove files manually from system rm /sapci/servers/DTSILOB4/data/mmg_db_data01
5> --folders may differ in each environment
6>
7> IF (@@error != 0)
8>  BEGIN
9>  PRINT 'Error CREATING DatabaseDevice mmg_db_data01'
10>  SELECT syb_quit()
11>  END
1>
2> --folders may differ for each environment
3> disk init name = 'mmg_db_data01',
4> physname = '/sapci/servers/DTSILOB4/data/mmg_db_data01.dat',
5> size = '4096M' , directio = true, skip_alloc = true
Warning: The 'skip_alloc' option of disk init/disk resize command is turned on, and does not ensure allocation of the specified number of pages. Please make sure file system has enough space for future operations.
1>
2>
3>
4> -----------------------------------------------------------------------------
5> -- DDL for DatabaseDevice 'mmg_db_log01'
6> -----------------------------------------------------------------------------
7>
8> print '<<<<< CREATING DatabaseDevice - "mmg_db_log01" >>>>>'
<<<<< CREATING DatabaseDevice - "mmg_db_log01" >>>>>
1>
2> use master
1>
2>
3> IF EXISTS (SELECT 1 FROM master.dbo.sysdevices WHERE name = 'mmg_db_log01')
4>  exec sp_dropdevice mmg_db_log01
5> --remove files manually from system rm /sapci/servers/DTSILOB4/data/mmg_db_dev_log01
6>
7> IF (@@error != 0)
8>  BEGIN
9>  PRINT 'Error CREATING DatabaseDevice mmg_db_log01'
10>  SELECT syb_quit()
11>  END
1>
2> disk init name = 'mmg_db_log01',
3> physname = '/sapci/servers/DTSILOB4/data/mmg_db_log01.dat',
4> size = '2048M' , directio = true, skip_alloc = true
Warning: The 'skip_alloc' option of disk init/disk resize command is turned on, and does not ensure allocation of the specified number of pages. Please make sure file system has enough space for future operations.
1>
2>
3> USE master
1>
2>
3> PRINT '<<<< CREATE DATABASE mmg_db>>>>'
<<<< CREATE DATABASE mmg_db>>>>
1>
2>
3> IF EXISTS (SELECT 1 FROM master.dbo.sysdatabases
4> WHERE name = 'mmg_db')
5>  DROP DATABASE mmg_db
1>
2>
3> IF (@@error != 0)
4>  BEGIN
5>  PRINT 'Error dropping database mmg_db'
6>  SELECT syb_quit()
7>  END
1>
2>
3> CREATE DATABASE mmg_db
4> ON mmg_db_data01 = '4096M'
5> LOG ON mmg_db_log01 = '2048M'
6> --WITH DURABILITY = FULL
CREATE DATABASE: allocating 1048576 logical pages (4096.0 megabytes) on disk 'mmg_db_data01' (1048576 logical pages requested).
CREATE DATABASE: allocating 524288 logical pages (2048.0 megabytes) on disk 'mmg_db_log01' (524288 logical pages requested).
Processed 615 allocation unit(s) out of 6144 units (allocation page 79360). 10% completed.
Processed 1229 allocation unit(s) out of 6144 units (allocation page 1204736). 20% completed.
Processed 1844 allocation unit(s) out of 6144 units (allocation page 1283328). 30% completed.
Processed 2458 allocation unit(s) out of 6144 units (allocation page 315648). 40% completed.
Processed 3072 allocation unit(s) out of 6144 units (allocation page 394240). 50% completed.
Processed 3687 allocation unit(s) out of 6144 units (allocation page 1519104). 60% completed.
Processed 4301 allocation unit(s) out of 6144 units (allocation page 576512). 70% completed.
Processed 4916 allocation unit(s) out of 6144 units (allocation page 733952). 80% completed.
Processed 5530 allocation unit(s) out of 6144 units (allocation page 891136). 90% completed.
Processed 6144 allocation unit(s) out of 6144 units (allocation page 1048320). 100% completed.
Database 'mmg_db' is now online.
1>
2> checkpoint
1>
2> use mmg_db
1> sp_addlogin mmg_admin_db_user, D2pB9uRC, mmg_db
Password correctly set.
Account unlocked.
New login created.
(return status = 0)
1>
1> sp_addlogin mmg_db_user, IYX8rW2y, mmg_db
Password correctly set.
Account unlocked.
New login created.
(return status = 0)
1>
2> use master
1>
2> create role mmg_user_role
1>
2> grant role mmg_user_role to mmg_admin_db_user
1>
2> grant role mmg_user_role to mmg_db_user
1>
2> use mmg_db
3> --go
4> --sp_dropalias mmg_admin_db_user
1> sp_addalias mmg_admin_db_user, dbo
Alias user added.
(return status = 0)
1> sp_addalias mmg_db_user, dbo
Alias user added.
(return status = 0)
-bash:2210::sybase@ip-198-19-6-159:~/scripts/mmg:

- Set the crontab for llbpal74.

 [root@llbpal74 ~]# crontab -l
# Chef Name: chef_client
0 * * * * /opt/chef/embedded/bin/ruby /etc/chef/agent.rb > /etc/chef/agent.log
*/20 * * * * docker exec -ti bamboo-server-1 bash -c 'docker login -u dbs-docker-user -p n813nZw22a2eVj4N https://dbs-docker-repository.docker.repositories.sap.ondemand.com > /dev/null && docker login -u dbs-docker-user -p n813nZw22a2eVj4N https://dbs-docker-repository.docker.repositories.sapcdn.io' > /dev/null

	-- summary --
	- Compared the configuration of us4geo and us4ciapi04 for report generation, they have similar configuration.
	- But had made little change in the /etc/anacrontab file, having doubt that because of huge log size it might be taking some time, so increased the windows.
	- Earlier it was 3-4, made it 0-1 (which showed good improvement). Now made it 0-3.
- Tried to generate the report on us4geo but was not able to write/create any file, raised issue with SysEng. ISS-1851
- Reset BDTrust password for user "gpatole" and "szuha" - ISS-1850
- Resolved the build/deploy its not the full list,
	https://bamboo.di-infra.sap.corp/deploy/viewDeploymentResult.action?deploymentResultId=135397993
	https://bamboo.di-infra.sap.corp/browse/LIV-LLKSMSEV-23/log
	https://bamboo.di-infra.sap.corp/deploy/viewDeploymentResult.action?deploymentResultId=135397996
- Reviewed and approved bunch of pull requests.
- Had a call/discussion with Abhilekh, Nidhi regarding the deployment of certificates in the UI part. We were able to resolve those.
- Fixed permanently the docker registry login issue by adding a cronjob on llbpal74.
- Recreated the mmg database, earlier the database name and users all were starting with eng. Now prefixed them with mmg.
- Tried to set the kubectl for my username "sachin", but facing some issues.
- Modification of templates for "mmg" is in progress, still working on that.


------------- 25 Feb 2020 -------------
- Internet slow issue.


-------------- 26 Feb 2020 ------------
- Worked on Rahul vyas request.
- Worked with Anna.
- Call with Bhomik.
- Check us4cidemo002 AAT CPU usage.

- Analysing and working on failure rate increase to 100 %, Service method: /mo-inbound/v1/message/receive .
	https://apm.cf.us10.hana.ondemand.com/e/f95fd477-ba09-47d0-ba91-d6f35a4e2f22/#failureanalysis;sci=SERVICE-094671BC6423E9AB;timeframe=custom1582701180000to1582701780000;servicefilter=0%1E9%11SERVICE_METHOD-69A491C13FA41ABC%14%5C0mo-inbound%5C0v1%5C0message%5C0receive%103%110;gf=all
	
- Related to unlinked accounts.
- 

 zcat messages-2020-02-26.gz | grep -i "^Feb 25" >  messages-2020-02-25
 	 gzip -c messages-2020-02-09 > messages-2020-02-09.gz
	 bash -x /opt/llk/cronscripts/cron_report.sh
	 
	 gzip -c messages-2020-02-25 > messages-2020-02-25.gz
	 
- Nidhi pull request.

- Barani. [‎26-‎02-‎2020 15:06]  Sankaran, Barani Sathishwaran:  
https://github.wdf.sap.corp/ms/ctool-ui.git 

[‎26-‎02-‎2020 15:27]  Sankaran, Barani Sathishwaran:  
https://foss-service.wdf.sap.corp/ui 

	- Had a call with him.
	- Worked from 4:00 to 8:00 on the same issue.


-------------- 27 Feb 2020 ------------	
- Had worked on deployments.



-------------- 28 Feb 2020 ------------
- Had a 2 hour call with CRM issue.
- Had a 1 hour call with Pablo regarding mm7.
- 2 hour call with Abhilekh for performance proxy.



-------------- 3 Feb 2020 -----------------
- Resolved AAT issue.
- Resolved Shikha Shah issue, build. Slack chat.
- Call with Abhilekh on UI-2GEN deployments.
	https://bamboo.di-infra.sap.corp/deploy/viewDeploymentResult.action?deploymentResultId=135398303
	https://bamboo.di-infra.sap.corp/deploy/viewDeploymentResult.action?deploymentResultId=135398304
	
- Dipti issue,
https://bamboo.di-infra.sap.corp/download/LIV-LLKAC0-BWM/build_logs/LIV-LLKAC0-BWM-381.log


zcat messages-2020-03-02.gz | grep -i "^Mar  1" > messages-2020-03-01

gzip -c messages-2020-03-10 > messages-2020-03-10.gz

- 

set $cert "-----BEGIN CERTIFICATE-----\nMIIDHjCCAgYCCQC09G8CbZ6yMDANBgkqhkiG9w0BAQUFADBRMQswCQYDVQQGEwJV\nUzEPMA0GA1UECAwGUmVzdG9uMREwDwYDVQQHDAhQb3J0bGFuZDEMMAoGA1UECgwD\nU0FQMRAwDgYDVQQDDAdzYXAuY29tMB4XDTE3MDMyMzAzMTU1NVoXDTIwMDMyMjAz\nMTU1NVowUTELMAkGA1UEBhMCVVMxDzANBgNVBAgMBlJlc3RvbjERMA8GA1UEBwwI\nUG9ydGxhbmQxDDAKBgNVBAoMA1NBUDEQMA4GA1UEAwwHc2FwLmNvbTCCASIwDQYJ\nKoZIhvcNAQEBBQADggEPADCCAQoCggEBAM5UO8v2ofvVjak7NgY/gfcZSyYNi9Np\n4z6bcopEbvbHHhaE+K0L1S2AOoRsuma2SLuBhH9H7KdeFv1ELcWgIthmhMr7CJp4\nFW/iZv2MC8BKWSXUYGO8O89NQWdFcNpppltaavUNOixrKtIuG7jrhmDnJFsQZUZ6\nkGSb+GroNen/bIySaa5f5ZFS4n9TQKd64x/Kik0B34FQpzb+qheLxl2XH4VwhiBy\n9FRcmgLpeh3CULQD2XOAgJWrSO2MNSaD0FHC9BFp37Paf6rycZO5xySz66TBoM0+\nP0qN9UBZWcdhV6S0ya+XnxVxsFEk2lCVbfvqyX13CSuttvbqCMbbNc8CAwEAATAN\nBgkqhkiG9w0BAQUFAAOCAQEAmC3ssfqVdMc8omyxzY4cJjTgT2gejPUEZRrjz2ge\nLFR5FG9BJdhVwsn9fmdoH1G3dAA4nynRgbMIVt2FTKnnCQflo/E950UrFryzYY0t\nj5hsUV6YXGTY4Tl9m16YtaLeaR9B6OaT44oigmJ9CSKtCHCkAvnQ717b/y/ZY221\nVs69EMy1v6iL2z7C0TcmTAGi+XtIh8TLFcihS9BEkhj7So63CJtVEiJE9HWxA5CP\n9HuGb7f2w24XqS5BDuWY9O5on8e8IVbTH7sWRA3NpwlhTmfWX1SDSBuAacAvwvC9\nvNxVKxWwEUuVkA8ENGo5NFIXXd+VVQggEr6Y0QKKHznb0A==\n-----END CERTIFICATE-----";


set $cert "-----BEGIN CERTIFICATE-----\nMIIDUjCCAjoCCQDTwddPSLm8jzANBgkqhkiG9w0BAQUFADBrMQswCQYDVQQGEwJV\nUzERMA8GA1UECBMIVmlyZ2luaWExDzANBgNVBAcTBlJlc3RvbjEMMAoGA1UEChMD\nU0FQMRgwFgYDVQQLEw9Nb2JpbGUgU2VydmljZXMxEDAOBgNVBAMTB3NhcC5jb20w\

- Approved the Abhilekh changes. Was able to successfully make changes.

- Made changes to the Wiki, https://wiki.wdf.sap.corp/wiki/display/CI365/6.+Secure+Software+Development+Lifecycle for Single user login.


----------------------- 4 March 2020 ----------------------------
- Worked on LLK Geo, refreshing the token.
- Worked with Abhilekh.

- Worked with Viney on fixing Sybase on AWS nodes.

[root@ip-10-6-5-188 ~]#
[root@ip-10-6-5-188 ~]# ps aux | grep -i sybase
root     25647  0.0  0.0 241228  4588 pts/0    S    08:14   0:00 sudo su - sybase
root     25649  0.0  0.0 191936  2440 pts/0    S    08:14   0:00 su - sybase
sybase   25650  0.0  0.0 115448  2064 pts/0    S    08:14   0:00 -bash
sybase   25671  1.2  0.8 3666700 265412 pts/0  Sl+  08:15   0:34 /tmp/install.dir.25671/Linux/resource/jre/bin/java -Djava.compiler=NONE -Xms20971520 com.zerog.lax.LAX /tmp/install.dir.25671/temp.lax /tmp/env.properties.25671
sybase   25887  0.0  0.0 113320  1448 pts/0    S+   08:15   0:00 /bin/sh /tmp/ismp003/2685622.tmp
root     32582  0.0  0.0 112716   996 pts/1    S+   08:59   0:00 grep --color=auto -i sybase
[root@ip-10-6-5-188 ~]#
[root@ip-10-6-5-188 ~]#
[root@ip-10-6-5-188 ~]#
[root@ip-10-6-5-188 ~]# cat /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
10.6.5.188  ip-10-6-5-188.bd.trustec2.internal
10.6.6.57   ip-10-6-6-57.bd.trustec2.internal
[root@ip-10-6-5-188 ~]#
[root@ip-10-6-5-188 ~]#
[root@ip-10-6-5-188 ~]# kill 25671 25887
[root@ip-10-6-5-188 ~]#
[root@ip-10-6-5-188 ~]#
[root@ip-10-6-5-188 ~]# ps aux | grep -i sybase
root     25647  0.0  0.0 241228  4588 pts/0    S    08:14   0:00 sudo su - sybase
root     25649  0.0  0.0 191936  2440 pts/0    S    08:14   0:00 su - sybase
sybase   25650  0.0  0.0 115448  2064 pts/0    S+   08:14   0:00 -bash
root     32601  0.0  0.0 112712   996 pts/1    S+   09:01   0:00 grep --color=auto -i sybase
[root@ip-10-6-5-188 ~]#
[root@ip-10-6-5-188 ~]#
[root@ip-10-6-5-188 ~]# exit
logout
-bash-4.2$ exit
logout
Connection to 10.6.5.188 closed.
[skesarkar@iad1bastion01 AWS_access]$ ssh -i aws4psilva.pem ec2-user@10.6.6.57
The authenticity of host '10.6.6.57 (10.6.6.57)' can't be established.
RSA key fingerprint is f2:ba:98:54:af:9f:28:93:c8:1f:38:a2:27:80:80:dc.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '10.6.6.57' (RSA) to the list of known hosts.
Last login: Wed Mar  4 04:40:50 2020 from ip-172-24-227-30.ec2.internal
[ec2-user@ip-10-6-6-57 ~]$ sudo su -
Last login: Wed Mar  4 06:48:18 UTC 2020 on pts/0
[root@ip-10-6-6-57 ~]#
[root@ip-10-6-6-57 ~]# ps aux | grep -i sybase
root      3059  0.0  0.0 112712   996 pts/1    S+   09:03   0:00 grep --color=auto -i sybase
[root@ip-10-6-6-57 ~]#
[root@ip-10-6-6-57 ~]#
[root@ip-10-6-6-57 ~]#


-- Had a call with Abhilekh.

- Call regarding CRM prod down.

- https://bamboo.di-infra.sap.corp/deploy/viewDeploymentResult.action?deploymentResultId=137592932

- Enable full logging for select into and Alter table for performance DB.

	sp_dboption "sch_db", "full logging for select into", "true"
	go
	sp_dboption "sch_db", "full logging for alter table", "true"
	go
	
	
	
------------------- 5 March 2020 ---------------------
- AAT_CACHE_TYPE=redis
AAT_REDIS_SENTINEL_NODES=llbpal55:26380
AAT_REDIS_SENTINEL_MASTER=master01
AAT_REDIS_SENTINEL_NODES_OLD=llbpal53:26380,llbpal55:26380
AAT_REDIS_PASSWORD=ITiNoTmo

- AAT issue.

llbpal55:/var/log # redis-cli -h llbpal55 -p 6380 -a ITiNoTmo
llbpal55:6380> INFO REPLICATION
# Replication
role:slave
master_host:10.48.144.173
master_port:6380
master_link_status:down
master_last_io_seconds_ago:-1
master_sync_in_progress:0
slave_repl_offset:1
master_link_down_since_seconds:1583395415
slave_priority:100
slave_read_only:1
connected_slaves:0
master_repl_offset:0
repl_backlog_active:0
repl_backlog_size:1048576
repl_backlog_first_byte_offset:0
repl_backlog_histlen:0
llbpal55:6380> quit

---------

llbpal53:/var/log # redis-cli -h llbpal55 -p 6380 -a ITiNoTmo
llbpal55:6380> INFO REPLICATION
# Replication
role:slave
master_host:10.48.144.173
master_port:6380
master_link_status:up
master_last_io_seconds_ago:1
master_sync_in_progress:0
slave_repl_offset:9104
slave_priority:100
slave_read_only:1
connected_slaves:0
master_repl_offset:0
repl_backlog_active:0
repl_backlog_size:1048576
repl_backlog_first_byte_offset:0
repl_backlog_histlen:0

- Resolved the Rahul's issue of docker-compose by reinstalling it.

Traceback (most recent call last):
  File "/usr/bin/docker-compose", line 11, in <module>
    sys.exit(main())
  File "/usr/lib/python2.7/site-packages/compose/cli/main.py", line 64, in main
    command()
  File "/usr/lib/python2.7/site-packages/compose/cli/main.py", line 116, in perform_command
    handler(command, command_options)
  File "/usr/lib/python2.7/site-packages/compose/cli/main.py", line 876, in up
    remove_orphans=remove_orphans)
  File "/usr/lib/python2.7/site-packages/compose/project.py", line 385, in up
    warn_for_swarm_mode(self.client)
  File "/usr/lib/python2.7/site-packages/compose/project.py", line 590, in warn_for_swarm_mode
    info = client.info()
  File "/usr/lib/python2.7/site-packages/docker/api/daemon.py", line 90, in info
    return self._result(self._get(self._url("/info")), True)
  File "/usr/lib/python2.7/site-packages/docker/utils/decorators.py", line 47, in inner
    return f(self, args, *kwargs)
  File "/usr/lib/python2.7/site-packages/docker/api/client.py", line 183, in _get
    return self.get(url, **self._set_request_timeout(kwargs))
  File "/usr/lib/python2.7/site-packages/requests/sessions.py", line 498, in get
    return self.request('GET', url, **kwargs)
  File "/usr/lib/python2.7/site-packages/requests/sessions.py", line 486, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python2.7/site-packages/requests/sessions.py", line 598, in send
    r = adapter.send(request, **kwargs)
  File "/usr/lib/python2.7/site-packages/requests/adapters.py", line 370, in send
    timeout=timeout
  File "/usr/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.py", line 582, in urlopen
    timeout_obj = self._get_timeout(timeout)
  File "/usr/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.py", line 309, in _get_timeout
    return Timeout.from_float(timeout)
  File "/usr/lib/python2.7/site-packages/requests/packages/urllib3/util/timeout.py", line 154, in from_float
    return Timeout(read=timeout, connect=timeout)
  File "/usr/lib/python2.7/site-packages/requests/packages/urllib3/util/timeout.py", line 97, in _init_
    self._connect = self._validate_timeout(connect, 'connect')
  File "/usr/lib/python2.7/site-packages/requests/packages/urllib3/util/timeout.py", line 127, in _validate_timeout
    "int or float." % (name, value))
ValueError: Timeout value connect was Timeout(connect=60, read=60, total=None), but it must be an int or float.



---------- 5 March 2020 -------------------
- Things to do.
	- Diagram of MMG.
	- Force MMG team to use Repository, tag the repository.
	- RoadMap of MMG having P1 priority.
	
- Authentication failure for LLK while sending A2P traffic.

[root@us4llkapi003 log]# cat messages | grep -i "600F Authentication failure" | head -n 5
Mar  6 05:49:32 us4llkapi001.bd.trust llk-sms-channel/e26e6710571a[1798]: 600F Authentication failure.
Mar  6 05:49:33 us4llkapi003.bd.trust llk-sms-channel/cce615825529[43066]: 600F Authentication failure.
Mar  6 05:49:33 us4llkapi003.bd.trust llk-sms-channel/cce615825529[43066]: 600F Authentication failure.
Mar  6 05:49:34 us4llkapi002.bd.trust llk-sms-channel/489c9e7542aa[1801]: 600F Authentication failure.
Mar  6 05:49:34 us4llkapi002.bd.trust llk-sms-channel/489c9e7542aa[1801]: 600F Authentication failure.
....
....
Mar  6 06:07:49 us4llkapi002.bd.trust llk-sms-channel/489c9e7542aa[1801]: 600F Authentication failure.
Mar  6 06:07:49 us4llkapi001.bd.trust llk-sms-channel/e26e6710571a[1798]: 600F Authentication failure.
Mar  6 06:07:49 us4llkapi002.bd.trust llk-sms-channel/489c9e7542aa[1801]: 600F Authentication failure.
Mar  6 06:07:49 us4llkapi001.bd.trust llk-sms-channel/e26e6710571a[1798]: 600F Authentication failure.
Mar  6 06:07:49 us4llkapi002.bd.trust llk-sms-channel/489c9e7542aa[1801]: 600F Authentication failure.


- 1 hour call with Pablo, Ruksana.


-------------- 9 March 2020 --------------------
- AAT QA issue.

2020-03-08T23:22:00-07:00 llbpal52.pal.sap.corp aat-api/acf034044c13[15465]: #011at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [na:1.8.0_171]
2020-03-08T23:22:00-07:00 llbpal52.pal.sap.corp aat-api/acf034044c13[15465]: #011at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_171]
2020-03-08T23:22:00-07:00 llbpal52.pal.sap.corp aat-api/acf034044c13[15465]: #011at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171]
2020-03-08T23:22:00-07:00 llbpal52.pal.sap.corp aat-api/acf034044c13[15465]: Caused by: redis.clients.jedis.exceptions.JedisExhaustedPoolException: Could not get a resource since the pool is exhausted
2020-03-08T23:22:00-07:00 llbpal52.pal.sap.corp aat-api/acf034044c13[15465]: #011at redis.clients.util.Pool.getResource(Pool.java:53) ~[jedis-2.10.2.jar!/:na]
2020-03-08T23:22:00-07:00 llbpal52.pal.sap.corp aat-api/acf034044c13[15465]: #011at redis.clients.jedis.JedisSentinelPool.getResource(JedisSentinelPool.java:214) ~[jedis-2.10.2.jar!/:na]
2020-03-08T23:22:00-07:00 llbpal52.pal.sap.corp aat-api/acf034044c13[15465]: #011at redis.clients.jedis.JedisSentinelPool.getResource(JedisSentinelPool.java:17) ~[jedis-2.10.2.jar!/:na]
2020-03-08T23:22:00-07:00 llbpal52.pal.sap.corp aat-api/acf034044c13[15465]: #011at org.springframework.data.redis.connection.jedis.JedisConnectionFactory.fetchJedisConnector(JedisConnectionFactory.java:276) ~[spring-data-redis-2.0.14.RELEASE.jar!/:2.0.14.RELEASE]
2020-03-08T23:22:00-07:00 llbpal52.pal.sap.corp aat-api/acf034044c13[15465]: #011... 21 common frames omitted
2020-03-08T23:22:00-07:00 llbpal52.pal.sap.corp aat-api/acf034044c13[15465]: Caused by: java.util.NoSuchElementException: Timeout waiting for idle object
2020-03-08T23:22:00-07:00 llbpal52.pal.sap.corp aat-api/acf034044c13[15465]: #011at org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:447) ~[commons-pool2-2.4.3.jar!/:2.4.3]
2020-03-08T23:22:00-07:00 llbpal52.pal.sap.corp aat-api/acf034044c13[15465]: #011at org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:361) ~[commons-pool2-2.4.3.jar!/:2.4.3]
2020-03-08T23:22:00-07:00 llbpal52.pal.sap.corp aat-api/acf034044c13[15465]: #011at redis.clients.util.Pool.getResource(Pool.java:50) ~[jedis-2.10.2.jar!/:na]
2020-03-08T23:22:00-07:00 llbpal52.pal.sap.corp aat-api/acf034044c13[15465]: #011... 24 common frames omitted
2020-03-08T23:22:00-07:00 llbpal52.pal.sap.corp aat-api/acf034044c13[15465]:


- Redis troubleshooting.

c5246840@llbpal55:~> /opt/redis/bin/redis-cli -a ITiNoTmo -p 6380
127.0.0.1:6380> INFO REPLICATION
# Replication
role:slave
master_host:10.48.144.173
master_port:6380
master_link_status:up
master_last_io_seconds_ago:0
master_sync_in_progress:0
slave_repl_offset:6487213
slave_priority:100
slave_read_only:1
connected_slaves:0
master_replid:54874e9d7e8d35205b17660b4896a463fb58f7f2
master_replid2:0000000000000000000000000000000000000000
master_repl_offset:6487213
second_repl_offset:-1
repl_backlog_active:1
repl_backlog_size:1048576
repl_backlog_first_byte_offset:5438638
repl_backlog_histlen:1048576


-----------

c5246840@llbpal53:~> redis-cli -a ITiNoTmo -p 6380
127.0.0.1:6380> INFO REPLICATION
# Replication
role:master
connected_slaves:1
slave0:ip=10.48.144.175,port=6380,state=online,offset=6469413,lag=1
master_replid:54874e9d7e8d35205b17660b4896a463fb58f7f2
master_replid2:0000000000000000000000000000000000000000
master_repl_offset:6469413
second_repl_offset:-1
repl_backlog_active:1
repl_backlog_size:1048576
repl_backlog_first_byte_offset:5420838
repl_backlog_histlen:1048576
127.0.0.1:6380> exit


---------------- logs ----------------------

-- 53 logs --

c5246840@llbpal53:/var/log>  tail -f redis_sentinel_26380.log
44064:X 08 Mar 12:26:41.667 # +try-failover master master01 10.48.144.173 6380
44064:X 08 Mar 12:26:41.669 # +vote-for-leader e1da411f5c3dd038e3d1b65dd7938263058411dd 231006
44064:X 08 Mar 12:26:41.671 # 8a9e89ecf0b9b0e4d64084bb23e1b6fd0a437cce voted for e1da411f5c3dd038e3d1b65dd7938263058411dd 231006
44064:X 08 Mar 12:26:52.322 # -failover-abort-not-elected master master01 10.48.144.173 6380
44064:X 08 Mar 12:26:52.375 # Next failover delay: I will not start a failover before Sun Mar  8 12:27:12 2020
44943:X 08 Mar 12:27:09.651 * Increased maximum number of open files to 10032 (it was originally set to 1024).
44943:X 08 Mar 12:27:09.651 # Creating Server TCP listening socket 0.0.0.0:26380: bind: Address already in use
44064:X 08 Mar 12:27:11.079 # -sdown master master01 10.48.144.173 6380
44064:X 08 Mar 12:27:11.079 # -odown master master01 10.48.144.173 6380
44064:X 08 Mar 12:34:29.598 # -sdown slave 10.48.144.175:6380 10.48.144.175 6380 @ master01 10.48.144.173 6380
^C
c5246840@llbpal53:/var/log> tail -f redis_sentinel_6380.log
44918:M 08 Mar 23:18:17.964 * 1 changes in 90 seconds. Saving...
44918:M 08 Mar 23:18:17.965 * Background saving started by pid 55014
55014:C 08 Mar 23:18:18.103 * DB saved on disk
55014:C 08 Mar 23:18:18.106 * RDB: 0 MB of memory used by copy-on-write
44918:M 08 Mar 23:18:18.166 * Background saving terminated with success
44918:M 08 Mar 23:19:49.067 * 1 changes in 90 seconds. Saving...
44918:M 08 Mar 23:19:49.068 * Background saving started by pid 56216
56216:C 08 Mar 23:19:49.071 * DB saved on disk
56216:C 08 Mar 23:19:49.072 * RDB: 0 MB of memory used by copy-on-write
44918:M 08 Mar 23:19:49.169 * Background saving terminated with success
^C


-- 55 logs --

llbpal55:/var/log # tail -f redis_sentinel_26380.log
54523:X 08 Mar 12:26:11.106 # +new-epoch 231005
54523:X 08 Mar 12:26:11.106 # +vote-for-leader e1da411f5c3dd038e3d1b65dd7938263058411dd 231005
54523:X 08 Mar 12:26:11.120 # Next failover delay: I will not start a failover before Sun Mar  8 12:28:11 2020
54523:X 08 Mar 12:26:41.668 # +new-epoch 231006
54523:X 08 Mar 12:26:41.668 # +vote-for-leader e1da411f5c3dd038e3d1b65dd7938263058411dd 231006
54523:X 08 Mar 12:26:41.711 # Next failover delay: I will not start a failover before Sun Mar  8 12:28:41 2020
54523:X 08 Mar 12:27:09.667 * +reboot master master01 10.48.144.173 6380
54523:X 08 Mar 12:27:09.729 # -sdown master master01 10.48.144.173 6380
54523:X 08 Mar 12:27:09.729 # -odown master master01 10.48.144.173 6380
54523:X 08 Mar 12:34:30.864 # -sdown slave 10.48.144.175:6380 10.48.144.175 6380 @ master01 10.48.144.173 6380
^C
llbpal55:/var/log # tail -f redis_sentinel_6380.log
24371:S 08 Mar 23:18:17.900 * 1 changes in 90 seconds. Saving...
24371:S 08 Mar 23:18:17.909 * Background saving started by pid 31017
31017:C 08 Mar 23:18:17.916 * DB saved on disk
31017:C 08 Mar 23:18:17.917 * RDB: 0 MB of memory used by copy-on-write
24371:S 08 Mar 23:18:18.010 * Background saving terminated with success
24371:S 08 Mar 23:19:49.050 * 1 changes in 90 seconds. Saving...
24371:S 08 Mar 23:19:49.050 * Background saving started by pid 31770
31770:C 08 Mar 23:19:49.052 * DB saved on disk
31770:C 08 Mar 23:19:49.053 * RDB: 0 MB of memory used by copy-on-write
24371:S 08 Mar 23:19:49.150 * Background saving terminated with success
^C

----------------------------

QA : 55

1>  select convert(varchar(30),o.name) AS table_name
2> from sysobjects o
3> where type = 'U'
4> order by table_name
5> go
 table_name
 ------------------------------
 DATABASECHANGELOG
 DATABASECHANGELOGLOCK
 campaign
 campaign_subscription
 organization
 organization_permission
 organization_sms_account
 outbound_sms
 parameter
 permission
 role
 role_permission
 sms_account
 subscriber
 task
 user_profile
 user_profile_sms_account

(17 rows affected)



1> select * from user_profile
2> go
 id                   sso_id                                                                                                                                                                                                                                                          first_name                                                                                           last_name                                                                                            email                                                                                                user_scope                                                                                           user_status                                                                                                                                                                                                                                                     record_status                                                                                                                                                                                                                                                   role_code                                                                                            last_login_time                 timezone_id                                                                                          default_account_id   create_time                     update_time                     creator_id           updator_id           organization_id      role_id              sms_account_id
 -------------------- --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ---------------------------------------------------------------------------------------------------- ---------------------------------------------------------------------------------------------------- ---------------------------------------------------------------------------------------------------- ---------------------------------------------------------------------------------------------------- --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ---------------------------------------------------------------------------------------------------- ------------------------------- ---------------------------------------------------------------------------------------------------- -------------------- ------------------------------- ------------------------------- -------------------- -------------------- -------------------- -------------------- --------------------
                    1 C5285411                                                                                                                                                                                                                                                        Pankaj                                                                                               Wani                                                                                                 pankaj.wani@sap.com                                                                                  NULL                                                                                                 NORMAL                                                                                                                                                                                                                                                          ACTIVE                                                                                                                                                                                                                                                          NULL                                                                                                             Feb 14 2020  7:47AM NULL                                                                                                                    1             Feb 14 2020  7:47AM                            NULL                 NULL                 NULL                 NULL                 NULL                 NULL
                    2 C5285383                                                                                                                                                                                                                                                        Shikha                                                                                               Shah                                                                                                 shikha.shah@sap.com                                                                                  NULL                                                                                                 NORMAL                                                                                                                                      

					ACTIVE       	
					
					
					
					
- Had call with ChenYu.
- Had call wit Bhomik, Mohit.
- Had call with Ruksana. 

fr1mmapp003, fr1mmapp004



Chatterjee, Ujjal <ujjal.chatterjee@sap.com>; Kamalakshan, Sanjay (external - Service) <sanjay.kamalakshan@sap.com> 

- Threadump for investigation.

[skesarkar@us4ciapi04 ~]$ docker ps | grep -i mo-ack
7e384145fa24        dbs-docker-repository.docker.repositories.sapcdn.io/llk-sms-channel-mo-ack:2.0.1.101                "sh /opt/llk/service   19 hours ago        Up 19 hours         5701/tcp, 0.0.0.0:15710->15700/tcp, 0.0.0.0:15711->15701/tcp                                                                                        llk-sms-channel-mo-ack
[skesarkar@us4ciapi04 ~]$ docker exec -it 7e384145fa24 sh
/ $ ps aux | grep java
    5 llk      41:46 java -Xms32g -Xmx128g -DPARENTHOSTNAME=us4ciapi04 -Dspring.config.additional-location=/opt/llk/services/conf/smschannel.yml -jar /srv/SMSChannel.jar
59555 llk       0:00 grep java
/ $
/ $
/ $
/ $ kill -3 5
/ $ ps aux | grep java
    5 llk      41:52 java -Xms32g -Xmx128g -DPARENTHOSTNAME=us4ciapi04 -Dspring.config.additional-location=/opt/llk/services/conf/smschannel.yml -jar /srv/SMSChannel.jar
59739 llk       0:00 grep java
/ $
/ $ ps aux | grep java
    5 llk      41:53 java -Xms32g -Xmx128g -DPARENTHOSTNAME=us4ciapi04 -Dspring.config.additional-location=/opt/llk/services/conf/smschannel.yml -jar /srv/SMSChannel.jar
59761 llk       0:00 grep java
/ $ exit
[skesarkar@us4ciapi04 ~]$ docker restart 7e384145fa24
7e384145fa24
[skesarkar@us4ciapi04 ~]$ docker ps | grep -i mo-ack
7e384145fa24        dbs-docker-repository.docker.repositories.sapcdn.io/llk-sms-channel-mo-ack:2.0.1.101                "sh /opt/llk/service   19 hours ago        Up 11 seconds       5701/tcp, 0.0.0.0:15710->15700/tcp, 0.0.0.0:15711->15701/tcp                                                                                        llk-sms-channel-mo-ack
[skesarkar@us4ciapi04 ~]$


-----------

- 45 minutes of call with Khalid, ChenYu for SSL, FR1, DB size.

- AAT Details.

	AAT reverse proxy : 14500
	AAT api : 14550
	AAT ui : 14540
	Redis : 26379
	AAT management : 14551

- open a firewall request, MMG, RMS, LLK and AAT.

AWS6 - North Virginia
AWS1 - 

MMG
Rabbit : 5673
Redis : 26382, 6382
DB : 5000

- Allow from APP to DB


LLK
Rabbit : 5672
Redis : 26385, 6385
DB : 5000


AAT
Redis : 26379
DB : 5000

RMS
Rabbit : 5671
Redis : 26381
DB : 5000


Redis : for RMS, MMG they are 26381
Rabbit : for LLK, RMS, MMG they are 5672



--------------------- 10 March 2020 --------------------------

[pablo@ip-198-19-11-212 configmap]$ cat configmap_mmg-ui-cm.yaml | grep -i int.cloud
  UPSTREAM_SGE: https://mmg-be-service.mmg:16500
  UPSTREAM_ACCOUNTS_MANAGEMENT_V2: https://llk-accounts-service.llk:15320
  UPSTREAM_CALLBACK_MANAGER: https://llk-callback-manager.llk:15000
  UPSTREAM_ENGAGEMENT: https://llk-engagement.lllk:16100
  UPSTREAM_ADMIN: https://llk-admin-service.llk:15920
  UPSTREAM_DOCUMENTATION: https://llk-docs.llk.int.cloud.sap:15930
  UPSTREAM_MESSAGES_MANAGEMENT_V2: https://llk-analytics.llk.int.cloud.sap:17200
  UPSTREAM_DOCUMENTATION2: https://llk-integrate-portal.llk.int.cloud.sap:15940
  UPSTREAM_SMS_CHANNEL: https://llk-sms-channel.llk.int.cloud.sap:15700
  UPSTREAM_SMS_EVENTSTORE: https://llk-sms-eventstore.llk.int.cloud.sap:15720
  AAT_TOKEN_GENERATE_URL: https://aat-service.aat.int.cloud.sap:14500/oauth/token
  AAT_TOKEN_VALIDATE_URL: https://aat-service.aat.int.cloud.sap:14500/oauth/token/validate
  AAT_TOKEN_INVALIDATE_URL: https://aat-service.aat.int.cloud.sap:14500/oauth/token/invalidate
  AAT_METADATA_URL: https://aat-service.aat.int.cloud.sap:14500/auth/metadata
  SYSTEM_URL: https://mmg-ui.mmg.int.cloud.sap:7443

- open a firewall request, MMG, RMS, LLK and AAT for AWS1.

- AAT Details.

	AAT reverse proxy : 14500
	AAT api : 14550, 14551
	AAT ui : 14540
	Redis : 26379
	AAT management : 14551

- MMG Details.

	Rabbit : 5673
	Redis : 26382, 6382
	DB : 5000
	SGE Port Core : 16500
	Scheduler port : 16400
	MMG UI : 16510 -> 15910

- LLK Details.

	Rabbit : 5672
	Redis : 26385, 6385
	DB : 5000
	Hazelcast : 5702
	
	JAVA​
	Accounts:15320-1​
	Spring Admin:15950-1​
	Analytics:17200-1​
	API Gateway:15200-1​
	Archival:15650​
	Callback Manager:15000-1​
	Docs:15930​
	Event Store:15720-1​
	Inbound:15500-1​
	Integrate Portal:15940​
	SMS Channel:15700-1​
	SMS Channel MO-ACK:15710-1​
	
	NGINX​
	Reverse Proxy:9999:30015​
	UI Admin Portal:15920​
	UI Gateway:15100-1​
	UI Portal:15910​


- RMS Details.

	Rabbit : 5671
	Redis : 26381
	DB : 5000
	Callback Mgr : 16600
	Core : 16100
	SLM : 16200
	Keywords : 16300



- Allow from DMZ to APP ports.

	
- Allow from APP to APP ports.
- Allow from APP to DB.
- Allow from APP to Redis.
- Allow fro APP  to Rabbit.

- Done with raising ticket with Debalina AMK-159 - 161.
- Done with rahul allowing access to AWS CRM.

- Shikha Shah on chat.

- Call with Amol 1 hour.


  SCHEDULER_APP_KEY: HFUiFHaFfJFc4dR2
  SCHEDULER_APP_SECRET: zCHwLmobn0xCreQCyjsSjrsxcASHlxlB
  ACK_ENABLED: "false"
  ACK_TIME_ENABLED: "false"
  CALLBACK_URL: https://mmg-be-service.mmg.int.cloud.sap:16500/api/outbound_sms/acknowledgement/{{message-id}}
  AAT_PUBLIC_KEY: "MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAzfJWm7sdzKf3cG7fKK5/B8rl5KmbsimqYy1mhSqtWQ7ux6nfeS3mK7lrovP7HQhld5u7DabJOKHffqasPprvhuCIuebCnqi+1otRsaK5a9OTBmToO6mZTKXOWKyNscrDftsM0qR8B/aqo8AJl794v6pG6vt/I8pM9dnTPgnoK2che4YXA6rq8Q/JFihMzMMM6hbTJuoGFqgwMKu1xEVWf6QBUuoXdyWrDYj90c+eXHebH9sDm8yPI/sdv2L9zvNSJsDZ8t82aEzVbo7x7fFO606F6leGgq8qqrpQJGqz5YxlQJRDi4+6ckX8EaHVaifsQGCWPthVnpe0lfU9Rk1bMQIDAQAB"

MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAzfJWm7sdzKf3cG7fKK5/B8rl5KmbsimqYy1mhSqtWQ7ux6nfeS3mK7lrovP7HQhld5u7DabJOKHffqasPprvhuCIuebCnqi+1otRsaK5a9OTBmToO6mZTKXOWKyNscrDftsM0qR8B/aqo8AJl794v6pG6vt/I8pM9dnTPgnoK2che4YXA6rq8Q/JFihMzMMM6hbTJuoGFqgwMKu1xEVWf6QBUuoXdyWrDYj90c+eXHebH9sDm8yPI/sdv2L9zvNSJsDZ8t82aEzVbo7x7fFO606F6leGgq8qqrpQJGqz5YxlQJRDi4+6ckX8EaHVaifsQGCWPthVnpe0lfU9Rk1bMQIDAQAB

"MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAzfJWm7sdzKf3cG7fKK5/B8rl5KmbsimqYy1mhSqtWQ7ux6nfeS3mK7lrovP7HQhld5u7DabJOKHffqasPprvhuCIuebCnqi+1otRsaK5a9OTBmToO6mZTKXOWKyNscrDftsM0qR8B/aqo8AJl794v6pG6vt/I8pM9dnTPgnoK2che4YXA6rq8Q/JFihMzMMM6hbTJuoGFqgwMKu1xEVWf6QBUuoXdyWrDYj90c+eXHebH9sDm8yPI/sdv2L9zvNSJsDZ8t82aEzVbo7x7fFO606F6leGgq8qqrpQJGqz5YxlQJRDi4+6ckX8EaHVaifsQGCWPthVnpe0lfU9Rk1bMQIDAQAB'


MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAzfJWm7sdzKf3cG7fKK5/B8rl5KmbsimqYy1mhSqtWQ7ux6nfeS3mK7lrovP7HQhld5u7DabJOKHffqasPprvhuCIuebCnqi+1otRsaK5a9OTBmToO6mZTKXOWKyNscrDftsM0qR8B/aqo8AJl794v6pG6vt/I8pM9dnTPgnoK2che4YXA6rq8Q/JFihMzMMM6hbTJuoGFqgwMKu1xEVWf6QBUuoXdyWrDYj90c+eXHebH9sDm8yPI/sdv2L9zvNSJsDZ8t82aEzVbo7x7fFO606F6leGgq8qqrpQJGqz5YxlQJRDi4+6ckX8EaHVaifsQGCWPthVnpe0lfU9Rk1bMQIDAQAB


------------------- Editing the ui configmap ---------------


  UPSTREAM_SGE: http://mmg-be-service.mmg:16500
  UPSTREAM_MAX_FAILS: "3"
  UPSTREAM_FAIL_TIMEOUT: "10"
  UPSTREAM_ACCOUNTS_MANAGEMENT_V2: http://llk-accounts-service.llk:15320
  UPSTREAM_CALLBACK_MANAGER: http://llk-callback-manager.llk:15000
  UPSTREAM_ENGAGEMENT: http://llk-engagement.llk:16100
  UPSTREAM_IN365_SOCIAL_API: sms-pp.sapmobileservices.com:443
  UPSTREAM_STREAM: llk-ui-gateway.llk:15100
  UPSTREAM_ACCOUNTS_MANAGEMENT: us4cidemo002.bd.trust:15300
  UPSTREAM_SMSLOGGER_MANAGEMENT: us4cidemo002.bd.trust:15900
  UPSTREAM_MO_INBOUND: us4cidemo002.bd.trust:15500
  UPSTREAM_API_GW: llk-apigateway.llk:15200
  UPSTREAM_ADMIN: http://llk-admin-service.llk:15920
  UPSTREAM_DOCUMENTATION: http://llk-docs.llk:15930
  UPSTREAM_CLIENT_CODES: us4cipapi01:15300
  UPSTREAM_USER_MANAGEMENT: llk-ui-gateway.llk:15100
  UPSTREAM_MESSAGES_API: llk-ui-gateway.llk:15100
  UPSTREAM_MESSAGES_MANAGEMENT_V2: http://llk-analytics.llk:17200
  UPSTREAM_DOCUMENTATION2: http://llk-integrate-portal.llk:15940
  UPSTREAM_SMS_CHANNEL: http://llk-sms-channel.llk:15700
  UPSTREAM_MESSAGES_MANAGEMENT: us4cidemo002.bd.trust:15800
  UPSTREAM_SMS_EVENTSTORE: http://llk-sms-eventstore.llk:15720
  AAT_CLIENT_ID: perf:2gengllk:ui
  AAT_CLIENT_SECRET: PW1OCNLWB7LB3EHOFDF664KAADWPTCDI
  AAT_TOKEN_GENERATE_URL: http://aat-service.aat:14500/oauth/token
  AAT_TOKEN_VALIDATE_URL: http://aat-service.aat:14500/oauth/token/validate
  AAT_TOKEN_INVALIDATE_URL: http://aat-service.aat:14500/oauth/token/invalidate
  AAT_METADATA_FILE: /opt/llk/services/conf/${ENV}_aat_metadata.json
  AAT_METADATA_URL: http://aat-service.aat:14500/auth/metadata
  AAT_USER_ROLES: LIVELINK_USER
  AAT_SCOPE: livelink:api
  AAT_APP_ROLE: LIVELINK_APP
  AAT_USER_ROLE: LIVELINK_USER
  AAT_COOKIE_NAME: LIVELINK
  AAT_LOGIN_ACTION: login
  AAT_LOGOUT_ACTION: logout
  AAT_ACCPETED_ROLES: LIVELINK_TRIAL
  SYSTEM_URL: http://mmg-ui.mmg:7443
  NGINX_SERVER_NAME: http://localhost
  IS_SSL: "true"
  CERTIFICATE: llbpal-wildcard
  JWT_CERTIFICATE: "-----BEGIN CERTIFICATE-----\\nMIIDHjCCAgYCCQC09G8CbZ6yMDANBgkqhkiG9w0BAQUFADBRMQswCQYDVQQGEwJV\\nUzEPMA0GA1UECAwGUmVzdG9uMREwDwYDVQQHDAhQb3J0bGFuZDEMMAoGA1UECgwD\\nU0FQMRAwDgYDVQQDDAdzYXAuY29tMB4XDTE3MDMyMzAzMTU1NVoXDTIwMDMyMjAz\\nMTU1NVowUTELMAkGA1UEBhMCVVMxDzANBgNVBAgMBlJlc3RvbjERMA8GA1UEBwwI\\nUG9ydGxhbmQxDDAKBgNVBAoMA1NBUDEQMA4GA1UEAwwHc2FwLmNvbTCCASIwDQYJ\\nKoZIhvcNAQEBBQADggEPADCCAQoCggEBAM5UO8v2ofvVjak7NgY/gfcZSyYNi9Np\\n4z6bcopEbvbHHhaE+K0L1S2AOoRsuma2SLuBhH9H7KdeFv1ELcWgIthmhMr7CJp4\\nFW/iZv2MC8BKWSXUYGO8O89NQWdFcNpppltaavUNOixrKtIuG7jrhmDnJFsQZUZ6\\nkGSb+GroNen/bIySaa5f5ZFS4n9TQKd64x/Kik0B34FQpzb+qheLxl2XH4VwhiBy\\n9FRcmgLpeh3CULQD2XOAgJWrSO2MNSaD0FHC9BFp37Paf6rycZO5xySz66TBoM0+\\nP0qN9UBZWcdhV6S0ya+XnxVxsFEk2lCVbfvqyX13CSuttvbqCMbbNc8CAwEAATAN\\nBgkqhkiG9w0BAQUFAAOCAQEAmC3ssfqVdMc8omyxzY4cJjTgT2gejPUEZRrjz2ge\\nLFR5FG9BJdhVwsn9fmdoH1G3dAA4nynRgbMIVt2FTKnnCQflo/E950UrFryzYY0t\\nj5hsUV6YXGTY4Tl9m16YtaLeaR9B6OaT44oigmJ9CSKtCHCkAvnQ717b/y/ZY221\\nVs69EMy1v6iL2z7C0TcmTAGi+XtIh8TLFcihS9BEkhj7So63CJtVEiJE9HWxA5CP\\n9HuGb7f2w24XqS5BDuWY9O5on8e8IVbTH7sWRA3NpwlhTmfWX1SDSBuAacAvwvC9\\nvNxVKxWwEUuVkA8ENGo5NFIXXd+VVQggEr6Y0QKKHznb0A==\\n-----END CERTIFICATE-----"
  DT_TAGS: service=llk-portal-ui-2geng environment=performance project=llk
  CONSUL_HOST: 172.24.227.46
  HOSTNAME: hostname
  SHLVL: "1"
  HOME: /root
  CONSUL_PORT: "8500"
  USE_CONSUL: "false"
  TERM: xterm
  PATH: /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/openresty/luajit/bin:/usr/local/openresty/nginx/sbin:/usr/local/openresty/bin
  CONSUL_PREFIX: configs/performance/llk/ui
  CONSUL_SOURCE: /configs/performance/llk/ui/llk-portal-ui-2geng/env_vars.json
  PWD: /



---------------------- 12-03-2020 -------------------

UPSTREAM_ACCOUNTS_MANAGEMENT_V2: http://llk-accounts-service.llk:15320
	livelink-accounts-management-v2
	https://livelink.sapmobileservices.com/
	 
UPSTREAM_CALLBACK_MANAGER: http://llk-callback-manager.llk:15000

UPSTREAM_ENGAGEMENT: http://llk-engagement.llk:16100

UPSTREAM_IN365_SOCIAL_API: sms-pp.sapmobileservices.com:443

UPSTREAM_STREAM: llk-ui-gateway.llk:15100
	livelink-stream
	livelink-user-management
	livelink-messages-api

UPSTREAM_ACCOUNTS_MANAGEMENT: us4cidemo002.bd.trust:15300
	


UPSTREAM_SMSLOGGER_MANAGEMENT: us4cidemo002.bd.trust:15900

UPSTREAM_MO_INBOUND: us4cidemo002.bd.trust:15500

UPSTREAM_API_GW: llk-apigateway.llk:15200
	livelink-api-gateway
	

UPSTREAM_ADMIN: http://llk-admin-service.llk:15920

UPSTREAM_DOCUMENTATION: http://llk-docs.llk:15930
	documentation

UPSTREAM_CLIENT_CODES: us4cipapi01:15300

UPSTREAM_USER_MANAGEMENT: llk-ui-gateway.llk:15100
	livelink-stream
	livelink-user-management
	livelink-messages-api

UPSTREAM_MESSAGES_API: llk-ui-gateway.llk:15100

UPSTREAM_MESSAGES_MANAGEMENT_V2: http://llk-analytics.llk:17200
	livelink-messages-v2

UPSTREAM_DOCUMENTATION2: http://llk-integrate-portal.llk:15940
	
	
UPSTREAM_SMS_CHANNEL: http://llk-sms-channel.llk:15700
	livelink-sms-channel

UPSTREAM_MESSAGES_MANAGEMENT: us4cidemo002.bd.trust:15800

UPSTREAM_SMS_EVENTSTORE: http://llk-sms-eventstore.llk:15720
	livelink-sms-eventstore
	
--------- Launch Redis for MMG on AWS6

MMG Master : 10.6.8.151  redis-master-mmg-aws6
MMG Sentinel : 10.6.8.151, 10.6.7.181, 10.6.7.193 redis-sentinel-mmg-aws6
MMG Slave :	10.6.7.181, 10.6.7.193 redis-slave-mmg-aws6
Port : 6382, 26382
https://jira.di-infra.sap.corp/browse/ISS-1874
Password : icbmPUs


[redis-master-mmg-aws6]
10.6.8.151

[redis-sentinel-mmg-aws6]
10.6.8.151
10.6.7.181
10.6.7.193

[redis-slave-mmg-aws6]
10.6.7.181
10.6.7.193


TASK [ansible-redis : add sentinel init config file] ******************************************************************************************************************
skipping: [10.6.8.151]
skipping: [10.6.7.181]
skipping: [10.6.7.193]

RUNNING HANDLER [ansible-redis : restart sentinel {{ redis_sentinel_port }}] ******************************************************************************************
changed: [10.6.8.151]
changed: [10.6.7.181]
changed: [10.6.7.193]

TASK [ansible-redis : ensure sentinel is running] *********************************************************************************************************************
ok: [10.6.7.181]
ok: [10.6.8.151]
ok: [10.6.7.193]

TASK [ansible-redis : create facts directory] *************************************************************************************************************************
ok: [10.6.7.181]
ok: [10.6.8.151]
ok: [10.6.7.193]

TASK [ansible-redis : create redis facts] *****************************************************************************************************************************
changed: [10.6.7.181]
changed: [10.6.8.151]
changed: [10.6.7.193]

PLAY RECAP ************************************************************************************************************************************************************
10.6.7.181                 : ok=63   changed=29   unreachable=0    failed=0
10.6.7.193                 : ok=63   changed=29   unreachable=0    failed=0
10.6.8.151                 : ok=60   changed=18   unreachable=0    failed=0

[ec2-user@ip-10-6-11-175 ansible]$ pwd
/home/ec2-user/sachin/Full devops scripts_github/ansible
[ec2-user@ip-10-6-11-175 ansible]$ ansible-playbook-2.7 redisClusterMMAWS.yml --private-key=/home/ec2-user/sachin/pemkeys/aws4psilva.pem -u ec2-user --extra-vars "my_env=development redis_port=6382"^C

---- Install docker container on all the 3 hosts.

[ec2-user@ip-10-6-11-175 ansible]$ ansible-playbook-2.7 dockerInstall.yml  --private-key=/home/ec2-user/sachin/pemkeys/aws4psilva.pem -u ec2-user --limit redis-sentinel-mmg-aws6

TASK [dockerInstall : Restart docker-engine-1.13] *********************************************************************************************************************
skipping: [10.6.8.151]
skipping: [10.6.7.181]
skipping: [10.6.7.193]

TASK [dockerInstall : Configuring docker private registry credentials] ************************************************************************************************
changed: [10.6.7.181]
changed: [10.6.8.151]
changed: [10.6.7.193]

PLAY RECAP ************************************************************************************************************************************************************
10.6.7.181                 : ok=7    changed=6    unreachable=0    failed=0
10.6.7.193                 : ok=7    changed=6    unreachable=0    failed=0
10.6.8.151                 : ok=7    changed=6    unreachable=0    failed=0





---- Now install Rabbit on the AWS6 nodes.

10.6.8.151, 10.6.7.181, 10.6.7.193

RABBITMQ_HOST=10.6.8.151
RABBITMQ_PORT=5673
RABBITMQ_USER=admin
RABBITMQ_PWD=abcd1234
RABBITMQ_ADDRESSES=10.6.8.151:5673,10.6.7.181:5673

docker run -d --restart=always --hostname localhost --name docker-rabbit-10.6.8.151 -p 4369:4369 -p 5672:5672 -p 15672:15672 -p 25672:25672 -p 35197:35197 -e RABBITMQ_DEFAULT_USER=admin -e RABBITMQ_DEFAULT_PASS=abcd1234 -e RABBITMQ_ERLANG_COOKIE='NSORXJLSGOOCLYMVCDQF' rabbitmq:3-management

docker run -d --restart=always --hostname localhost --name docker-rabbit-10.6.7.181 -p 4369:4369 -p 5672:5672 -p 15672:15672 -p 25672:25672 -p 35197:35197 -e RABBITMQ_DEFAULT_USER=admin -e RABBITMQ_DEFAULT_PASS=abcd1234 -e RABBITMQ_ERLANG_COOKIE='NSORXJLSGOOCLYMVCDQF' rabbitmq:3-management

On 10.6.7.181 join cluster rabbit@10.6.8.151

docker exec docker-rabbit-10.6.7.181 rabbitmqctl stop_app
docker exec docker-rabbit-10.6.7.181 rabbitmqctl join_cluster rabbit@10.6.8.151
docker exec docker-rabbit-10.6.7.181 rabbitmqctl start_app
docker exec docker-rabbit-10.6.7.181 rabbitmqctl set_policy ha-nodes "^nodes\." \
'{"ha-mode":"nodes","ha-params":["rabbit@10.6.8.151", "rabbit@10.6.7.181"]}'


------- getting some error for that.

[root@ip-10-6-8-151 redis]# docker run -d --restart=always --hostname localhost --name docker-rabbit-10.6.8.151 -p 4369:4369 -p 5672:5672 -p 15672:15672 -p 25672:25672 -p 35197:35197 -e RABBITMQ_DEFAULT_USER=admin -e RABBITMQ_DEFAULT_PASS=abcd1234 -e RABBITMQ_ERLANG_COOKIE='NSORXJLSGOOCLYMVCDQF' rabbitmq:3-management
65cfff56e8701c5bb50285e92422fab3d702b79b725ca3374437c6d365feed51
[root@ip-10-6-8-151 redis]#
[root@ip-10-6-8-151 redis]#
[root@ip-10-6-8-151 redis]# docker ps
CONTAINER ID        IMAGE                   COMMAND                CREATED             STATUS              PORTS                                                                                                                                               NAMES
65cfff56e870        rabbitmq:3-management   "docker-entrypoint.s   24 seconds ago      Up 23 seconds       0.0.0.0:4369->4369/tcp, 5671/tcp, 0.0.0.0:5672->5672/tcp, 0.0.0.0:15672->15672/tcp, 0.0.0.0:25672->25672/tcp, 0.0.0.0:35197->35197/tcp, 15671/tcp   docker-rabbit-10.6.8.151

----

[root@ip-10-6-7-181 redis]# docker run -d --restart=always --hostname localhost --name docker-rabbit-10.6.7.181 -p 4369:4369 -p 5672:5672 -p 15672:15672 -p 25672:25672 -p 35197:35197 -e RABBITMQ_DEFAULT_USER=admin -e RABBITMQ_DEFAULT_PASS=abcd1234 -e RABBITMQ_ERLANG_COOKIE='NSORXJLSGOOCLYMVCDQF' rabbitmq:3-management
52ac972f2a2c8cae13166dce07e4948a0091f8be9502c25a53768438b80b9745
[root@ip-10-6-7-181 redis]#
[root@ip-10-6-7-181 redis]# docker exec docker-rabbit-10.6.7.181 rabbitmqctl stop_app
Stopping rabbit application on node rabbit@localhost ...
[root@ip-10-6-7-181 redis]# docker exec docker-rabbit-10.6.7.181 rabbitmqctl join_cluster rabbit@10.6.8.151
Clustering node rabbit@localhost with rabbit@10.6.8.151
Error:
{:badrpc_multi, {:EXIT, {{:function_clause, [{:gen, :do_for_proc, [{:rex, {:error, {:node_name, :short}}}, #Function<0.9801092/1 in :gen.call/4>], [file: 'gen.erl', line: 220]}, {:gen_server, :call, 3, [file: 'gen_server.erl', line: 219]}, {:rpc, :do_call, 3, [file: 'rpc.erl', line: 327]}, {:lists, :foldl, 3, [file: 'lists.erl', line: 1263]}, {:rabbit_mnesia, :discover_cluster, 1, [file: 'src/rabbit_mnesia.erl', line: 779]}, {:rabbit_mnesia, :join_cluster, 2, [file: 'src/rabbit_mnesia.erl', line: 212]}, {:rpc, :"-handle_call_call/6-fun-0-", 5, [file: 'rpc.erl', line: 197]}]}, {:gen_server, :call, [{:rex, {:error, {:node_name, :short}}}, {:call, :rabbit_mnesia, :cluster_status_from_mnesia, [], #PID<0.62.0>}, :infinity]}}}, [error: {:node_name, :short}]}
[root@ip-10-6-7-181 redis]#


-----------------------------------


Call with PAblo.

UI component : 
	- AAT cert variable value replacement.
	- AAT client creation.
	- 
	
	
	
------------------ 13 March 2020 -----------------
- Had a 1.5 hours call with PAblo in morning 6:00.

- Checking the Dynatrace issue, 
	https://apm.cf.us10.hana.ondemand.com/e/f95fd477-ba09-47d0-ba91-d6f35a4e2f22/#problems/problemdetails;pid=8761438077415185497_1584073020000V2
	
- Worked with Rajul, chat.

- PAR request 100172356, 100172357

- Call with Dipti, Nidhi regarding Dyntace monitoring. refer to Email.

 k logs --tail 500 alb-ingress-controller-66945cdfcf-2c84k -n kube-system
 k logs -f mmg-be-deployment-6866c49875-zh7js
 
 
"Headers: host: 198.19.8.177:30955\x0Aconnection: close\x0Aaccept-encoding: gzip, compressed\x0Auser-agent: ELB-HealthChecker/2.0\x0A"
"-"
198.19.8.126 - - [13/Mar/2020:10:31:42 +0000] "GET /users/C5246840?projection=extended-user HTTP/1.1" 499 0 "https://aa8fd7ab-mmg-mmgingress-ac84-497892438.us-east-1.elb.amazonaws.com/app/" "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:72.0) Gecko/20100101 Firefox/72.0" 60.001

"Headers: host: aa8fd7ab-mmg-mmgingress-ac84-497892438.us-east-1.elb.amazonaws.com\x0Ax-amzn-trace-id: Root=1-5e6b60d2-c421326897db73b6918dd984\x0Ax-forwarded-port: 443\x0Acookie: LIVELINK=eyJhbGciOiJSUzI1NiJ9.eyJjbGFpbWFudCI6eyJpZCI6MjUwMDAwMjMsInNzb0lkIjoiQzUyNDY4NDAiLCJmaXJzdE5hbWUiOiJTYWNoaW4iLCJsYXN0TmFtZSI6Iktlc2Fya2FyIiwiZW1haWwiOiJzYWNoaW4ua2VzYXJrYXJAc2FwLmNvbSIsInNjb3BlIjoiYXdzZGV2Om1tZzp1aSIsInJvbGVDb2RlcyI6WyJMSVZFTElOS19BQ0NPVU5UX0FETUlOIiwiMkZBX1RSSUFMX1JPTEUiLCJBQV9BRE1JTiIsIlNHRV9BQ0NFU1MiLCJQQ19BRE1JTiIsIkxJVkVMSU5LX0FQUCIsIlNBUF9BRE1JTiIsIkxJVkVMSU5LX0VOR0FHRU1FTlQiLCJQQ19PUEVSQVRPUiIsIjJGQV9BRE1JTl9ST0xFIiwiRUVfQURNSU4iLCJMSVZFTElOS19BRE1JTiIsIjJGQV9DTElFTlRfUk9MRSIsIlBDX1ZJRVdFUiIsIkxJVkVMSU5LX1NVUEVSX0FETUlOIiwiTElWRUxJTktfVVNFUiIsIkxMS19FTkdBR0VNRU5UIl19LCJpc3MiOiJTQ0lfQVVUSEVOVElDQVRJT05fQVVUSE9SSVRZIiwianRpIjoielE2dTFBakZISTQyNU5kM29NS1RMMkg2MWZSRExJalciLCJleHAiOjE1ODQwOTgxMzl9.urgJK1Ft-KuDOZm4xanK1r0ZDJAoT5NKEd7fkYNvg2NZJs2lFbEiOG8y_tq9uetjbUTynJrEbfiQozsBDq-k36tcUV1U_fEQpU1pSNMn9oAgIco2Lc1oKfT8aFKgW3zlwe4hD5tUSguF4CU0Z6FOR2pO4Hxc1D2SChkzqFzdp4hCHrjkrLw53AOvg9D_k9HVAtZnNG6WnFl0wZL1yCsDFntJph-3WcMLbznJ_NYOkUWcUQucIeIxuvzrpqIbv87XTP1lWGVmLI03tvPd5vUQRI_z8HCOmdoPiMoTcMOObFZiJO3WfOT3_CBVGLrzqKA_6u6SCV9cBrFAwn4WIRjU5Q; LiveLinkTrialDisabled=1\x0Ax-forwarded-proto: https\x0Areferer: https://aa8fd7ab-mmg-mmgingress-ac84-497892438.us-east-1.elb.amazonaws.com/app/\x0Ax-forwarded-for: 205.218.33.30\x0Auser-agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:72.0) Gecko/20100101 Firefox/72.0\x0Ax-requested-with: XMLHttpRequest\x0Aaccept-encoding: gzip, deflate, br\x0Aaccept-language: en-US,en;q=0.5\x0Aaccept: application/json, text/javascript, */*; q=0.01\x0A"


------------------- config map ui backup working -------------------------
------- call with Abhilekh 1.5 hrs

upstream livelink-stream{
server demo-livelink.sapdigitalinterconnect.com:443 max_fails=3 fail_timeout=10s;
}
upstream livelink-user-management{
server demo-livelink.sapdigitalinterconnect.com:443 max_fails=3 fail_timeout=10s;
}
upstream livelink-accounts-management-v2{
server demo-livelink.sapdigitalinterconnect.com:443 max_fails=3 fail_timeout=10s;
}


upstream livelink-api-gateway{
server demo-livelink.sapdigitalinterconnect.com:443 max_fails=3 fail_timeout=10s;
}




--------------------------- 16 March 2020 ---------------------------------------------

- Spoke with Rajul, Shikha, Pankaj, ChenYu, Nidhi(regarding ChenYu project). Check Slack.

- Had 2 hours call with Barani regarding KT on Jenkins.

- Setup the initial GIT configuration, pushed the mmg to git.tools for almost 30 mins.

- Worked with Sikha for troubleshooting the BE service.

UPSTEAM_SGE : backend_endpoint:16500 --> in UI , configmap_mmg-ui-cm.yaml

UPSTEAM_SGE : https://mmg-be-service.mmg:16500


backend, Hostname : backend_enpoint , no ports

scheduler, Hostname : backend_endpoint , no ports


/usr/local/openresty/nginx/conf/llk_nginx.conf
/usr/local/openresty/nginx/conf/locations.conf
/usr/local/openresty/nginx/conf/upstreams.conf



[pablo@ip-198-19-11-212 configmap]$ kubectl get pod -o=custom-columns=NODE:.spec.nodeName,NAME:.metadata.name -n mmg
NODE                           NAME
ip-198-19-8-38.ec2.internal    mmg-be-deployment-7fc55595c4-8wwn8
ip-198-19-8-38.ec2.internal    mmg-sch-deployment-79d8bb5b7f-7s77t
ip-198-19-7-251.ec2.internal   mmg-ui-deployment-674fd4cd9d-9zg5z

-------------------------------------------------------------------------

[pablo@ip-198-19-11-212 service]$ cat service_mmg-be-svc.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app: mmg
    component: mmg-be-service
    managed-by: DevOps
    environment: awsdev
    version: "1"
  namespace: mmg
  name: mmg-be-service
spec:
  ports:
  - name: service-port
    port: 16500
    protocol: TCP
    targetPort: 16500
  selector:
    component: mmg-be-service
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}

----------------------------------------------
curl --location --request POST 'http://mmg-be-service.mmg:16500/api/outbound_sms/acknowledgement/3d199105-4b10-4ac8-949e-cfe312de6ef' \
--header 'Content-Type: application/json' \
--data-raw '{
    "message": "Test Message",
    "status": "sent"
}'

curl --location --request POST 'http://mmg-be-service.mmg/api/outbound_sms/acknowledgement/3d199105-4b10-4ac8-949e-cfe312de6ef' \
--header 'Content-Type: application/json' \
--data-raw '{
    "message": "Test Message",
    "status": "sent"
}'
----------------------------------------------

apiVersion: v1
kind: Service
metadata:
  creationTimestamp: "2020-03-16T12:09:25Z"
  labels:
    app: mmg-be
    run: mmg-be
  name: mmg-be-deployment
  namespace: mmg
  resourceVersion: "59726255"
  selfLink: /api/v1/namespaces/mmg/services/mmg-be-deployment
  uid: fa64f5ee-677e-11ea-be6f-0a76f667ede1
spec:
  clusterIP: 10.100.24.126
  ports:
  - port: 16500
    protocol: TCP
    targetPort: 16500
  selector:
    environment: awsdev
    run: mmg-be
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}



------------------- 17 March 2020 ---------------------
- Had a discussion with Nidhi regarding the hotfix and loadbalancer issue.
- Approved Shikha request.
- Worked on Nitasha login issue to JIRA.
- 

- 

Hi SysEng,

We need ReadOnly access to following person on our BD TRUST domain. Please add him/her to JIRA group on LDAP.

Soni, Anuj (external - Project) <anuj.soni01@sap.com> : PAR Request 100172410
Shivankar, Balaji (external - Project) <balaji.shivankar@sap.com> : PAR Request 100172406
Deokar, Manisha (external - Project) <manisha.deokar@sap.com> : PAR Request 100172407
Gade, Prachi (external - Project) <prachi.gade@sap.com> : PAR Request 100172408
Zamindar, Ankur (external - Project) <ankur.zamindar@sap.com> : PAR Request 100172409

1.5 hours
----------

- Tower jobs failing, restarted the tower.

https://bamboo.di-infra.sap.corp/deploy/viewDeploymentResult.action?deploymentResultId=140247193
https://bamboo.di-infra.sap.corp/deploy/viewDeploymentResult.action?deploymentResultId=140247189

------------

- Checked on Novel request to restart the DB.

I would like to request your help with our QAS environment. We are facing issues since the weekend. 
Error 502 can be noticed in some endpoint calls. Handres has done some investigation but no success yet, apparently there is a connectivity leak with the database.

------------

But troubleshooting the Kubernetes ingress issue.

[pablo@ip-198-19-11-212 service]$ kubectl get ingress -n llk
NAME   HOSTS   ADDRESS                                                        PORTS   AGE
llk    *       aa8fd7ab-llk-llk-3184-1022293305.us-east-1.elb.amazonaws.com   80      28d
[pablo@ip-198-19-11-212 service]$ kubectl describe ing llk -n llk
Error from server (NotFound): the server could not find the requested resource
[pablo@ip-198-19-11-212 service]$
[pablo@ip-198-19-11-212 service]$ kubectl describe ing aa8fd7ab-mmg-mmgingress-ac84-497892438.us-east-1.elb.amazonaws.com -n mmg
Error from server (NotFound): ingresses.extensions "aa8fd7ab-mmg-mmgingress-ac84-497892438.us-east-1.elb.amazonaws.com" not found
[pablo@ip-198-19-11-212 service]$
[pablo@ip-198-19-11-212 service]$
[pablo@ip-198-19-11-212 service]$ kubectl get ing -n mmg
NAME          HOSTS   ADDRESS                                                              PORTS   AGE
mmg-ingress   *       aa8fd7ab-mmg-mmgingress-ac84-497892438.us-east-1.elb.amazonaws.com   80      4d20h
[pablo@ip-198-19-11-212 service]$
[pablo@ip-198-19-11-212 service]$ kubectl describe ing mmg-ingress -n mmg
Error from server (NotFound): the server could not find the requested resource
[pablo@ip-198-19-11-212 service]$

-----------------

livelink - production
database production ip, password
app code


/usr/local/openresty/nginx/logs/error.log

-----------------

198.19.7.251 - - [17/Mar/2020:15:21:03 +0000] "GET /smsConfigs?clientId=1&appKey=st8pMyVSFgADCRx5 HTTP/1.1" 504 1499 "https://aa8fd7ab-mmg-mmgingress-ac84-497892438.us-east-1.elb.amazonaws.com/app/" "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:72.0) Gecko/20100101 Firefox/72.0" 60.000

"Headers: host: aa8fd7ab-mmg-mmgingress-ac84-497892438.us-east-1.elb.amazonaws.com\x0Ax-amzn-trace-id: Root=1-5e70eaa3-4cd557114664d40cddddd3d8\x0Ax-forwarded-port: 443\x0Acookie: BIGipServerPOOL-demo-livelink.sapmobileservices.com-9999=704835594.3879.0000; LIVELINK=eyJhbGciOiJSUzI1NiJ9.eyJjbGFpbWFudCI6eyJpZCI6MjUwMDAwMjMsInNzb0lkIjoiQzUyNDY4NDAiLCJmaXJzdE5hbWUiOiJTYWNoaW4iLCJsYXN0TmFtZSI6Iktlc2Fya2FyIiwiZW1haWwiOiJzYWNoaW4ua2VzYXJrYXJAc2FwLmNvbSIsInNjb3BlIjoiYXdzZGV2Om1tZzp1aSIsInJvbGVDb2RlcyI6WyJMSVZFTElOS19BQ0NPVU5UX0FETUlOIiwiMkZBX1RSSUFMX1JPTEUiLCJBQV9BRE1JTiIsIlNHRV9BQ0NFU1MiLCJQQ19BRE1JTiIsIkxJVkVMSU5LX0FQUCIsIlNBUF9BRE1JTiIsIkxJVkVMSU5LX0VOR0FHRU1FTlQiLCJQQ19PUEVSQVRPUiIsIjJGQV9BRE1JTl9ST0xFIiwiRUVfQURNSU4iLCJMSVZFTElOS19BRE1JTiIsIjJGQV9DTElFTlRfUk9MRSIsIlBDX1ZJRVdFUiIsIkxJVkVMSU5LX1NVUEVSX0FETUlOIiwiTElWRUxJTktfVVNFUiIsIkxMS19FTkdBR0VNRU5UIl19LCJpc3MiOiJTQ0lfQVVUSEVOVElDQVRJT05fQVVUSE9SSVRZIiwianRpIjoidlZtTHdOUUVkY3BNVGZzaVk3SDN6NWJBMjl4Vzczc2oiLCJleHAiOjE1ODQ0NjEwOTF9.gcqHA1xm8uOFt2kRpFCvDHM0IK2yKRsR9NYcdSDPSuPVXlXqNM2IMO9e0FQziwPLR_7z8wuGBr3J1NeRjCc9HeBpvgjKiJxPntFi1rgpjv6GK46zXHdGR__V7GxQoWZBuRsy2LY5Lsr8urnJo8LJL3HFJA44ln8cUqkQYEerL5OZ7H5vXic9_uSZAh--Ir_Kc2fnICgi9tLTe_RrukIDn_uT-X1p2VDNWCQHlOsM9VIBdUdM-uJYxFpRgGbf_40EGUcdEZjJZqHNGC4jiyeE2Rcd-ksX19HC1uHA2EVTgQfAk9WWjuABJIwkI6d_3VA_iLoL-FhGsQjNUQEdgnST4w; LiveLinkTrialDisabled=1\x0Ax-forwarded-proto: https\x0Areferer: https://aa8fd7ab-mmg-mmgingress-ac84-497892438.us-east-1.elb.amazonaws.com/app/\x0Ax-forwarded-for: 205.218.33.30\x0Auser-agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:72.0) Gecko/20100101 Firefox/72.0\x0Ax-requested-with: XMLHttpRequest\x0Aaccept-encoding: gzip, deflate, br\x0Aaccept-language: en-US,en;q=0.5\x0Aaccept: application/json, text/javascript, */*; q=0.01\x0A"


--------------

198.19.8.177 - - [17/Mar/2020:15:21:14 +0000] "POST /campaigns HTTP/1.1" 499 0 "https://aa8fd7ab-mmg-mmgingress-ac84-497892438.us-east-1.elb.amazonaws.com/app/" "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:72.0) Gecko/20100101 Firefox/72.0" 60.001

"Headers: host: aa8fd7ab-mmg-mmgingress-ac84-497892438.us-east-1.elb.amazonaws.com\x0Acontent-type: application/json\x0Ax-amzn-trace-id: Root=1-5e70eaae-c0f70ad78482307e2bb51f66\x0Ax-forwarded-port: 443\x0Acookie: BIGipServerPOOL-demo-livelink.sapmobileservices.com-9999=704835594.3879.0000; LIVELINK=eyJhbGciOiJSUzI1NiJ9.eyJjbGFpbWFudCI6eyJpZCI6MjUwMDAwMjMsInNzb0lkIjoiQzUyNDY4NDAiLCJmaXJzdE5hbWUiOiJTYWNoaW4iLCJsYXN0TmFtZSI6Iktlc2Fya2FyIiwiZW1haWwiOiJzYWNoaW4ua2VzYXJrYXJAc2FwLmNvbSIsInNjb3BlIjoiYXdzZGV2Om1tZzp1aSIsInJvbGVDb2RlcyI6WyJMSVZFTElOS19BQ0NPVU5UX0FETUlOIiwiMkZBX1RSSUFMX1JPTEUiLCJBQV9BRE1JTiIsIlNHRV9BQ0NFU1MiLCJQQ19BRE1JTiIsIkxJVkVMSU5LX0FQUCIsIlNBUF9BRE1JTiIsIkxJVkVMSU5LX0VOR0FHRU1FTlQiLCJQQ19PUEVSQVRPUiIsIjJGQV9BRE1JTl9ST0xFIiwiRUVfQURNSU4iLCJMSVZFTElOS19BRE1JTiIsIjJGQV9DTElFTlRfUk9MRSIsIlBDX1ZJRVdFUiIsIkxJVkVMSU5LX1NVUEVSX0FETUlOIiwiTElWRUxJTktfVVNFUiIsIkxMS19FTkdBR0VNRU5UIl19LCJpc3MiOiJTQ0lfQVVUSEVOVElDQVRJT05fQVVUSE9SSVRZIiwianRpIjoidlZtTHdOUUVkY3BNVGZzaVk3SDN6NWJBMjl4Vzczc2oiLCJleHAiOjE1ODQ0NjEwOTF9.gcqHA1xm8uOFt2kRpFCvDHM0IK2yKRsR9NYcdSDPSuPVXlXqNM2IMO9e0FQziwPLR_7z8wuGBr3J1NeRjCc9HeBpvgjKiJxPntFi1rgpjv6GK46zXHdGR__V7GxQoWZBuRsy2LY5Lsr8urnJo8LJL3HFJA44ln8cUqkQYEerL5OZ7H5vXic9_uSZAh--Ir_Kc2fnICgi9tLTe_RrukIDn_uT-X1p2VDNWCQHlOsM9VIBdUdM-uJYxFpRgGbf_40EGUcdEZjJZqHNGC4jiyeE2Rcd-ksX19HC1uHA2EVTgQfAk9WWjuABJIwkI6d_3VA_iLoL-FhGsQjNUQEdgnST4w; LiveLinkTrialDisabled=1\x0Areferer: https://aa8fd7ab-mmg-mmgingress-ac84-497892438.us-east-1.elb.amazonaws.com/app/\x0Aorigin: https://aa8fd7ab-mmg-mmgingress-ac84-497892438.us-east-1.elb.amazonaws.com\x0Ax-forwarded-proto: https\x0Acontent-length: 28\x0Ax-forwarded-for: 205.218.33.30\x0Auser-agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:72.0) Gecko/20100101 Firefox/72.0\x0Ax-requested-with: XMLHttpRequest\x0Aaccept-encoding: gzip, deflate, br\x0Aaccept-language: en-US,en;q=0.5\x0Aaccept: */*\x0A"
"{\x22campaign_type\x22:\x22QUICKSMS\x22}"


- 1 hour call with Abhilekh.



-------------------- 18 March 2020 --------------------------

- Created a MOP.
- Working with Pankaj on DEV and QA's DB issue.
- performed deployment on Performance environment.
- Worked with Pablo till late night.


------------------- 19 March 2020 ----------------------------
- Resolved the docker issue.

[root@llbpal74 ~]# sudo docker exec -ti bamboo-server-1 bash -c 'docker login -u dbs-docker-user -p n813nZw22a2eVj4N https://dbs-docker-repository.docker.repositories.sap.ondemand.com && docker login -u dbs-docker-user -p n813nZw22a2eVj4N https://dbs-docker-repository.docker.repositories.sapcdn.io'
WARNING! Using --password via the CLI is insecure. Use --password-stdin.
WARNING! Your password will be stored unencrypted in /root/.docker/config.json.
Configure a credential helper to remove this warning. See
https://docs.docker.com/engine/reference/commandline/login/#credentials-store

Login Succeeded
WARNING! Using --password via the CLI is insecure. Use --password-stdin.
WARNING! Your password will be stored unencrypted in /root/.docker/config.json.
Configure a credential helper to remove this warning. See
https://docs.docker.com/engine/reference/commandline/login/#credentials-store

Login Succeeded

	- Restarted the old docker container bamboo-server-1 resolved the issue.

[root@llbpal74 ~]# sudo docker exec -ti bamboo-server-2 bash -c 'docker login -u dbs-docker-user -p n813nZw22a2eVj4N https://dbs-docker-repository.docker.repositories.sap.ondemand.com && docker login -u dbs-docker-user -p n813nZw22a2eVj4N https://dbs-docker-repository.docker.repositories.sapcdn.io'

Login Succeeded

Login Succeeded

------------------------------------

[root@llbpal74 ~]# sudo docker exec -ti bamboo-server-1 bash -c 'docker login -u dbs-docker-user -p n813nZw22a2eVj4N https://dbs-docker-repository.docker.repositories.sap.ondemand.com && docker login -u dbs-docker-user -p n813nZw22a2eVj4N https://dbs-docker-repository.docker.repositories.sapcdn.io'
WARNING! Using --password via the CLI is insecure. Use --password-stdin.
WARNING! Your password will be stored unencrypted in /root/.docker/config.json.
Configure a credential helper to remove this warning. See
https://docs.docker.com/engine/reference/commandline/login/#credentials-store

Login Succeeded
WARNING! Using --password via the CLI is insecure. Use --password-stdin.
WARNING! Your password will be stored unencrypted in /root/.docker/config.json.
Configure a credential helper to remove this warning. See
https://docs.docker.com/engine/reference/commandline/login/#credentials-store

Login Succeeded


- Had a call with Jason, Khalid.



------------------- 19 March 2020 ----------------------------

- VDI down for 6 hours.
- Had a call with Amol Nawale.
- Had a call with Abhilekh.
- Late night call with all the team.


----------------- 21 March 2020 ---------------------------
- Issue with Livelink.



------------------ 23 March 2020 -----------------------------
- Resolved Evelio build issue, check chat. restarted ansible server.
- Dropping the database in AWSDEV for testing purpose.

isql -sDTSILOB4 -Ummg_admin_db_user -X -PD2pB9uRC -w8000

isql -Usa -SDTSILOB4 -w5000 -PY3schokra
DROP DATABASE mmg_db

- Dropped the database.

-bash:2292::sybase@ip-198-19-6-159:/sapci/servers/DTSILOB4/data: isql -Usa -SDTSILOB4 -w5000 -PY3schokra
1> DROP DATABASE mmg_db
2> go
Processed 616 allocation unit(s) out of 6144 units (allocation page 157440). 10% completed.
Processed 1232 allocation unit(s) out of 6144 units (allocation page 315136). 20% completed.
Processed 1848 allocation unit(s) out of 6144 units (allocation page 472832). 30% completed.
Processed 2464 allocation unit(s) out of 6144 units (allocation page 630528). 40% completed.
Processed 3072 allocation unit(s) out of 6144 units (allocation page 786176). 50% completed.
Processed 3688 allocation unit(s) out of 6144 units (allocation page 943872). 60% completed.
Processed 4304 allocation unit(s) out of 6144 units (allocation page 1101568). 70% completed.
Processed 4920 allocation unit(s) out of 6144 units (allocation page 1259264). 80% completed.
Processed 5536 allocation unit(s) out of 6144 units (allocation page 1416960). 90% completed.
Processed 6144 allocation unit(s) out of 6144 units (allocation page 1572608). 100% completed.

- -bash:2304::sybase@ip-198-19-6-159:~/scripts: ls
aat  add  arc  drop  lle  llk  mfa  mmg  ppc  slm  sms
-bash:2305::sybase@ip-198-19-6-159:~/scripts: cd mmg/
-bash:2306::sybase@ip-198-19-6-159:~/scripts/mmg: ls
eng_db.out  mm_db.out  mmg_db.24feb2020.out  mmg_db.sql  queries  sch_db.out  sch_db.sql
-bash:2307::sybase@ip-198-19-6-159:~/scripts/mmg: date
Mon Mar 23 05:23:43 UTC 2020
-bash:2308::sybase@ip-198-19-6-159:~/scripts/mmg: history | grep -i mmg_db.sql
 2186  mv eng_db.sql mmg_db.sql
 2188  vim mmg_db.sql
 2189  cat mmg_db.sql | grep -i  eng
 2190  cat mmg_db.sql | grep -i  mmg
 2193  vim mmg_db.sql
 2203  isql -Usa -SDTSILOB4 -w5000 -PY3schokra -i mmg_db.sql -o mmg_db.24feb2020.out -e
 2287  cat mmg_db.sql | grep -i sybase
 2288  less mmg_db.sql
 2308  history | grep -i mmg_db.sql


cd /sapci/servers/DTSILOB4/data/

- sp_procxmode ‘dbo.SP_CheckCampaigns’,’unchained’



------------------------------ AAT Prod issue ---------------------------------

[aatadm@us4ciapi04 logs]$ tail -f application.log
2020-03-23 06:49:58.556  INFO 19468 --- [http-nio-14550-exec-132] c.s.s.a.a.r.impl.TokenRepositoryImpl     : method="getToken" duration="0ms"
2020-03-23 06:49:58.556  INFO 19468 --- [http-nio-14550-exec-132] c.s.s.a.api.service.OAuthService         : method="getTokenByAccessToken" duration="1ms"
2020-03-23 06:49:58.557  INFO 19468 --- [http-nio-14550-exec-132] c.s.s.a.api.oauth.TokenValidator         : method="getAppByToken" duration="0ms"
2020-03-23 06:49:58.557  INFO 19468 --- [http-nio-14550-exec-132] c.s.s.a.api.oauth.TokenValidator         : method="getClaimantByToken" duration="1ms"
2020-03-23 06:49:58.558  INFO 19468 --- [http-nio-14550-exec-132] c.s.s.a.api.oauth.TokenValidator         : method="validate" duration="0ms"
2020-03-23 06:50:02.074  INFO 19468 --- [http-nio-14550-exec-134] c.s.s.a.api.service.OAuthService         : method="getGrantType" duration="0ms"
2020-03-23 06:50:07.423  INFO 19468 --- [http-nio-14550-exec-133] c.s.s.a.a.r.impl.TokenRepositoryImpl     : method="getToken" duration="0ms"
2020-03-23 06:50:07.423  INFO 19468 --- [http-nio-14550-exec-133] c.s.s.a.api.service.OAuthService         : method="getTokenByAccessToken" duration="1ms"
2020-03-23 06:50:07.425  INFO 19468 --- [http-nio-14550-exec-133] c.s.s.a.api.oauth.TokenValidator         : method="getAppByToken" duration="1ms"
2020-03-23 06:50:07.425  INFO 19468 --- [http-nio-14550-exec-133] c.s.s.a.api.oauth.TokenValidator         : method="getClaimantByToken" duration="1ms"
2020-03-23 06:50:11.194  WARN 19468 --- [http-nio-14550-exec-130] o.h.engine.jdbc.spi.SqlExceptionHelper   : SQL Warning Code: 0, SQLState: 010SK
2020-03-23 06:50:11.194  WARN 19468 --- [http-nio-14550-exec-130] o.h.engine.jdbc.spi.SqlExceptionHelper   : 010SK: Database cannot set connection option SET_READONLY_TRUE.

2020-03-23 06:50:12.183  INFO 19468 --- [http-nio-14550-exec-138] o.s.security.saml.log.SAMLDefaultLogger  : AuthNRequest;SUCCESS;10.200.0.33;sap:ci:365;https://accounts.sap.com;;;
2020-03-23 06:50:12.518  INFO 19468 --- [http-nio-14550-exec-137] c.s.s.a.a.r.impl.TokenRepositoryImpl     : method="getToken" duration="0ms"
2020-03-23 06:50:12.519  INFO 19468 --- [http-nio-14550-exec-137] c.s.s.a.api.service.OAuthService         : method="getTokenByAccessToken" duration="3ms"
2020-03-23 06:50:12.520  INFO 19468 --- [http-nio-14550-exec-137] c.s.s.a.api.oauth.TokenValidator         : method="getAppByToken" duration="0ms"
2020-03-23 06:50:12.520  INFO 19468 --- [http-nio-14550-exec-137] c.s.s.a.api.oauth.TokenValidator         : method="getClaimantByToken" duration="1ms"
2020-03-23 06:50:12.521  INFO 19468 --- [http-nio-14550-exec-137] c.s.s.a.api.oauth.TokenValidator         : method="validate" duration="0ms"
2020-03-23 06:50:13.871  INFO 19468 --- [http-nio-14550-exec-135] c.s.s.a.api.service.OAuthService         : method="getGrantType" duration="0ms"
2020-03-23 06:50:13.872  INFO 19468 --- [http-nio-14550-exec-135] .s.a.a.o.a.ClientCredentialsAuthorizator : method="getApp" duration="1ms"
2020-03-23 06:50:14.237  INFO 19468 --- [http-nio-14550-exec-132] colMessageXMLSignatureSecurityPolicyRule : SAML protocol message was not signed, skipping XML signature processing
2020-03-23 06:50:14.388  INFO 19468 --- [http-nio-14550-exec-132] o.s.security.saml.log.SAMLDefaultLogger  : AuthNResponse;SUCCESS;10.200.0.33;sap:ci:365;https://accounts.sap.com;C5228790;;
2020-03-23 06:50:14.388  INFO 19468 --- [http-nio-14550-exec-132] a.s.SAMLAuthenticationSuccessHandlerImpl : method="onAuthenticationSuccess" clientId="aat:ui:prod"
2020-03-23 06:50:14.388  INFO 19468 --- [http-nio-14550-exec-132] a.s.SAMLAuthenticationSuccessHandlerImpl : method="onAuthenticationSuccess" refresh="true"
2020-03-23 06:50:14.417  WARN 19468 --- [http-nio-14550-exec-132] o.h.engine.jdbc.spi.SqlExceptionHelper   : SQL Warning Code: 0, SQLState: 010SK
2020-03-23 06:50:14.417  WARN 19468 --- [http-nio-14550-exec-132] o.h.engine.jdbc.spi.SqlExceptionHelper   : 010SK: Database cannot set connection option SET_READONLY_TRUE.
2020-03-23 06:50:14.456  WARN 19468 --- [http-nio-14550-exec-132] o.h.engine.jdbc.spi.SqlExceptionHelper   : SQL Warning Code: 0, SQLState: 010SK
2020-03-23 06:50:14.456  WARN 19468 --- [http-nio-14550-exec-132] o.h.engine.jdbc.spi.SqlExceptionHelper   : 010SK: Database cannot set connection option SET_READONLY_TRUE.
2020-03-23 06:50:14.457  INFO 19468 --- [http-nio-14550-exec-132] c.s.s.a.a.s.impl.ClientServiceImpl       : method="getClientMetadata" duration="68ms"
2020-03-23 06:50:14.459  INFO 19468 --- [http-nio-14550-exec-132] c.s.s.a.a.r.impl.TokenRepositoryImpl     : method="getToken" duration="0ms"
2020-03-23 06:50:14.459  INFO 19468 --- [http-nio-14550-exec-132] c.s.s.a.api.util.TokenHelper             : method="getExistingToken" duration="0ms"
2020-03-23 06:50:14.477  INFO 19468 --- [http-nio-14550-exec-132] c.s.s.a.a.r.impl.TokenRepositoryImpl     : method="saveToken" duration="18ms"
2020-03-23 06:50:15.777  INFO 19468 --- [http-nio-14550-exec-134] c.s.s.a.api.service.OAuthService         : method="getGrantType" duration="0ms"
2020-03-23 06:50:15.779  INFO 19468 --- [http-nio-14550-exec-134] .s.a.a.o.a.ClientCredentialsAuthorizator : method="getApp" duration="2ms"
2020-03-23 06:50:15.781  INFO 19468 --- [http-nio-14550-exec-134] c.s.s.a.a.r.impl.TokenRepositoryImpl     : method="getToken" duration="0ms"
2020-03-23 06:50:15.781  INFO 19468 --- [http-nio-14550-exec-134] c.s.s.a.api.util.TokenHelper             : method="getExistingToken" duration="1ms"
2020-03-23 06:50:15.782  INFO 19468 --- [http-nio-14550-exec-134] .s.a.a.o.a.ClientCredentialsAuthorizator : method="authorize" duration="4ms"
^C
[aatadm@us4ciapi04 logs]$ tail -f application.log
2020-03-23 06:50:28.096  INFO 19468 --- [http-nio-14550-exec-128] c.s.s.a.api.oauth.TokenValidator         : method="getClaimantByToken" duration="1ms"
2020-03-23 06:50:28.271  INFO 19468 --- [http-nio-14550-exec-112] c.s.s.a.a.r.impl.TokenRepositoryImpl     : method="getToken" duration="0ms"
2020-03-23 06:50:28.271  INFO 19468 --- [http-nio-14550-exec-112] c.s.s.a.api.service.OAuthService         : method="getTokenByAccessToken" duration="1ms"
2020-03-23 06:50:28.273  INFO 19468 --- [http-nio-14550-exec-112] c.s.s.a.api.oauth.TokenValidator         : method="getAppByToken" duration="1ms"
2020-03-23 06:50:28.273  INFO 19468 --- [http-nio-14550-exec-112] c.s.s.a.api.oauth.TokenValidator         : method="getClaimantByToken" duration="1ms"
2020-03-23 06:50:28.274  INFO 19468 --- [http-nio-14550-exec-112] c.s.s.a.api.oauth.TokenValidator         : method="validate" duration="0ms"
2020-03-23 06:50:31.883  INFO 19468 --- [http-nio-14550-exec-138] c.s.s.a.api.service.OAuthService         : method="getGrantType" duration="0ms"
2020-03-23 06:50:32.019  INFO 19468 --- [http-nio-14550-exec-137] c.s.s.a.api.service.OAuthService         : method="getGrantType" duration="0ms"
2020-03-23 06:50:32.895  WARN 19468 --- [http-nio-14550-exec-135] o.h.engine.jdbc.spi.SqlExceptionHelper   : SQL Warning Code: 0, SQLState: 010SK
2020-03-23 06:50:32.895  WARN 19468 --- [http-nio-14550-exec-135] o.h.engine.jdbc.spi.SqlExceptionHelper   : 010SK: Database cannot set connection option SET_READONLY_TRUE.
2020-03-23 06:50:33.973  INFO 19468 --- [http-nio-14550-exec-132] c.s.s.a.a.r.impl.TokenRepositoryImpl     : method="getToken" duration="1ms"
2020-03-23 06:50:33.973  INFO 19468 --- [http-nio-14550-exec-132] c.s.s.a.api.service.OAuthService         : method="getTokenByAccessToken" duration="2ms"
2020-03-23 06:50:33.975  INFO 19468 --- [http-nio-14550-exec-132] c.s.s.a.api.oauth.TokenValidator         : method="getAppByToken" duration="1ms"
2020-03-23 06:50:33.975  INFO 19468 --- [http-nio-14550-exec-132] c.s.s.a.api.oauth.TokenValidator         : method="getClaimantByToken" duration="1ms"
2020-03-23 06:50:33.976  INFO 19468 --- [http-nio-14550-exec-132] c.s.s.a.api.oauth.TokenValidator         : method="validate" duration="0ms"
2020-03-23 06:50:34.142  INFO 19468 --- [http-nio-14550-exec-134] c.s.s.a.a.r.impl.TokenRepositoryImpl     : method="getToken" duration="1ms"
2020-03-23 06:50:34.142  INFO 19468 --- [http-nio-14550-exec-134] c.s.s.a.api.service.OAuthService         : method="getTokenByAccessToken" duration="3ms"
2020-03-23 06:50:34.143  INFO 19468 --- [http-nio-14550-exec-134] c.s.s.a.api.oauth.TokenValidator         : method="getAppByToken" duration="1ms"
2020-03-23 06:50:34.143  INFO 19468 --- [http-nio-14550-exec-134] c.s.s.a.api.oauth.TokenValidator         : method="getClaimantByToken" duration="1ms"
2020-03-23 06:50:34.144  INFO 19468 --- [http-nio-14550-exec-134] c.s.s.a.api.oauth.TokenValidator         : method="validate" duration="0ms"
2020-03-23 06:50:38.066  INFO 19468 --- [http-nio-14550-exec-133] c.s.s.a.api.service.OAuthService         : method="getGrantType" duration="0ms"
2020-03-23 06:50:38.068  INFO 19468 --- [http-nio-14550-exec-133] .s.a.a.o.a.ClientCredentialsAuthorizator : method="getApp" duration="1ms"
2020-03-23 06:50:38.070  INFO 19468 --- [http-nio-14550-exec-133] c.s.s.a.a.r.impl.TokenRepositoryImpl     : method="getToken" duration="0ms"
2020-03-23 06:50:38.070  INFO 19468 --- [http-nio-14550-exec-133] c.s.s.a.api.util.TokenHelper             : method="getExistingToken" duration="1ms"
2020-03-23 06:50:38.071  INFO 19468 --- [http-nio-14550-exec-133] .s.a.a.o.a.ClientCredentialsAuthorizator : method="authorize" duration="3ms"
2020-03-23 06:50:39.097  INFO 19468 --- [http-nio-14550-exec-130] c.s.s.a.api.service.OAuthService         : method="getGrantType" duration="0ms"
2020-03-23 06:50:39.099  INFO 19468 --- [http-nio-14550-exec-130] .s.a.a.o.a.ClientCredentialsAuthorizator : method="getApp" duration="1ms"
2020-03-23 06:50:40.218  INFO 19468 --- [http-nio-14550-exec-128] c.s.s.a.a.r.impl.TokenRepositoryImpl     : method="getToken" duration="0ms"
2020-03-23 06:50:40.218  INFO 19468 --- [http-nio-14550-exec-128] c.s.s.a.api.service.OAuthService         : method="getTokenByAccessToken" duration="1ms"
2020-03-23 06:50:40.219  INFO 19468 --- [http-nio-14550-exec-128] c.s.s.a.api.oauth.TokenValidator         : method="getAppByToken" duration="1ms"



  683  cd /opt/aatadm/aat_SpringBoot/
  
  pkill -u aatadm
  
  
java -Dspring.config.additional-location=/opt/aatadm/aat_SpringBoot/conf/environment.properties -Dlog.execution-time -jar /opt/aatadm/aat_SpringBoot/authentication-authority-api.jar &

- Had 1 hours call with Khalid.
- Had 1.5 hours call with ChenYu regarding MMG integration with LLK and AAT.
- Had 3 hours call with MMG team on rectifying the things.



------------------ 24 March 2020 -----------------------------

- Had a call with Ruksana, regarding hitting the API from frontend. Cleaned the DB.
- 

docker run -d --restart=always --hostname 127.0.0.1 --name docker-rabbit-aws -p 4369:4369 -p 5672:5672 -p 15672:15672 -p 25672:25672 -p 35197:35197 -e RABBITMQ_DEFAULT_USER=admin -e RABBITMQ_DEFAULT_PASS=gv946uhFPY -e RABBITMQ_ERLANG_COOKIE='NSORXJLSGOOCLYMVCDQF' rabbitmq:3-management

- Had 1 hour call with Khalid.

- Had 2 hour call with ChenYu.

- Provided support to MMG team.

- Edited the JIRA MOP ticket for helm, IAM user, steps to create the cluster and all.


------------ 25 March 2020 -----------------------
- Provided support to AAT downtime issue





------------ 26 March 2020 --------------------
Raised PAR for balaji : 100172594

- Discussed with Amol regarding the Jdk 1.8 on bdtrust.
- VDI was down for 1 hour.
- Resolved the Shikh Shah build issue of, https://bamboo.di-infra.sap.corp/deploy/viewDeploymentResult.action?deploymentResultId=143032852
	- restarted the ansibler tower server.
	
- Committed changes to the GIT repository.

- Worked on helm chart.

- Pablo chat doubts.
	
	- We have start this activity from monday onwards ?
	- MDIA-2 : We have to assign the subtasks to Network team after filling the template.
	- MDIA-17 : You have mentioned account service just for reference ? Need to assign each subtask to Augusto only ?
	- MDIA-3 : Need to assign to network team ?
	- MDIA-4 : Need to assign to network team ? How we will test it if open or not ?
	- MDIA-6 : why do we require this ?
	- MDIA-14, 16 : UI testing, performance testing, to whom to assign.
	- MDIA-1 : Can others do this ? or only augusto can do it ? how we will verify it ?
	- MDIA-7 : Import *.sapdigitalinterconnect.com wildcard certificate
	- MDIA-5 : Configure CNAME to ALB, who will do this ? Everything needs to be done from command line ?
	- We are missing configure the user to have a proper access rights from IAM and also to configure the helm.
	
	- I can see the Target start and End Date but where to set it for each task ?
	

- Questions to Deigo.
	- What is your approach to create a chart for new application, i meant the hierarcy of configmap, svc, deployment. Can you please check this directory and let me know if it is correct. I am trying to create a SubCharts.
	
		[pablo@ip-198-19-11-212 mmg_chart]$ pwd
		/home/pablo/eng/kubernetes/mmg_helm/mmg_chart
		[pablo@ip-198-19-11-212 mmg_chart]$ ls charts
		mmg-be  mmg-sch  mmg-ui
	
	- Can we debug only a singe template, what would be command for that.
	- Do you have list of commands that you use often to create/debug the charts.


--------------- 27 March 2020 ---------------------
- Issue facing while reaching to A2P url.	: 3 hours for analysis. on call with NOC. Found that the issue is related to black list hub account.

https://apm.cf.us10.hana.ondemand.com/e/f95fd477-ba09-47d0-ba91-d6f35a4e2f22/#responsetimeanalysis;sci=SERVICE-094671BC6423E9AB;eventid=5923217061953834502_1585276020000;pid=6128083115016006079_1585276020000V2;timeframe=custom1585276020000to1585283160000;servicefilter=0%1E10%11SERVICE_METHOD_GROUP-7902774419517426%109%11SERVICE_METHOD-A5A90BAC8BE9923C%14%5C0api%5C0v2%5C0sms;gf=all

	- Found a high response time. Refer to send mail to Network team.
	
	


Mar 27 03:51:17 us4ciapi03 docker/4f98860adae9[20634]: org.springframework.web.client.ResourceAccessException: I/O error on POST request for "https://sms-eu2-mpls.sapdigitalinterconnect.com/cmn/sap_liveli46650/sap_liveli46650.sms": Connect to sms-eu2-mpls.sapdigitalinterconnect.com:443 [sms-eu2-mpls.sapdigitalinterconnect.com/178.248.228.11] failed: connect timed out; nested exception is org.apache.http.conn.ConnectTimeoutException: Connect to sms-eu2-mpls.sapdigitalinterconnect.com:443 [sms-eu2-mpls.sapdigitalinterconnect.com/178.248.228.11] failed: connect timed out
Mar 27 03:51:17 us4ciapi01 docker/cdb88ea2c24b[46271]: org.springframework.web.client.ResourceAccessException: I/O error on POST request for "https://sms-eu2-mpls.sapdigitalinterconnect.com/cmn/sap_liveli46650/sap_liveli46650.sms": Connect to sms-eu2-mpls.sapdigitalinterconnect.com:443 [sms-eu2-mpls.sapdigitalinterconnect.com/178.248.228.11] failed: connect timed out; nested exception is org.apache.http.conn.ConnectTimeoutException: Connect to sms-eu2-mpls.sapdigitalinterconnect.com:443 [sms-eu2-mpls.sapdigitalinterconnect.com/178.248.228.11] failed: connect timed out
Mar 27 03:51:17 us4ciapi03 docker/4f98860adae9[20634]: org.springframework.web.client.ResourceAccessException: I/O error on POST request for "https://sms-eu2-mpls.sapdigitalinterconnect.com/cmn/sap_liveli46650/sap_liveli46650.sms": Connect to sms-eu2-mpls.sapdigitalinterconnect.com:443 [sms-eu2-mpls.sapdigitalinterconnect.com/178.248.228.11] failed: connect timed out; nested exception is org.apache.http.conn.ConnectTimeoutException: Connect to sms-eu2-mpls.sapdigitalinterconnect.com:443 [sms-eu2-mpls.sapdigitalinterconnect.com/178.248.228.11] failed: connect timed out
Mar 27 03:51:17 us4ciapi03 docker/4f98860adae9[20634]: org.springframework.web.client.ResourceAccessException: I/O error on POST request for "https://sms-eu2-mpls.sapdigitalinterconnect.com/cmn/sap_liveli46650/sap_liveli46650.sms": Connect to sms-eu2-mpls.sapdigitalinterconnect.com:443 [sms-eu2-mpls.sapdigitalinterconnect.com/178.248.228.11] failed: connect timed out; nested exception is org.apache.http.conn.ConnectTimeoutException: Connect to sms-eu2-mpls.sapdigitalinterconnect.com:443 [sms-eu2-mpls.sapdigitalinterconnect.com/178.248.228.11] failed: connect timed out
Mar 27 03:51:17 us4ciapi03 docker/4f98860adae9[20634]: org.springframework.web.client.ResourceAccessException: I/O error on POST request for "https://sms-eu2-mpls.sapdigitalinterconnect.com/cmn/sap_liveli46650/sap_liveli46650.sms": Connect to sms-eu2-mpls.sapdigitalinterconnect.com:443 [sms-eu2-mpls.sapdigitalinterconnect.com/178.248.228.11] failed: connect timed out; nested exception is org.apache.http.conn.ConnectTimeoutException: Connect to sms-eu2-mpls.sapdigitalinterconnect.com:443 [sms-eu2-mpls.sapdigitalinterconnect.com/178.248.228.11] failed: connect timed out
Mar 27 03:51:17 us4ciapi03 docker/4f98860adae9[20634]: org.springframework.web.client.ResourceAccessException: I/O error on POST request for "https://sms-eu2-mpls.sapdigitalinterconnect.com/cmn/sap_liveli46650/sap_liveli46650.sms": Connect to sms-eu2-mpls.sapdigitalinterconnect.com:443 [sms-eu2-mpls.sapdigitalinterconnect.com/178.248.228.11] failed: connect timed out; nested exception is org.apache.http.conn.ConnectTimeoutException: Connect to sms-eu2-mpls.sapdigitalinterconnect.com:443 [sms-eu2-mpls.sapdigitalinterconnect.com/178.248.228.11] failed: connect timed out




- Build MMG on performance and deploy it.

https://bamboo.di-infra.sap.corp/browse/MMG-SCHED1-21
https://bamboo.di-infra.sap.corp/deploy/viewDeploymentResult.action?deploymentResultId=143032983 - restarted the Ansible Tower.

- Rajul failed deployment.
https://bamboo.di-infra.sap.corp/deploy/viewDeploymentResult.action?deploymentResultId=143032982


- Allocation of Tasks.

VPC creation at AWSX : Handeras		30-31
- Configure 6 nodes. 3 for redis and 3 for rabbitmq.	31

Deigo : 
	- Deploy EKS and configure HELM and configure user with proper rights.	31-1

Pablo :
	- IPSEC VPN to IAD.
	- Import *.sapdigitalinterconnect.com wildcard certificate to AWSX.	30-1

Sachin :
	- Deploy Scheduler, Core & UI.	2-4
	- Firewall requests to allow Ingress/Egress between AWSX and IAD1.	30-2
	- Make ICMP work for IAD1Bastion-Network to AWSX.					30-2
	- Sanity Testing.													6-8

Augusto :
	- Setup RabbitMq and Redis.		1-2
	- Configure CNAME to ALB.		1-2
	
Nidhi :
	- UI testing.					8-10
	
Anol : performance testing.			13-14

-----------------------------------------------

AWS on AWS1 and Frankfurt.

Handeras :
1) MDIA-1: VPC creation at AWS Frankfurt.
2) Create 3 instances (aws1redis006~8) for Redis at AWS1.
3) Create 2 instances (aws1rabbitmq006~8) for Rabbit at AWS1.

Pablo :
1) MDIA-3: IPSEC VPN to IAD1
2) MDIA-7: Import *.sapdigitalinterconnect.com wildcard certificate into AWSX


Augusto :
1) MDIA-21 : Install Redis cluster on (aws1redis006~8) at AWS1.
2) MDIA-22 : Install RabbitMQ cluster on (aws1rabbitmq006~8) at AWS1.
3) MDIA-5: Configure CNAME for ALB

Diego:
1) MDIA-6: Make ICMP Work for IAD1Bastion-Network (172.24.227.0) to AWSX (10.X.0.0/16)
2) MDIA-4: Firewall requests to allow Ingress/Egress connectivity between AWS1 and IAD1

Sachin :
1) MDIA-18~20 : Deploy UI, Core, Scheduler Service.
2) MDIA-24 : Cost performance analysis for Cluster and Databases.( I will need help here, as I am not exactly aware about the process)

Febu!@34






------------------ 30 March 2020 ----------------------------

28-29 March : created the template for deployment.

5246840@llbpal51:~> docker ps | grep -i rabbit
Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
c5246840@llbpal51:~> sudo su -
llbpal51:~ # docker ps | grep -i rabbit
Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
llbpal51:~ # uptime
 23:35pm  up 2 days  5:50,  2 users,  load average: 0.18, 0.09, 0.15
 
- Performed deployment for geo-branch on Demo.

https://bamboo.di-infra.sap.corp/deploy/viewDeploymentResult.action?deploymentResultId=144310433

- Installing the procedures on AWSDEV environment. Check chat for details.

isql -sDTSILOB4 -Ummg_admin_db_user -X -PD2pB9uRC -w8000

isql -Usa -SDTSILOB4 -w5000 -PY3schokra -i mmg_db.sql -o mmg_db.23mar2020.out -e


isql -sDTSILOB4 -Ummg_admin_db_user -X -PD2pB9uRC -i 1_checkCampaigns.sql -o 1_checkCampaigns.out -e

isql -sDTSILOB4 -Ummg_admin_db_user -X -PD2pB9uRC -i 2_chkCampSmsBroadcast.sql -o 2_chkCampSmsBroadcast.out -e

isql -sDTSILOB4 -Ummg_admin_db_user -X -PD2pB9uRC -i 3_updateDeliveryStatus.sql -o 3_updateDeliveryStatus.out -e

isql -sDTSILOB4 -Ummg_admin_db_user -X -PD2pB9uRC -i 4_campaignChk.sql -o 4_campaignChk.out -e

isql -sDTSILOB4 -Ummg_admin_db_user -X -PD2pB9uRC -w8000 -i ./insert_sms.sql -o ./insert_sms.sql.out

-------------- done executing the queries -----------------------
-bash:2375::sybase@ip-198-19-6-159:~/scripts/mmg/queries: isql -sDTSILOB4 -Ummg_admin_db_user -X -PD2pB9uRC -i 1_checkCampaigns.sql -o 1_checkCampaigns.out -e
-bash:2376::sybase@ip-198-19-6-159:~/scripts/mmg/queries: echo 4?
4?
-bash:2377::sybase@ip-198-19-6-159:~/scripts/mmg/queries: echo $?
0
-bash:2378::sybase@ip-198-19-6-159:~/scripts/mmg/queries:
-bash:2378::sybase@ip-198-19-6-159:~/scripts/mmg/queries: isql -sDTSILOB4 -Ummg_admin_db_user -X -PD2pB9uRC -i 2_chkCampSmsBroadcast.sql -o 2_chkCampSmsBroadcast.out -e
-bash:2379::sybase@ip-198-19-6-159:~/scripts/mmg/queries: echo $?
0
-bash:2380::sybase@ip-198-19-6-159:~/scripts/mmg/queries:
-bash:2380::sybase@ip-198-19-6-159:~/scripts/mmg/queries: isql -sDTSILOB4 -Ummg_admin_db_user -X -PD2pB9uRC -i 3_updateDeliveryStatus.sql -o 3_updateDeliveryStatus.out -e
-bash:2381::sybase@ip-198-19-6-159:~/scripts/mmg/queries: echo $?
0
-bash:2382::sybase@ip-198-19-6-159:~/scripts/mmg/queries: isql -sDTSILOB4 -Ummg_admin_db_user -X -PD2pB9uRC -i 4_campaignChk.sql -o 4_campaignChk.out -e
-bash:2383::sybase@ip-198-19-6-159:~/scripts/mmg/queries: echo $?
0
-bash:2384::sybase@ip-198-19-6-159:~/scripts/mmg/queries: ls
1_checkCampaigns.out  2_chkCampSmsBroadcast.out  3_updateDeliveryStatus.out  4_campaignChk.out  checkCampaigns.out
1_checkCampaigns.sql  2_chkCampSmsBroadcast.sql  3_updateDeliveryStatus.sql  4_campaignChk.sql  insert_sms.sql
-bash:2385::sybase@ip-198-19-6-159:~/scripts/mmg/queries: ls *.out
1_checkCampaigns.out  2_chkCampSmsBroadcast.out  3_updateDeliveryStatus.out  4_campaignChk.out  checkCampaigns.out
-bash:2386::sybase@ip-198-19-6-159:~/scripts/mmg/queries:
-bash:2386::sybase@ip-198-19-6-159:~/scripts/mmg/queries:
-bash:2386::sybase@ip-198-19-6-159:~/scripts/mmg/queries: cat *.out
-bash:2387::sybase@ip-198-19-6-159:~/scripts/mmg/queries:

- Performed the deployment of the MMG on AWSDEV.


----------------------------- 31 March 2020 --------------------------------

- Add nandkumar to JIRA group on LDAP.
- Worked on Ekta Chattbar login to JIRA request.

- Provided command output to Pankaj regarding SP.

sp_helptext SP_CheckCampaigns
sp_helptext SP_CheckCampaign_SmsBroadcast

- Helped Amol with his queries.
- Helped Rajul to provide her access on demo and build/deploy code on demo.

- Redeployed the mmg application from github.

- Dropping the procedure as per Pankaj request.

drop procedure SP_CheckCampaign_SmsBroadcast
drop procedure SP_CheckCampaigns


 2517  history | grep -i isql | tail -n 5
-bash:2518::sybase@ip-198-19-6-159:~/scripts/mmg/queries: history | grep -i isql | grep -i 3_SP_CheckCampaigns.sql
 2459  isql -sDTSILOB4 -Ummg_admin_db_user -X -PD2pB9uRC -w8000 -i ./3_SP_CheckCampaigns.sql -o ./3_SP_CheckCampaigns.out
 2518  history | grep -i isql | grep -i 3_SP_CheckCampaigns.sql
-bash:2519::sybase@ip-198-19-6-159:~/scripts/mmg/queries: history | grep -i isql | grep -i 2_CheckCampaign_SmsBroadcast.sql
 2445  isql -sDTSILOB4 -Ummg_admin_db_user -X -PD2pB9uRC -w8000 -i ./2_CheckCampaign_SmsBroadcast.sql -o ./2_CheckCampaign_SmsBroadcast.out
 2519  history | grep -i isql | grep -i 2_CheckCampaign_SmsBroadcast.sql
-bash:2520::sybase@ip-198-19-6-159:~/scripts/mmg/queries:
-bash:2520::sybase@ip-198-19-6-159:~/scripts/mmg/queries: history | grep -i isql | grep -i insert_sms
 2267  isql -sDTSILOB4 -Ummg_admin_db_user -X -PD2pB9uRC -w8000 -i insert_sms.sql -o insert_sms.sql.out
 2269  isql -sDTSILOB4 -Ummg_admin_db_user -X -PD2pB9uRC -w8000 -i ./insert_sms.sql -o ./insert_sms.sql.out
 2520  history | grep -i isql | grep -i insert_sms
-bash:2521::sybase@ip-198-19-6-159:~/scripts/mmg/queries:


 isql -Usa -SDTSILOB4 -w5000 -PY3schokra -i ./2_CheckCampaign_SmsBroadcast.sql -o ./2_CheckCampaign_SmsBroadcast.out
 
 
 
CREATE OR REPLACE PROCEDURE [dbo].SP_CheckCampaigns
(
	@campaign_count int = 10,
	@campaign_expiry int = 7,
	@return_status int OUTPUT
	--x: number of campaigns being checked
)
AS
BEGIN
	DECLARE @current_count int
	DECLARE @campaign_type varchar(20)
	DECLARE @campaign_id int
	DECLARE @tmp_status int
	SET @current_count = 0
	--Create processing table | not able to use tempdb because of strange uncommitted issue
	IF NOT EXISTS (SELECT 1 FROM sysobjects WHERE name = 'CampaignChecking' AND type = 'U')
	BEGIN 
		PRINT 'Create Table'
		EXECUTE('CREATE TABLE CampaignChecking (campaign_id int, campaign_type varchar(20), result_value int)')
	END
	DELETE FROM CampaignChecking
	WHILE @campaign_count > @current_count
	BEGIN
		SELECT TOP 1 @campaign_id = id, @campaign_type = campaign_type FROM campaign WHERE status = 'RUNNING' AND campaign_type = 'SMSBROADCAST' ORDER BY checked_on ASC
		--No running campaign
		IF (@@rowcount = 0)
		BEGIN 
			GOTO finished
		END
		--Campaign repeated
		IF EXISTS (SELECT 1 FROM CampaignChecking WHERE campaign_id = @campaign_id)
		BEGIN 
			GOTO finished 
		END
		IF (@campaign_type = 'SMSBROADCAST')
		BEGIN 
			EXEC SP_CheckCampaign_SmsBroadcast @campaign_id, @campaign_expiry, @tmp_status OUTPUT
		END
		ELSE IF (@campaign_type = 'QUICKSMS')
		BEGIN 
			PRINT 'QUICKSMS'
		END
		ELSE 
		BEGIN 
			PRINT 'OTHER'
		END
		INSERT INTO CampaignChecking VALUES (@campaign_id, @campaign_type, @tmp_status)
		SET @current_count = @current_count + 1
	END 
finished:
	UPDATE campaign SET checked_on = GETUTCDATE() WHERE id = @campaign_id
	/* Adaptive Server has expanded all '*' elements in the following statement */ SELECT CampaignChecking.campaign_id, CampaignChecking.campaign_type, CampaignChecking.result_value FROM CampaignChecking
	SET @return_status = @current_count
	RETURN @return_status 
END

-- Edited the procedure to make it run in the ASE Database server.
	- Check the #mmg-devops chat window.
	
-------------------- 1 April 2020 -----------------
- Dipti reported issue of deployment.
	https://bamboo.di-infra.sap.corp/deploy/viewDeploymentResult.action?deploymentResultId=144310671
	https://bamboo.di-infra.sap.corp/deploy/viewDeploymentResult.action?deploymentResultId=144310669
	
	- found this error.
	failed: [us4cipapi02] (item={u'volumes': [u'/opt/llk/eventLogs/:/opt/llk/eventLogs'], u'layer': u'be', u'ports': [u'15700:15700', u'15701:15701'], u'ulimits': [u'nofile:65535:65535'], u'name': u'llk-sms-channel'}) => {"failed": true, "item": {"layer": "be", "name": "llk-sms-channel", "ports": ["15700:15700", "15701:15701"], "ulimits": ["nofile:65535:65535"], "volumes": ["/opt/llk/eventLogs/:/opt/llk/eventLogs"]}, "msg": "Error inspecting container: UnixHTTPConnectionPool(host='localhost', port=None): Read timed out. (read timeout=60)"}
	
	- found docker is not starting.
	
	- Started daemon after removing the docker.pid.
	
	- Had a call with Khalid.
	
	- Supported Amol, MMG team.
	
	- Check mails.
	
	
	
-------------------- 2 April 2020 ---------------------

2020-04-01 17:17:34.769 ERROR 7 --- [ment-Executor-1] i.g.j.c.liquibase.AsyncSpringLiquibase   : Liquibase could not start correctly, your database is NOT ready: Validation Failed:
     2 change sets check sum
          config/liquibase/changelog/SP_CheckCampaign_SmsBroadcast.xml::createProcedure-checkCampaigns-smsBroadcast::jhipster was: 7:ceadd917695543b9711f6e5846667c77 but is now: 7:346e33c419cc1c642a8cd997200df6be
....
....
2020-04-01 17:17:36.846  WARN 7 --- [           main] ConfigServletWebServerApplicationContext : Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'tokenProvider': Invocation of init method failed; nested exception is java.lang.IllegalArgumentException: Illegal base64 character 22
2020-04-01 17:17:36.865 DEBUG 7 --- [           main] com.zaxxer.hikari.pool.HikariPool        : HikariPool-1 - Before shutdown stats (total=3, active=1, idle=2, waiting=0)

| quote | b64enc

mmg-be-secret
AAT_PUBLIC_KEY - requires quote

mmg-sch-secret
AAT_PUBLIC_KEY - require quote


- Seeing if by removing the base64 characters things can be set or not.

	-- When I remove base64 encoding it gives following error. It means it needs base64 encoded string but there is some conditions to numeric characters.

[debug] CHART PATH: /home/pablo/eng/kubernetes/GIT_AWSDEV/devops/mmg/mmg_helm/mmg_chart

Error: release mmgsachin failed: Secret in version "v1" cannot be handled as a Secret: v1.Secret.Data: decode base64: illegal base64 data at input byte 4, error found in #10 byte of ...|:"4hh9SmX","SCHEDULE|..., bigger context ...|SE_PASSWORD":"D2pB9uRC","REDIS_PASSWORD":"4hh9SmX","SCHEDULER_APP_KEY":"pGRtoA0Vn4ev4KgS","SCHEDULER|...


	-- Hence we need to check for each password specifically.

-- Finally resolved the helm_chart after sitting for 5 hours.	
	
 1335  helm del --purge mmgsachin --tls
 1336  helm install --name mmgsachin . --debug --tls
 1337  kubectl get pods -n mmg
 1338  kubectl logs -f mmg-be-deployment-9846b7576-snnws -n mmg


 kubectl get pods -n mmg



- Resolved the issue of changelock after a container got abruptly terminated.

Currently locked by mmg-sch-deployment-f6f9b66-9pbxr (198.19.7.6) since 4/2/20 5:32 P 


2020-04-02 17:54:44.449  WARN 1 --- [           main] ConfigServletWebServerApplicationContext : Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'liquibase' defined in class path resource [com/sap/di/config/DatabaseConfiguration.class]: Invocation of init method failed; nested exception is liquibase.exception.LockException: Could not acquire change log lock.  Currently locked by mmg-sch-deployment-f6f9b66-9pbxr (198.19.7.6) since 4/2/20 5:32 PM
2020-04-02 17:54:47.517 ERROR 1 --- [           main] o.s.boot.SpringApplication               : Application run failed


-------------------- 3 April 2020 ---------------------

- Resolved the issue of Ravi.
	https://bamboo.di-infra.sap.corp/browse/LIV-LLKAN0-29
	https://bamboo.di-infra.sap.corp/browse/LIV-LLKAP-54
	https://bamboo.di-infra.sap.corp/browse/LIV-LIVSBA-9
	
- Resolved Rajul issue as well.
		- Check llk-devops chat.
	
- Demo's docker is down.

skesarkar@us4cidemo002 $
skesarkar@us4cidemo002 $ docker ps
Cannot connect to the Docker daemon. Is 'docker -d' running on this host?
skesarkar@us4cidemo002 $ date
Fri Apr  3 07:32:33 GMT 2020
skesarkar@us4cidemo002 $ uptime
 07:32:35 up 388 days, 16:19,  3 users,  load average: 0.11, 0.13, 0.13
skesarkar@us4cidemo002 $

---- resolved the issue -----
root@us4cidemo002 $ ls /var/lock/
lvm/    subsys/
root@us4cidemo002 $ ls /var/run/docker
docker/      docker.pid   docker.sock
root@us4cidemo002 $ cd /var/run/
root@us4cidemo002 $ ls -l docker.
docker.pid   docker.sock
root@us4cidemo002 $ ls -l docker.
docker.pid   docker.sock
root@us4cidemo002 $ ls -l docker.pid
-rw-r--r-- 1 root root 5 Nov 25 14:04 docker.pid
root@us4cidemo002 $ rm docker.pid
rm: remove regular file `docker.pid'? y
root@us4cidemo002 $ /etc/init.d/docker status
docker dead but subsys locked
root@us4cidemo002 $ /etc/init.d/docker start
Starting docker:                                           [  OK  ]
root@us4cidemo002 $ /etc/init.d/docker status
docker (pid  10692) is running...
root@us4cidemo002 $ docker ps

Error response from daemon: Cannot start container 4c40946fc998: Bind for 0.0.0.0:16100 failed: port is already allocated
Error: failed to start containers: [4c40946fc998]
root@us4cidemo002 $ docker start 9c77ef91ecfc
Error response from daemon: Cannot start container 9c77ef91ecfc: Bind for 0.0.0.0:16600 failed: port is already allocated
Error: failed to start containers: [9c77ef91ecfc]
root@us4cidemo002 $ docker start 04fef5e30491
Error response from daemon: Cannot start container 04fef5e30491: Bind for 0.0.0.0:16300 failed: port is already allocated
Error: failed to start containers: [04fef5e30491]
root@us4cidemo002 $ docker start c7d84a8da39a
Error response from daemon: Cannot start container c7d84a8da39a: Bind for 0.0.0.0:17201 failed: port is already allocated
Error: failed to start containers: [c7d84a8da39a]


root@us4cidemo002 $ netstat -ntulp | grep -i 17201
tcp        0      0 :::17201                    :::*                        LISTEN      11322/docker-proxy
root@us4cidemo002 $ ps aux | grep -i 11322
root     11322  0.0  0.0 147876  6752 ?        Sl   07:37   0:00 docker-proxy -proto tcp -host-ip 0.0.0.0 -host-port 17201 -container-ip 172.17.0.4 -container-port 17201
root     24907  0.0  0.0 103328   904 pts/0    S+   07:56   0:00 grep -i 11322
root@us4cidemo002 $ kill 11322
root@us4cidemo002 $ ps aux | grep -i 11322
root     11322  0.0  0.0      0     0 ?        Z    07:37   0:00 [docker] <defunct>
root     24939  0.0  0.0 103328   908 pts/0    S+   07:56   0:00 grep -i 11322
root@us4cidemo002 $ netstat -ntulp | grep -i 17201
root@us4cidemo002 $

root@us4cidemo002 $ ps aux | grep -i 11322
root     11322  0.0  0.0      0     0 ?        Z    07:37   0:00 [docker] <defunct>
root     24939  0.0  0.0 103328   908 pts/0    S+   07:56   0:00 grep -i 11322
-----------------------


docker start 07928edae145
docker start 65b73b718a82
docker start 50d9539764c6
docker start 91c640c86abe
docker start f51a55c74317
docker start bf18a48633d3
docker start cf6f670cb87e
docker start 9c5359b7377f
docker start b19acdbdd255
docker start 532ecaf0be89
docker start 2cb1bf9dae0a
docker start 2fbaac46f884
docker start 0e60d0c76156

-----------------------
docker start d11d33227d12
docker start de87bb2a973d
docker start 46686769e4a3
docker start 73c881c0bef4
docker start cca135d472b8
docker start 07928edae145
docker start 89030ec364ec
docker start 69eae1f6052f
docker start 1df350650be5
docker start c62729cf5cf3
docker start 9cca818d3898
docker start 08d8d67acae8
docker start 173c33675a84
docker start 6ab6213237be
docker start 65b73b718a82
docker start b330cd498f48
docker start 442ba235643b
docker start 20f7b2e52321
docker start 50d9539764c6
docker start 91c640c86abe
docker start f51a55c74317
docker start bf18a48633d3
docker start cf6f670cb87e
docker start 9c5359b7377f
docker start b19acdbdd255
docker start 5cf497239bf1
docker start 1751a3fe4faf
docker start 532ecaf0be89
docker start 9fb92576129f
docker start 2e8158f04dae
docker start 2cb1bf9dae0a
docker start 2fbaac46f884
docker start 0e60d0c76156
----------------------------------------------------------------------------------------

- Had a 30 minutes call with Bhomik, Mohit. regarding JIRA test case migration.

----------------------------------------------------------------------------------------

- Had a call with Barani for 30 minutes regarding this.


https://sapdips.awsapps.com/console


- Prod deployment details.

backend:
  scheduler_app_key: pGRtoA0Vn4ev4KgS
  scheduler_app_secret: njxrzNZwhGPiKR8vCgDAHpwsHTc5Hk07
  aat_public_key: "MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAzfJWm7sdzKf3cG7fKK5/B8rl5KmbsimqYy1mhSqtWQ7ux6nfeS3mK7lrovP7HQhld5u7DabJOKHffqasPprvhuCIuebCnqi+1otRsaK5a9OTBmToO6mZTKXOWKyNscrDftsM0qR8B/aqo8AJl794v6pG6vt/I8pM9dnTPgnoK2che4YXA6rq8Q/JFihMzMMM6hbTJuoGFqgwMKu1xEVWf6QBUuoXdyWrDYj90c+eXHebH9sDm8yPI/sdv2L9zvNSJsDZ8t82aEzVbo7x7fFO606F6leGgq8qqrpQJGqz5YxlQJRDi4+6ckX8EaHVaifsQGCWPthVnpe0lfU9Rk1bMQIDAQAB"

scheduler:
  aat_public_key: "MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAzfJWm7sdzKf3cG7fKK5/B8rl5KmbsimqYy1mhSqtWQ7ux6nfeS3mK7lrovP7HQhld5u7DabJOKHffqasPprvhuCIuebCnqi+1otRsaK5a9OTBmToO6mZTKXOWKyNscrDftsM0qR8B/aqo8AJl794v6pG6vt/I8pM9dnTPgnoK2che4YXA6rq8Q/JFihMzMMM6hbTJuoGFqgwMKu1xEVWf6QBUuoXdyWrDYj90c+eXHebH9sDm8yPI/sdv2L9zvNSJsDZ8t82aEzVbo7x7fFO606F6leGgq8qqrpQJGqz5YxlQJRDi4+6ckX8EaHVaifsQGCWPthVnpe0lfU9Rk1bMQIDAQAB"
  
  
ui:
  aat_client_id: awsdev:mmg:ui
  aat_client_secret: PW1OCNLWB7LB3EHOFDF664KAADWPTCDI
  aat_user_roles: LIVELINK_USER
  aat_scope: livelink:api
  aat_app_role: LIVELINK_APP
  aat_user_role: LIVELINK_USER
  aat_cookie_name: LIVELINK
  aat_login_action: login
  aat_logout_action: logout
  aat_accpeted_roles: LIVELINK_TRIAL
  jwt_certificate: '-----BEGIN CERTIFICATE-----\nMIIDUjCCAjoCCQDTwddPSLm8jzANBgkqhkiG9w0BAQUFADBrMQswCQYDVQQGEwJV\nUzERMA8GA1UECBMIVmlyZ2luaWExDzANBgNVBAcTBlJlc3RvbjEMMAoGA1UEChMD\nU0FQMRgwFgYDVQQLEw9Nb2JpbGUgU2VydmljZXMxEDAOBgNVBAMTB3NhcC5jb20w\nHhcNMTcwMjIwMTczMzUxWhcNMjAwMjIwMTczMzUxWjBrMQswCQYDVQQGEwJVUzER\nMA8GA1UECBMIVmlyZ2luaWExDzANBgNVBAcTBlJlc3RvbjEMMAoGA1UEChMDU0FQ\nMRgwFgYDVQQLEw9Nb2JpbGUgU2VydmljZXMxEDAOBgNVBAMTB3NhcC5jb20wggEi\nMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDN8labux3Mp/dwbt8orn8HyuXk\nqZuyKapjLWaFKq1ZDu7Hqd95LeYruWui8/sdCGV3m7sNpsk4od9+pqw+mu+G4Ii5\n5sKeqL7Wi1Gxorlr05MGZOg7qZlMpc5YrI2xysN+2wzSpHwH9qqjwAmXv3i/qkbq\n+38jykz12dM+CegrZyF7hhcDqurxD8kWKEzMwwzqFtMm6gYWqDAwq7XERVZ/pAFS\n6hd3JasNiP3Rz55cd5sf2wObzI8j+x2/Yv3O81ImwNny3zZoTNVujvHt8U7rToXq\nV4aCryqqulAkarPljGVAlEOLj7pyRfwRodVqJ+xAYJY+2FWel7SV9T1GTVsxAgMB\nAAEwDQYJKoZIhvcNAQEFBQADggEBAKkx/XdJ5b38XgbT25zEcAdU9u2HZt0r1QAt\n/iEgA+NvwYq5fsm61Z36jUPrG8ggT1mwwYozOKLf8XxqAcGObf6uNDAr6xhtdfrz\nzIg2zgu26+AB8xJH77kXlUx2kTAzkWx0a/Qa90w18eKJabwHNl//BtALCtGL/6jK\nj6AfVE43nvZypSaRGcQbQLezymrCQ+7aFYLZAY54WUFuVlhx6bqHkIP6g21P2Da7\nJh41CLILEkmapXKlcsocqbWiau/Scuaocn2V4HLQXCdFl6eLx7CsF13nnJ0qw29n\ndAlrE+Or7qJ1QGpOGBRJ0I715tauQgE89CmaieYT+sZ9XJXHGPo=\n-----END CERTIFICATE-----'


---------------------------- 6 March 2020 ------------------------------

- Configured the Prod AAT and Livelink for installation purpose.
- Checked on Evelio request ofr Build.
https://bamboo.di-infra.sap.corp/deploy/viewDeploymentResult.action?deploymentResultId=144311169

- Deployed the Livelink on Demo, check all the chats.

- Discussed issue with MMG team regarding integration of AAT and LLK. Initially they said there is an issue with deployment, later on went through all the logs.

- Checked on RMS.

- Had a call with MMG team.


------------------------ 7 March 2020 ----------------------------------------

- The CTOOL upgrade was cancelled, discussed same with Nidhi.

- VDI is down.

- Reviewing the disk usage, memory usage, number of open files.

- Analysed the spike in the traffic.
https://apm.cf.us10.hana.ondemand.com/e/f95fd477-ba09-47d0-ba91-d6f35a4e2f22/#problems/problemdetails;pid=-2772828015821514764_1586249760000V2

- Checking the status of each nodes.

https://jira.di-infra.sap.corp/browse/ISS-1894
----------------------------------------
us4ciapi01:

[skesarkar@us4ciapi01 ~]$ free -g
             total       used       free     shared    buffers     cached
Mem:          1009        350        659          0          2        129
-/+ buffers/cache:        218        791
Swap:           15          0         15
[skesarkar@us4ciapi01 ~]$ df -h
Filesystem            Size  Used Avail Use% Mounted on
/dev/mapper/systemvg-root
                      3.9G  929M  2.8G  26% /
tmpfs                 505G  520K  505G   1% /dev/shm
/dev/sda2             477M   84M  368M  19% /boot
/dev/sda1             200M  260K  200M   1% /boot/efi
/dev/mapper/systemvg-opt
                      241G   48G  181G  21% /opt
/dev/mapper/systemvg-tmp
                      4.8G  131M  4.5G   3% /tmp
/dev/mapper/systemvg-usr
                      3.9G  1.8G  2.0G  48% /usr
/dev/mapper/systemvg-var
                       15G  8.9G  4.9G  65% /var
/dev/mapper/systemvg-home
                      4.8G  2.2G  2.5G  47% /var/home
us4cimaprvip215:/mapr/us4maprcluster.bd.trust/cimapr/fs/pc/sftp/sap/
                       32G   27M   32G   1% /opt/jail/sap

[root@us4ciapi01 ~]# cat /proc/sys/fs/file-max
105243492
[root@us4ciapi01 ~]# lsof | wc -l | tail -n 1
18080

[skesarkar@us4ciapi01 ~]$ for i in `docker ps -q`; do echo $i;docker exec -it $i cat /proc/sys/fs/file-nr; done
0fd1210d8182
10240   0       105243492
910a357b424b
10240   0       105243492
2c6de956be4b
10240   0       105243492
a947a6d3a1cf
10240   0       105243492
822c642c5cba
10240   0       105243492
45770797a5dd
10240   0       105243492
4999742e21e1
10240   0       105243492
39dc6c7818f0
10240   0       105243492
363e8cca340e
10240   0       105243492
8a7cd984cec3
10240   0       105243492
9a4ef5a05ba5
10240   0       105243492
cdb88ea2c24b
10240   0       105243492
773eb4fc2929
10240   0       105243492
ecaaac8c6bb1
10240   0       105243492
c1f4b90f2074
10240   0       105243492
dca0a8cf7f1a
10240   0       105243492
9db13b3f5354
10240   0       105243492
fa94b5df115c
10240   0       105243492

----------------------------------------
us4ciapi03:

[skesarkar@us4ciapi03 ~]$ free -g
             total       used       free     shared    buffers     cached
Mem:          1009        399        610          0          2        227
-/+ buffers/cache:        169        840
Swap:           15          0         15
[skesarkar@us4ciapi03 ~]$ df -h
Filesystem            Size  Used Avail Use% Mounted on
/dev/mapper/systemvg-root
                      3.9G  442M  3.2G  12% /
tmpfs                 505G  628K  505G   1% /dev/shm
/dev/sda2             477M   57M  395M  13% /boot
/dev/sda1             200M  260K  200M   1% /boot/efi
/dev/mapper/systemvg-opt
                      241G  114G  115G  50% /opt
/dev/mapper/systemvg-tmp
                      4.8G   11M  4.6G   1% /tmp
/dev/mapper/systemvg-usr
                      3.9G  1.1G  2.6G  31% /usr
/dev/mapper/systemvg-var
                       15G  4.3G  9.5G  32% /var
/dev/mapper/systemvg-home
                      4.8G  730M  3.9G  16% /var/home

[skesarkar@us4ciapi03 ~]$ for i in `docker ps -q`; do echo $i;docker exec -it $i cat /proc/sys/fs/file-nr; done
7a4e5b8d1741
11520   0       105243499
32fe239efc96
11520   0       105243499
fc09982cc221
11520   0       105243499
c8f2cc134525
11520   0       105243499
3b1ef679b0e3
11520   0       105243499
925b8b4413c2
11520   0       105243499
b1440a1408da
11520   0       105243499
fdf829751f0d
11520   0       105243499
a6e16b98ed0f
11520   0       105243499
83e2c2f2df61
11520   0       105243499
4f98860adae9
11520   0       105243499
9046308ccd19
11520   0       105243499
568cf29c120e
11520   0       105243499
1862bec7c47d
11520   0       105243499
d954411df406
11520   0       105243499
f1833724f661
11520   0       105243499
0a2fd5d34778
11520   0       105243499

---------------------------------------
us4ciapi04:

[skesarkar@us4ciapi04 ~]$ free -g
             total       used       free     shared    buffers     cached
Mem:          1009        299        710          0          2        131
-/+ buffers/cache:        165        844
Swap:           15          0         15
[skesarkar@us4ciapi04 ~]$ df -h
Filesystem            Size  Used Avail Use% Mounted on
/dev/mapper/systemvg-root
                      3.9G  688M  3.0G  19% /
tmpfs                 505G  524K  505G   1% /dev/shm
/dev/sda2             477M   84M  368M  19% /boot
/dev/sda1             200M  260K  200M   1% /boot/efi
/dev/mapper/systemvg-opt
                      241G   89G  140G  39% /opt
/dev/mapper/systemvg-tmp
                      4.8G   11M  4.6G   1% /tmp
/dev/mapper/systemvg-usr
                      3.9G  1.4G  2.3G  38% /usr
/dev/mapper/systemvg-var
                       15G  3.3G   11G  24% /var
/dev/mapper/systemvg-home
                      4.8G  1.7G  2.9G  37% /var/home
us4cimaprvip216:/mapr/us4maprcluster.bd.trust/cimapr
                      100G   13G   88G  13% /mapr01
us4cimaprvip215:/mapr/us4maprcluster.bd.trust/cimapr/fs/pc/sftp/sap/
                       32G   27M   32G   1% /opt/jail/sap

[skesarkar@us4ciapi04 ~]$ for i in `docker ps -q`; do echo $i;docker exec -it $i cat /proc/sys/fs/file-nr; done
c3120ca72d05
10560   0       105243791
c6a24d481b32
10560   0       105243791
7e384145fa24
10560   0       105243791
701feed35ef1
10560   0       105243791
bbd4ecb6616f
10560   0       105243791
a744ea38ded3
10560   0       105243791
c6d724d2437a
10560   0       105243791
4756fd6fc1cf
10560   0       105243791
198c07917157
10560   0       105243791
9bacc75687d2
10560   0       105243791
739753e795c1
10560   0       105243791
be2a56dfb97a
10560   0       105243791
b203bd5a0bea
10560   0       105243791
b8f1af201eef
10560   0       105243791
0857f67e734d
10560   0       105243791
bdfbaf68b65c
10560   0       105243791
42604b3f12f1
10560   0       105243791


df: ‘/mnt’: Stale file handle

- SYSTEM_URL=https://aa8fd7ab-mmg-mmgingress-ac84-497892438.us-east-1.elb.amazonaws.com
SYSTEM_URL=https://localhost:16101/

aa8fd7ab-mmg-mmgingress-ac84-497892438.us-east-1.elb.amazonaws.com/ (edited) 

- Call with Abhilekh for analytics :

UPSTREAM_MESSAGES_MANAGEMENT_V2=llbpal53.pal.sap.corp:17200	<< dev

UPSTREAM_MESSAGES_API=demo-livelink.sapdigitalinterconnect.com:443
UPSTREAM_MESSAGES_MANAGEMENT_V2=demo-livelink.sapdigitalinterconnect.com:443


---

 upstream_messages_management_v2: demo-livelink.sapdigitalinterconnect.com:443
 UPSTREAM_MESSAGES_MANAGEMENT_V2: {{ .Values.ui.upstream_messages_management_v2 }}

---

Apr  7 18:04:16 us4cidemo002 docker/bc3ef2f4f8d9[8534]: 52.4.112.92 - - [07/Apr/2020:18:04:16 +0000] "GET /v2/measures/clients/644/daily?startDate=2020-04-01&endDate=2020-04-30 HTTP/1.1" 404 1482 "https://aa8fd7ab-mmg-mmgingress-ac84-497892438.us-east-1.elb.amazonaws.com/app/" "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:74.0) Gecko/20100101 Firefox/74.0"
Apr  7 18:04:16 us4cidemo002 docker/bc3ef2f4f8d9[8534]: 52.4.112.92 - - [07/Apr/2020:18:04:16 +0000] "GET /v2/sms/smsLog?clientId=644&sort=timestamp,desc&startUTCTime=202004071803&endUTCTime=202004071804&size=5&page=1 HTTP/1.1" 404 1482 "https://aa8fd7ab-mmg-mmgingress-ac84-497892438.us-east-1.elb.amazonaws.com/app/" "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:74.0) Gecko/20100101 Firefox/74.0"
Apr  7 18:04:16 us4cidemo002 docker/bc3ef2f4f8d9[8534]: 52.4.112.92 - - [07/Apr/2020:18:04:16 +0000] "GET /apps?sort=isPrimary,desc&sort=name,asc&search=client.id:644,active:true&size=7&page=0 HTTP/1.1" 200 1078 "https://aa8fd7ab-mmg-mmgingress-ac84-497892438.us-east-1.elb.amazonaws.com/app/" "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:74.0) Gecko/20100101 Firefox/74.0"
Apr  7 18:04:16 us4cidemo002 docker/bc3ef2f4f8d9[8534]: 52.4.112.92 - - [07/Apr/2020:18:04:16 +0000] "GET /v2/sms/smsLog?clientId=644&sort=timestamp,desc&startUTCTime=202004071803&endUTCTime=202004071804&size=5&page=1 HTTP/1.1" 404 1482 "https://aa8fd7ab-mmg-mmgingress-ac84-497892438.us-east-1.elb.amazonaws.com/app/" "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:74.0) Gecko/20100101 Firefox/74.0"
Apr  7 18:04:16 us4cidemo002 docker/bc3ef2f4f8d9[8534]: 52.4.112.92 - - [07/Apr/2020:18:04:16 +0000] "GET /v2/measures/clients/644/daily?startDate=2020-04-01&endDate=2020-04-30 HTTP/1.1" 404 1482 "https://aa8fd7ab-mmg-mmgingress-ac84-497892438.us-east-1.elb.amazonaws.com/app/" "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:74.0) Gecko/20100101 Firefox/74.0"
Apr  7 18:04:16 us4cidemo002 docker/bc3ef2f4f8d9[8534]: 52.4.112.92 - - [07/Apr/2020:18:04:16 +0000] "GET /apps?sort=isPrimary,desc&sort=name,asc&search=client.id:644,active:true&size=7&page=0 HTTP/1.1" 200 1078 "https://aa8fd7ab-mmg-mmgingress-ac84-497892438.us-east-1.elb.amazonaws.com/app/" "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:74.0) Gecko/20100101 Firefox/74.0"


aa8fd7ab-mmg-mmgingress-ac84-497892438


Guys, need one help. In livelink-frontend-reverse proxy I can't find the locations where it redirect to analytics service but in llk-portal-ui the locations have been properly defined. Because of which I can see the analytics being getting loaded in the livelink UI part.

----------------------

ui-portal : locations

/usr/local/openresty/nginx/conf # cat locations.conf | egrep -i "analytics | smslog"
        location /analytics {
        location /lle-analytics {


frontend-reverse-proxy:

-----------------------

skesarkar@us4cidemo002 $ docker ps | grep -i reverse
bc3ef2f4f8d9        dbs-docker-repository.docker.repositories.sapcdn.io/llk-frontend-reverse-proxy:2.0.2.6           "/bin/sh -c 'envsubs   32 hours ago        Up 32 hours         0.0.0.0:9999->9999/tcp, 443/tcp                                                              llk-frontend-reverse-proxy
b19acdbdd255        dbs-docker-repository.docker.repositories.sap.ondemand.com/mfa-frontend-reverse-proxy:1.9.4.15   "/bin/sh -c 'envsubs   5 months ago        Up 4 days           0.0.0.0:14100->14100/tcp                                                                     mfa-frontend-reverse-proxy
skesarkar@us4cidemo002 $
skesarkar@us4cidemo002 $
skesarkar@us4cidemo002 $ docker ps | grep -i analytics
f41304a04923        dbs-docker-repository.docker.repositories.sapcdn.io/llk-analytics:2.0.2.0                        "sh /opt/llk/service   4 days ago          Up 4 days           0.0.0.0:17200-17201->17200-17201/tcp    

------------------------

skesarkar@us4cidemo002 $ docker ps | grep -i reverse
bc3ef2f4f8d9        dbs-docker-repository.docker.repositories.sapcdn.io/llk-frontend-reverse-proxy:2.0.2.6           "/bin/sh -c 'envsubs   32 hours ago        Up 32 hours         0.0.0.0:9999->9999/tcp, 443/tcp                                                              llk-frontend-reverse-proxy
b19acdbdd255        dbs-docker-repository.docker.repositories.sap.ondemand.com/mfa-frontend-reverse-proxy:1.9.4.15   "/bin/sh -c 'envsubs   5 months ago        Up 4 days           0.0.0.0:14100->14100/tcp                                                                     mfa-frontend-reverse-proxy
skesarkar@us4cidemo002 $
skesarkar@us4cidemo002 $
skesarkar@us4cidemo002 $ docker ps | grep -i analytics
f41304a04923        dbs-docker-repository.docker.repositories.sapcdn.io/llk-analytics:2.0.2.0                        "sh /opt/llk/service   4 days ago          Up 4 days           0.0.0.0:17200-17201->17200-17201/tcp                                                         llk-analytics
skesarkar@us4cidemo002 $ docker exec -it bc3ef2f4f8d9 sh
/ # cd /usr/local/openresty/nginx/
/usr/local/openresty/nginx # ls
client_body_temp  fastcgi_temp      logs              proxy_temp        scgi_temp
conf              html              modules           sbin              uwsgi_temp
/usr/local/openresty/nginx # cd conf/
/usr/local/openresty/nginx/conf # ls
fastcgi.conf            fastcgi_params.default  mime.types              nginx.conf.default      uwsgi_params
fastcgi.conf.default    koi-utf                 mime.types.default      scgi_params             uwsgi_params.default
fastcgi_params          koi-win                 nginx.conf              scgi_params.default     win-utf
/usr/local/openresty/nginx/conf # vim nginx.conf
sh: vim: not found
/usr/local/openresty/nginx/conf # cat nginx.conf | grep -i 17200
/usr/local/openresty/nginx/conf # cat nginx.conf | grep -i 15200
      server us4cidemo002.bd.trust:15200 max_fails=3 fail_timeout=10s;
	  
-----------------------------

skesarkar@us4cidemo002 $ docker ps | grep -i reverse
bc3ef2f4f8d9        dbs-docker-repository.docker.repositories.sapcdn.io/llk-frontend-reverse-proxy:2.0.2.6           "/bin/sh -c 'envsubs   32 hours ago        Up 32 hours         0.0.0.0:9999->9999/tcp, 443/tcp
skesarkar@us4cidemo002 $ docker ps | grep -i analytics
f41304a04923        dbs-docker-repository.docker.repositories.sapcdn.io/llk-analytics:2.0.2.0                        "sh /opt/llk/service   4 days ago          Up 4 days           0.0.0.0:17200-17201->17200-17201/tcp                                                         llk-analytics

- Can't see a analytics upstream port being defined in the nginx.conf whereas for other services it is being defined.

skesarkar@us4cidemo002 $ docker exec -it bc3ef2f4f8d9 sh
/ # cd /usr/local/openresty/nginx/
/usr/local/openresty/nginx # cd conf/
/usr/local/openresty/nginx/conf # cat nginx.conf | grep -i 17200
/usr/local/openresty/nginx/conf # cat nginx.conf | grep -i 15200
      server us4cidemo002.bd.trust:15200 max_fails=3 fail_timeout=10s;
-----------------------------

        location /analytics {
            access_by_lua_file /usr/local/openresty/nginx/lua/access_by_lua_block.lua;
            proxy_pass http://livelink-messages-v2/v2/measures/clients;
                        proxy_next_upstream error timeout http_500;
        }
        location /clients {
            access_by_lua_file /usr/local/openresty/nginx/lua/access_by_lua_block.lua;
            proxy_set_header Host $http_host;
            proxy_pass http://livelink-accounts-management-v2/api/livelink/clients;
                        proxy_next_upstream error timeout http_500;
        }
        location /smsLog {
            access_by_lua_file /usr/local/openresty/nginx/lua/access_by_lua_block.lua;
            proxy_set_header Host $http_host;
            proxy_pass http://livelink-sms-eventstore/v2/sms/smsLog;
                        proxy_next_upstream error timeout http_500;
        }

---------

upstream livelink-messages-v2{
server us4cidemo002.bd.trust:17200 max_fails=3 fail_timeout=10s;
}
upstream livelink-sms-eventstore{
server us4cidemo002.bd.trust:15720 max_fails=3 fail_timeout=10s;
}


- Checked till late night with llk team regarding the analytics functionality.



------------------------ 8 March 2020 ----------------------------------------

- 19 -March-2020 : AWSDEV was deployed. Sanity testing needed to be started at this time which I believe didn't. As per the plan We should have deployed the code on AWS1PROD by 6-April but we are pending because of below issues.
  
  -------- As per production deployment ----------
  3-April-2020 : Done with configuring the AAT, LLK production. Deployed the AWS1PROD template on AWSDEV environment.
  6-April-2020 : Development team found that the code is not working properly with the AAT and LLK Production, as there were some hardcoded environment.
				 - Discussed SSL issue with the team, that was communicated almost 20 days back.
  7-April-2020 : No response from developers regarding AAT and LLK integration with the MMG code. Also at the end of the day Testing team found that Analytics is not working.
				 - Which shows that we are missing the initial Sanity testing here.
				 - Had a discussion with LLK team regarding what could be the issue.
  8-April-2020 : Made required changes to the UI proxy post which we are able to hit the analytics part. Requested MMG team to complete this sanity testing soon, as we were supposed to deploy the MMG on AWSPROD on 6-April-2020.
		- We are still missing with SSL integration and code changes to be done for LLK and AAT integration.
  ------------------------------------------------
  
- Had a call with Team regarding accessing the AWS console.

UPDATE mmg_db.dbo.user_profile
SET default_account_id=1
WHERE sso_id=''C5285574'; 

UPDATE mmg_db.dbo.user_profile
SET default_account_id=1
WHERE sso_id='C5285574'

- Had a callback with Prashasti.

Logan#2203


----------------------- 9 March 2020 --------------------------------------------

- Check chat :

openssl s_client -connect https://aa8fd7ab-aat-aatingress-837a-1461999413.us-east-1.elb.amazonaws.com

openssl s_client -connect https://aa8fd7ab-aat-aatingress-837a-1461999413.us-east-1.elb.amazonaws.com

- 

Hi SysEng,

We need ReadOnly access to Kumar, Nand (external - Project) <nand.kumar@sap.com> on our BD TRUST domain. Please add him to JIRA group on LDAP.


Hi SysEng,

We need ReadOnly access to Kumar, Nand (external - Project) <nand.kumar@sap.com> on our BD TRUST domain. Please add him to JIRA group on LDAP.

The PAR request is  100172845.

Thank you.

Regards,
Sachin. K.
DevOps.

- openssl s_client -connect https://aa8fd7ab-aat-aatingress-837a-1461999413.us-east-1.elb.amazonaws.com


keytool -import -alias llbpal /opt/appdata/java/jdk1.8.0_101/jre/lib/security/cacerts -file 

- Configured the Cloudwatch logs for Kubernetes.
	https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Container-Insights-setup-logs.html
	
	
- Readiness probe and liveness probe :

https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#define-a-tcp-liveness-probe

spec:
  containers:
  - name: goproxy
    image: k8s.gcr.io/goproxy:0.1
    ports:
    - containerPort: 8080
    readinessProbe:
      tcpSocket:
        port: 8080
      initialDelaySeconds: 5
      periodSeconds: 10
    livenessProbe:
      tcpSocket:
        port: 8080
      initialDelaySeconds: 15
      periodSeconds: 20

- backend : image: dbs-docker-repository.docker.repositories.sap.ondemand.com/second-gen-engagement:2.0.2.1

        ports:
        - containerPort: 16500
          name: be
          protocol: TCP
        readinessProbe:
          tcpSocket:
            port: 16500
          initialDelaySeconds: 10
          periodSeconds: 10
        livenessProbe:
          tcpSocket:
            port: 16500
          initialDelaySeconds: 15
          periodSeconds: 20
		  
		  
		  
---------------- 13 April 2020 ---------------------
- Worked on Bamboo server.
- Nidhi request, check chat.

- Had a call with ChenYu. Meeting.

- Worked on Bamboo resolution. restarting of Ansible server. Check Chat mmg_devops.
	https://bamboo.di-infra.sap.corp/deploy/viewDeploymentResult.action?deploymentResultId=146505730
- Giving access to Ravi.
	


- backend :  dbs-docker-repository.docker.repositories.sap.ondemand.com/second-gen-engagement:2.0.2.1
- sch :  image: dbs-docker-repository.docker.repositories.sap.ondemand.com/scheduler-service:2.0.2.22
- ui : image: dbs-docker-repository.docker.repositories.sap.ondemand.com/llk-portal-ui:2.0.2.15-SNAPSHOT

- Backend :
        ports:
        - containerPort: 16500
          name: be
          protocol: TCP
        readinessProbe:
          tcpSocket:
            port: 16500
          initialDelaySeconds: 10
          periodSeconds: 10
        livenessProbe:
          tcpSocket:
            port: 16500
          initialDelaySeconds: 15
          periodSeconds: 20

- Scheduler :
        ports:
        - containerPort: 16400
          name: sch-be
          protocol: TCP
        readinessProbe:
          tcpSocket:
            port: 16400
          initialDelaySeconds: 10
          periodSeconds: 10
        livenessProbe:
          tcpSocket:
            port: 16400
          initialDelaySeconds: 15
          periodSeconds: 20	  
		  
		  
- UI :
        ports:
        - containerPort: 15910
          name: mmg-ui
          protocol: TCP
        readinessProbe:
          tcpSocket:
            port: 15910
          initialDelaySeconds: 10
          periodSeconds: 10
        livenessProbe:
          tcpSocket:
            port: 15910
          initialDelaySeconds: 15
          periodSeconds: 20	  		  
		  
-------------------

- Initial account for Augusto or Sachin.
	- Augusto user profile copied to DB.
	- Default SMS account.
	
	
	
------------------------------- 14 Apr 2020 ---------------------------------
- Made changes from be to core for mmg-AWSDEV.


SGE_PORT=16500 to SGE_PORT=19100

Scheduler 16400 > 19200

UI Port changed port change 7443> 19910

- added all these new variables on Shikha request.

ssl_key_store: /opt/sch/services/conf/server.p12
ssl_key_store_type: PKCS12
ssl_key_alias: llbpal
ssl_key_store_password: abcd1234
max_schedule_duration: 24
callback_rabbit_key: scheduler_retryDLX
callback_rabbit_exchange: scheduler_retry_exchange
callback_queue: scheduler_retry_queue
livelink_path_sms: /api/v2/sms
sch_port: 19200
sch_scope: sch:core
peformance_enabled: true




  SSL_KEY_STORE: {{ .Values.scheduler.ssl_key_store  }}
  SSL_KEY_STORE_TYPE: {{ .Values.scheduler.ssl_key_store_type  }}
  SSL_KEY_ALIAS: {{ .Values.scheduler.ssl_key_alias  }}
  SSL_KEY_STORE_PASSWORD: {{ .Values.scheduler.ssl_key_store_password  }}
  MAX_SCHEDULE_DURATION: {{ .Values.scheduler.max_schedule_duration | quote }}
  CALLBACK_RABBIT_KEY: {{ .Values.scheduler.callback_rabbit_key  }}
  CALLBACK_RABBIT_EXCHANGE: {{ .Values.scheduler.callback_rabbit_exchange  }}
  CALLBACK_QUEUE: {{ .Values.scheduler.callback_queue  }}
  LIVELINK_PATH_SMS: {{ .Values.scheduler.livelink_path_sms  }}
  SCH_PORT: {{ .Values.scheduler.sch_port | quote }}
  SCH_SCOPE: {{ .Values.scheduler.sch_scope  }}
  PEFORMANCE_ENABLED: {{ .Values.scheduler.peformance_enabled | quote }}
  
- Made changes to the values.yaml.awsprd
  
- Check mmg-devops chat.


---------------------- 15 April 2020 -----------------------
- Made changes to the MMG template, performed couple of deployments.

- Checking with Vinay as to why DB on AWS6 is down.

-bash:606::sybase@ip-10-6-5-188:~: isql -Usa -SDTSILOB4 -w5000 -PAdh3r3nt
CT-LIBRARY error:
        ct_connect(): network packet layer: internal net library error: Net-Lib protocol driver call to connect two endpoints failed
-bash:607::sybase@ip-10-6-5-188:~:

----------------------- 15 April 2020 ----------------------------

- Made changes to templates, tried installing on other environment.


----------------------- 16 April 2020 -----------------------------

- Deployment failed issue resolved.

for Ravi : https://bamboo.di-infra.sap.corp/deploy/viewDeploymentVersion.action?versionId=148963411
for Noel : https://bamboo.di-infra.sap.corp/deploy/viewDeploymentVersion.action?versionId=148963410

- Made changes to values.yaml for final deployment to AWS1.

- Performed deployment of ingress on AWS1 but didn't got the address of the ingress controller.

- Installing DB on AWS6.

isql -Usa -SDTSILOB4 -w5000 -PAdh3r3nt -i mmg_db.sql -o mmg_db.out -e

-

I0416 11:52:32.683866       1 annotations.go:150] annotation Tags in o.GetKind() mmg/mmg-ingress: &{map[]}
E0416 11:52:53.754725       1 controller.go:217] kubebuilder/controller "msg"="Reconciler error" "error"="failed to build LoadBalancer configuration due to failed to resolve 2 qualified subnet for ALB. Subnets must contains these tags: 'kubernetes.io/cluster/eks-cluster-name': ['shared' or 'owned'] and 'kubernetes.io/role/elb': ['' or '1']. See https://kubernetes-sigs.github.io/aws-alb-ingress-controller/guide/controller/config/#subnet-auto-discovery for more details. Resolved qualified subnets: '[]'"  "controller"="alb-ingress-controller" "request"={"Namespace":"mmg","Name":"mmg-ingress"}

kubernetes.io/cluster/eks-cluster-name
kubernetes.io/role/elb : AWS1_ProdOps
    kubernetes.io/cluster/eks-cluster-name: aws1-eks-encrypted
    kubernetes.io/role/elb: AWS1_ProdOps


aws1-eks-encrypted

kubectl logs -f alb-ingress-controler-55c757cdd6-k8tc9 -n kube-system | grep -i llk


Hi  , I was creating a ALB ingress on our aws1-eks-encrypted cluster. But it was giving below issue and I found below logs in the ALB controller.

E0416 11:52:53.754725       1 controller.go:217] kubebuilder/controller "msg"="Reconciler error" "error"="failed to build LoadBalancer configuration due to failed to resolve 2 qualified subnet for ALB. Subnets must contains these tags: 'kubernetes.io/cluster/eks-cluster-name': ['shared' or 'owned'] and 'kubernetes.io/role/elb': ['' or '1']. See https://kubernetes-sigs.github.io/aws-alb-ingress-controller/guide/controller/config/#subnet-auto-discovery for more details. Resolved qualified subnets: '[]'"  "controller"="alb-ingress-controller" "request"={"Namespace":"mmg","Name":"mmg-ingress"}

same logs I found for the LLK controller as well.

E0416 11:53:40.716725       1 controller.go:217] kubebuilder/controller "msg"="Reconciler error" "error"="failed to build LoadBalancer configuration due to failed to resolve 2 qualified subnet for ALB. Subnets must contains these tags: 'kubernetes.io/cluster/eks-cluster-name': ['shared' or 'owned'] and 'kubernetes.io/role/elb': ['' or '1']. See https://kubernetes-sigs.github.io/aws-alb-ingress-controller/guide/controller/config/#subnet-auto-discovery for more details. Resolved qualified subnets: '[]'"  "controller"="alb-ingress-controller" "request"={"Namespace":"llk","Name":"llk-ingress"}

So me and @pablo found that the tags to the subnet aws1-dmz-public-1/2 were configured as "aws1-eks" where as our cluster name of the cluster is "aws1-eks-encrypted". So we changed the tag to correct one that is "aws1-eks-encrypted". But after chaning tag found that the ALB pod in the kube-system namespace went offline. There is no way to check the logs now. Can you please have a look at this.


10.1.5.183

--------------------- 17 April 2020 ----------------------------
- The deployment is done but installation of procedures and NetEng task is remaining.

- Facing issue while executing the task on Sybase server.

---------------------------
[psilva@aws1asedb001 ~]$ sudo su - sybase

We trust you have received the usual lecture from the local System
Administrator. It usually boils down to these three things:

    #1) Respect the privacy of others.
    #2) Think before you type.
    #3) With great power comes great responsibility.

[sudo] password for psilva:
psilva is not in the sudoers file.  This incident will be reported.
[psilva@aws1asedb001 ~]$ sudo su -
[sudo] password for psilva:
psilva is not in the sudoers file.  This incident will be reported.



isql -Ummg_db_admin -SPDSILOB101 -w5000 -Pw#pfzNP5ml


https://f4c63f91-mmg-mmgingress-ac84-1921277801.us-east-1.elb.amazonaws.com/aat/response


------------------------------ 20 April -----------------------------------------

kubectl create configmap cluster-info \
--from-literal=cluster.name=aws1-eks-encrypted \
--from-literal=logs.region=us-east-1 -n amazon-cloudwatch


- Getting error if loggroups.

2020-04-20 11:14:08 +0000 [warn]: #0 [out_cloudwatch_logs_systemd] failed to flush the buffer. retry_time=3 next_retry_seconds=2020-04-20 11:14:13 +0000 chunk="5a3b6fec064b99e3f9a4f91f436b8288" error_class=Aws::CloudWatchLogs::Errors::AccessDeniedException error="User: arn:aws:sts::808830264230:assumed-role/eksctl-aws1-eks-encrypted-nodegro-NodeInstanceRole-13A2NL6ZYN3IX/i-01d05b100e81b9c34 is not authorized to perform: logs:DescribeLogGroups on resource: arn:aws:logs:us-east-1:808830264230:log-group::log-stream:"
  2020-04-20 11:14:08 +0000 [warn]: #0 suppressed same stacktrace
2020-04-20 11:14:10 +0000 [warn]: #0 [out_cloudwatch_logs_host_logs] failed to flush the buffer. retry_time=4 next_retry_seconds=2020-04-20 11:14:18 +0000 chunk="5a3b6fea20c92919189876d2a261ff5e" error_class=Aws::CloudWatchLogs::Errors::AccessDeniedException error="User: arn:aws:sts::808830264230:assumed-role/eksctl-aws1-eks-encrypted-nodegro-NodeInstanceRole-13A2NL6ZYN3IX/i-01d05b100e81b9c34 is not authorized to perform: logs:DescribeLogGroups on resource: arn:aws:logs:us-east-1:808830264230:log-group::log-stream:"
  2020-04-20 11:14:10 +0000 [warn]: #0 suppressed same stacktrace
2020-04-20 11:14:10 +0000 [warn]: #0 [out_cloudwatch_logs_containers] failed to flush the buffer. retry_time=4 next_retry_seconds=2020-04-20 11:14:17 +0000 chunk="5a3b6feb50a28df49224c47f8606c706" error_class=Aws::CloudWatchLogs::Errors::AccessDeniedException error="User: arn:aws:sts::808830264230:assumed-role/eksctl-aws1-eks-encrypted-nodegro-NodeInstanceRole-13A2NL6ZYN3IX/i-01d05b100e81b9c34 is not authorized to perform: logs:DescribeLogGroups on resource: arn:aws:logs:us-east-1:808830264230:log-group::log-stream:"
  2020-04-20 11:14:10 +0000 [warn]: #0 suppressed same stacktrace
  
2020-04-20 11:14:14 +0000 [warn]: #0 [out_cloudwatch_logs_systemd] failed to flush the buffer. retry_time=4 next_retry_seconds=2020-04-20 11:14:22 +0000 chunk="5a3b6fec064b99e3f9a4f91f436b8288" error_class=Aws::CloudWatchLogs::Errors::AccessDeniedException error="User: arn:aws:sts::808830264230:assumed-role/eksctl-aws1-eks-encrypted-nodegro-NodeInstanceRole-13A2NL6ZYN3IX/i-01d05b100e81b9c34 is not authorized to perform: logs:DescribeLogGroups on resource: arn:aws:logs:us-east-1:808830264230:log-group::log-stream:"


  2020-04-20 11:14:14 +0000 [warn]: #0 suppressed same stacktrace
2020-04-20 11:14:18 +0000 [warn]: #0 [out_cloudwatch_logs_containers] failed to flush the buffer. retry_time=5 next_retry_seconds=2020-04-20 11:14:34 +0000 chunk="5a3b6feb50a28df49224c47f8606c706" error_class=Aws::CloudWatchLogs::Errors::AccessDeniedException error="User: arn:aws:sts::808830264230:assumed-role/eksctl-aws1-eks-encrypted-nodegro-NodeInstanceRole-13A2NL6ZYN3IX/i-01d05b100e81b9c34 is not authorized to perform: logs:DescribeLogGroups on resource: arn:aws:logs:us-east-1:808830264230:log-group::log-stream:"



2020-04-20 11:53:59 +0000 [warn]: #0 [out_cloudwatch_logs_host_logs] failed to flush the buffer. retry_time=12 next_retry_seconds=2020-04-20 12:24:02 +0000 chunk="5a3b7154563af68f00fb90a5b7423feb" error_class=Aws::CloudWatchLogs::Errors::AccessDeniedException error="User: arn:aws:sts::808830264230:assumed-role/eksctl-aws1-eks-encrypted-nodegro-NodeInstanceRole-13A2NL6ZYN3IX/i-01e9a349efbb0ed1c is not authorized to perform: logs:DescribeLogGroups on resource: arn:aws:logs:us-east-1:808830264230:log-group::log-stream:"


- Need to increase the replica.

- Dynatrace agent.

- Public URL.

- SysEng ReadOnly.

- Configured the CloudWatch logs.



-------------------------------------------------- 21 April 2020 ----------------------------------------------------

  upstream_sge: https://mmg-core-service.int.cloud.sap:19100
  scheduler_api: https://mmg-sch-service.int.cloud.sap:19200/api/v1


-- helm upgrade is not working.

[skesarkar@ip-10-1-12-98 mmg_helm]$ helm upgrade -f ./mmg_chart/values.yaml mmg mmg_chart
Release "mmg" has been upgraded. Happy Helming!
NAME: mmg
LAST DEPLOYED: Tue Apr 21 06:00:13 2020
NAMESPACE: mmg
STATUS: deployed
REVISION: 20
TEST SUITE: None
NOTES:
Application mmg_chart deployed successful.
Please check the functional Pod using 'kubectl get pods' command.

Thanks for using Helm templating for AAT, in case of doubts/problems, please contact the SAPDI DevOps team through our DL: DL_59BADC0B5F99B7EEC4000406@global.corp.sap (DL SAP PE Digital Interconnect Industry Services DevOps)

Have a nice day.


####################################################################


[skesarkar@ip-10-1-12-98 mmg_chart]$ cd ..
[skesarkar@ip-10-1-12-98 mmg_helm]$ helm install mmg mmg_chart/
NAME: mmg
LAST DEPLOYED: Tue Apr 21 06:11:06 2020
NAMESPACE: mmg
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
Application mmg_chart deployed successful.
Please check the functional Pod using 'kubectl get pods' command.

Thanks for using Helm templating for AAT, in case of doubts/problems, please contact the SAPDI DevOps team through our DL: DL_59BADC0B5F99B7EEC4000406@global.corp.sap (DL SAP PE Digital Interconnect Industry Services DevOps)

Have a nice day.


####################################################################
####################################################################


[skesarkar@ip-10-1-12-98 mmg_helm]$ kubectl exec -it dnsutils -- nslookup mmg-sch-service.mmg.int.cloud.sap
Server:         172.20.0.10
Address:        172.20.0.10#53

Name:   mmg-sch-service.mmg.svc.cluster.local
Address: 172.20.167.213

[skesarkar@ip-10-1-12-98 mmg_helm]$ kubectl exec -it dnsutils -- nslookup mmg-sch-service.mmg
Server:         172.20.0.10
Address:        172.20.0.10#53

Name:   mmg-sch-service.mmg.svc.cluster.local
Address: 172.20.167.213

[skesarkar@ip-10-1-12-98 mmg_helm]$ kubectl exec -it dnsutils -- nslookup mmg-sch-service.mmg.svc.cluster.local
Server:         172.20.0.10
Address:        172.20.0.10#53

Name:   mmg-sch-service.mmg.svc.cluster.local
Address: 172.20.167.213

[skesarkar@ip-10-1-12-98 mmg_helm]$ kubectl exec -it dnsutils -- nslookup mmg-sch-service.int.cloud.sap
Server:         172.20.0.10
Address:        172.20.0.10#53

** server can't find mmg-sch-service.int.cloud.sap: NXDOMAIN

command terminated with exit code 1



---------------------- 22 April 2020 -----------------------
- Tried to configure the Widget on AWS1 environment.

- Performed the deployment of MMG using new modified values.

HTTP 5XX, 4XX
Region: us-east-1•ApplicationELB•HTTPCode_ELB_5XX_Count•LoadBalanc

EKS :

parse @message "* * * * * [*] * *" as c1, c2, c3, c4, c5, c6, exceptionName, exceptionMessage
| filter @message like /ERROR/
| filter @message not like /Subscriber opted-out so message not sent, request: synapta.messaging./
| stats count() as exceptionCount by exceptionMessage
| sort exceptionCount desc
| display exceptionCount,exceptionMessage


filter action="REJECT"
| stats count(*) as numRejections by srcAddr,bin(5m) as period
| sort numRejections desc
| limit 20
| display period, numRejections


fields kubernetes.container_name, @message
| filter @message like /Exception/ and @message not like /WARN/
| sort kubernetes.container_name asc
| limit 20



10.1.7.204, 10.1.8.172

aws1-eks-nodes

- Tried installing Solarwinds agent on AWS1 EKS nodes.

- Installing Solarwinds on AWS1-EKS nodes.

https://172.24.227.23/#/jobs/playbook/125699?job_search=page_size:20;order_by:-finished;not__launch_type:sync

{
    "msg": "SSH Error: data could not be sent to remote host \"10.1.7.204\". Make sure this host can be reached over ssh",
    "unreachable": true,
    "changed": false
}


- Installing Dynatrace on AWS1-EKS nodes.



---------------------------------- 22 April 2020 -----------------------------------

- Helped purge RabbitMQ queue for AWS1 MMG groupId='e4e73e1a-4510-49aa-af04-58d2f0a3d064'.

- Raised request on behalf of Cheyu, Pankaj.

- Performed deployment of latest image on AWSDEV.

- Continue working on Cloudwatch widget.

parse @message "* * * * * [*] * *" as c1, c2, c3, c4, c5, c6, exceptionName, exceptionMessage
| filter @message like /ERROR/
| filter @message not like /Subscriber opted-out so message not sent, request: synapta.messaging./
| stats count() as exceptionCount by exceptionMessage
| sort exceptionCount desc
| display exceptionCount,exceptionMessage


filter action="REJECT"
| stats count(*) as numRejections by srcAddr,bin(5m) as period
| sort numRejections desc
| limit 20
| display period, numRejections


fields kubernetes.container_name, @message
| filter @message like /Exception/ and @message not like /WARN/
| sort kubernetes.container_name asc
| limit 20

| filter kubernetes.container_name like /mmg-sch/



;39m \u001b[36mc.sap.di.service.MessageConsumerService \u001b[0;39m \u001b[2m:\u001b[0;39m Response from SMS Client:::{\"livelinkOrder


fields @timestamp, @message
| filter kubernetes.container_name like /mmg-sch/
| fields strcontains(@message, "Response from SMS Client")
| sort @timestamp desc
| limit 20


- Updated with new configuration.


####################################################################
####################################################################
[skesarkar@ip-10-1-12-98 mmg_helm]$ helm list
NAME    NAMESPACE       REVISION        UPDATED                                 STATUS          CHART           APP VERSION
mmg     mmg             3               2020-04-22 18:43:32.248455377 +0000 UTC deployed        mmg_chart-0.1.0 1.0


[skesarkar@ip-10-1-12-98 mmg_helm]$ kubectl exec -it mmg-core-deployment-8579d7b4f4-52kcs -n mmg env | grep -i callback
CALLBACK_URL=https://messagemanager.sapdigitalinterconnect.com/llk_ack/{{message-id}}
[skesarkar@ip-10-1-12-98 mmg_helm]$ kubectl exec -it mmg-core-deployment-8579d7b4f4-52kcs -n mmg env | grep -i scheduler_api
SCHEDULER_API=https://mmg-sch-service.mmg.int.cloud.sap:19200/api/v1
[skesarkar@ip-10-1-12-98 mmg_helm]$

[skesarkar@ip-10-1-12-98 mmg_helm]$ kubectl exec -it mmg-core-deployment-8579d7b4f4-tqnk4 -n mmg env | grep -i scheduler_api
SCHEDULER_API=https://mmg-sch-service.mmg.int.cloud.sap:19200/api/v1
[skesarkar@ip-10-1-12-98 mmg_helm]$
[skesarkar@ip-10-1-12-98 mmg_helm]$ kubectl exec -it mmg-core-deployment-8579d7b4f4-tqnk4 -n mmg env | grep -i callback
CALLBACK_URL=https://messagemanager.sapdigitalinterconnect.com/llk_ack/{{message-id}}



Node 1 Internal IP : 10.1.8.172, 10.1.8.35
Node 2 Internal IP : 10.1.7.204, 10.1.7.7





-------------------------- 23 April 2020 -----------------------------

- Deployed RMS subscriber list manager with older revision #9, #9 was having revision f9b8750565c128fc72d528405fb142f98467687d
	-  #23	Custom build by SACHIN.KESARKAR@SAP.COM with revision f9b8750565c128fc72d528405fb142f98467687d
	- performed the deployment of the same to UK4 environment.
		https://bamboo.di-infra.sap.corp/deploy/viewDeploymentResult.action?deploymentResultId=150602138
	- so version v2.0.2.46(which was #9 earlier) is now deployed on UK4

- v2.0.2.44 again redeployed on DMO, although the auto trigger was disabled by me but that was done at last moment till that time auto trigger had already started working.
	- So I did the redeployment of v2.0.2.44 again on DMO, https://bamboo.di-infra.sap.corp/deploy/viewDeploymentResult.action?deploymentResultId=150602141
	- Enabled the auto trigger again.
	
	
- Changed the system_url for AWSPRD deployment.

[skesarkar@ip-10-1-12-98 mmg_helm]$ kubectl exec -it mmg-ui-deployment-5f46bdb657-vrmd9 -n mmg env | grep -i system
SYSTEM_URL=https://messagemanager.sapdigitalinterconnect.com

[skesarkar@ip-10-1-12-98 mmg_helm]$ kubectl exec -it mmg-ui-deployment-5f46bdb657-bj88c -n mmg env | grep -i system
SYSTEM_URL=https://messagemanager.sapdigitalinterconnect.com

- 1 hour call with Khadid


-------------------------------- 24 April 2020 ---------------------------------

- Wasted time on https://jira.di-infra.sap.corp/browse/AMK-174
	- Augusto had performed PCR which was not informed to me.

 aat_db
 arc_db
 ele_db
 event_db
 kwm_db
 llk_db
 mfa_db
 mmg_db
 ppc_db
 rms_db
 sch_db
 slm_db
 sms_db 
 
 
2020-04-24 06:27:29.817 DEBUG 7 --- [onnection adder] com.zaxxer.hikari.pool.HikariPool        : HikariPool-1 - Cannot acquire connection from data source

java.sql.SQLException: JZ006: Caught IOException: java.net.ConnectException: Connection refused (Connection refused)
        at com.sybase.jdbc4.jdbc.SybConnection.getAllExceptions(SybConnection.java:2526)
        at com.sybase.jdbc4.jdbc.SybConnection.handleSQLE(SybConnection.java:2394)
        at com.sybase.jdbc4.jdbc.SybConnection.tryLogin(SybConnection.java:474)
        at com.sybase.jdbc4.jdbc.SybConnection.handleHAFailover(SybConnection.java:2760)
        at com.sybase.jdbc4.jdbc.SybConnection.<init>(SybConnection.java:323)
        at com.sybase.jdbc4.jdbc.SybConnection.<init>(SybConnection.java:238)
        at com.sybase.jdbc4.jdbc.SybDriver.connect(SybDriver.java:211)
        at com.zaxxer.hikari.util.DriverDataSource.getConnection(DriverDataSource.java:117)
        at com.zaxxer.hikari.util.DriverDataSource.getConnection(DriverDataSource.java:123)
        at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:365)
        at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:194)
        at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:460)
        at com.zaxxer.hikari.pool.HikariPool.access$100(HikariPool.java:71)
        at com.zaxxer.hikari.pool.HikariPool$PoolEntryCreator.call(HikariPool.java:699)
        at com.zaxxer.hikari.pool.HikariPool$PoolEntryCreator.call(HikariPool.java:685)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

we have to connect to 102 node.

10.1.6.116 5002		-- connect

10.1.5.183 


isql -Usch_db_admin -S10.1.5.183:5002 -w5000 -PFZEPPnePZX

isql -Ummg_db_admin -S10.1.5.183:5002 -w5000 -Pw#pfzNP5ml



--------------------------------------- 27 April 2020 -----------------------------------

Nainapillai Shaikdawood, Mohamed Iqbal <mohamed.iqbal.nainapillai.shaikdawood@sap.com>

As we have encountered a couple of similar cases which need to find out the notification Id from LiveLink response, and then raise the request to query the notification email delivery status,
Could you please kindly help to confirm if there is any public API provided by LiveLink/Email service for the notification status query?

- Enabled performance for mmg.

[skesarkar@ip-10-1-12-98 mmg_helm]$ kubectl exec -it mmg-sch-deployment-d9bc59697-fkjhv -n mmg env | grep -i performance
PERFORMANCE_ENABLED=true

- Resolved the issue of by restarting the containers.

"org.springframework.web.client.ResourceAccessException: I/O error on POST request for \"https://login.sapdigitalinterconnect.com/oauth/token\": login.sapdigitalinterconnect.com; nested exception is java.net.UnknownHostException: login.sapdigitalinterconnect.com\n\tat org.springframework.web.client.RestTemplate.doExecute(RestTemplate.java:696)\n\tat org.springframework.web.client.RestTemplate.execute(RestTemplate.java:644)\n\tat org.springframework.web.client.RestTemplate.exchange(RestTemplate.java:564)\n\tat com.sap.smcs.secondgenengagement.service.impl.Authenti


- Resolving the Dipti issue.

https://bamboo.di-infra.sap.corp/deploy/viewDeploymentResult.action?deploymentResultId=151978003

- Configured widget for following instances

i-0c537ebc20ea4f3c8 -

i-08e6f249b1fc1e7c5
i-00b96bfae8099ba99 - 
i-09d0c943f4f5d55bf
i-066e492559b391681
i-075900716c2db8c3a
i-0bbd433096da4759a


[pablo@ip-198-19-11-212 ~]$ kubectl get ing -n mmg
NAME                   HOSTS   ADDRESS                                                                          PORTS   AGE
mmg-ingress            *       aa8fd7ab-mmg-mmgingress-ac84-497892438.us-east-1.elb.amazonaws.com                80      45d



https://login.sapdigitalinterconnect.com/oauth/token



2020-04-27 14:43:17.376  INFO 7 --- [  XNIO-2 task-6] c.s.s.s.s.i.AuthenticationServiceImpl    : Getting token : App=YDDmaJ11ZStXHfNR Secret=NqbwFbSy0eF0456kok2O1ajpd9
ZuTbo5 Basic=Basic WUREbWFKMTFaU3RYSGZOUjpOcWJ3RmJTeTBlRjA0NTZrb2syTzFhanBkOVp1VGJvNQ== TokenUrl=https://login.sapdigitalinterconnect.com/oauth/token
2020-04-27 14:43:17.526  INFO 7 --- [  XNIO-2 task-6] c.s.s.s.s.i.AuthenticationServiceImpl    : Exception while generating token: {}

org.springframework.web.client.ResourceAccessException: I/O error on POST request for "https://login.sapdigitalinterconnect.com/oauth/token": login.sapdigitalinterconn
ect.com; nested exception is java.net.UnknownHostException: login.sapdigitalinterconnect.com
        at org.springframework.web.client.RestTemplate.doExecute(RestTemplate.java:696)
        at org.springframework.web.client.RestTemplate.execute(RestTemplate.java:644)
        at org.springframework.web.client.RestTemplate.exchange(RestTemplate.java:564)
        at com.sap.smcs.secondgenengagement.service.impl.AuthenticationServiceImpl.getToken(AuthenticationServiceImpl.java:53)
        at com.sap.smcs.secondgenengagement.service.impl.AuthenticationServiceImpl$$FastClassBySpringCGLIB$$b2e0a6a6.invoke(<generated>)


/ $ nslookup login.sapdigitalinterconnect.com
nslookup: can't resolve '(null)': Name does not resolve

Name:      login.sapdigitalinterconnect.com
Address 1: 74.117.14.240

-----------------
/ $ for i in {1..20}; do
> nslookup login.sapdigitalinterconnect.com
> done
nslookup: can't resolve '(null)': Name does not resolve

nslookup: can't resolve 'login.sapdigitalinterconnect.com': Name does not resolve



for i in {1..20}; do  nslookup login.sapdigitalinterconnect.com; sleep 2;  done

for i in {1..20}; do  nslookup login.sapdigitalinterconnect.com; done

---------------------

found the issue.

/ $ nslookup login.sapdigitalinterconnect.com
nslookup: can't resolve '(null)': Name does not resolve

nslookup: can't resolve 'login.sapdigitalinterconnect.com': Name does not resolve
/ $
/ $ nslookup livelink.sapmobileservices.com
nslookup: can't resolve '(null)': Name does not resolve

Name:      livelink.sapmobileservices.com
Address 1: 74.117.14.245



------------------------------------------


[skesarkar@ip-10-1-12-98 ~]$ kubectl exec -it dnsutils sh -n mmg
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl kubectl exec [POD] -- [COMMAND] instead.
/ # nslookup login.sapdigitalinterconnect.com
Server:         172.20.0.10
Address:        172.20.0.10#53

Non-authoritative answer:
Name:   login.sapdigitalinterconnect.com
Address: 74.117.14.240

/ # nslookup login.sapdigitalinterconnect.com
Server:         172.20.0.10
Address:        172.20.0.10#53

Non-authoritative answer:
Name:   login.sapdigitalinterconnect.com
Address: 74.117.14.240
** server can't find login.sapdigitalinterconnect.com: NXDOMAIN				<<<<<<<<<<<<<<<<<<<<<<<<

/ # nslookup login.sapdigitalinterconnect.com
Server:         172.20.0.10
Address:        172.20.0.10#53

** server can't find login.sapdigitalinterconnect.com: NXDOMAIN				<<<<<<<<<<<<<<<<<<<<<<<<
---------
/ # nslookup livelink.sapmobileservices.com
Server:         172.20.0.10
Address:        172.20.0.10#53

Non-authoritative answer:
Name:   livelink.sapmobileservices.com
Address: 74.117.14.245

/ # nslookup livelink.sapmobileservices.com
Server:         172.20.0.10
Address:        172.20.0.10#53

Non-authoritative answer:
Name:   livelink.sapmobileservices.com
Address: 74.117.14.245
---------
/ # nslookup livelink.sapmobileservices.com
Server:         172.20.0.10
Address:        172.20.0.10#53

Name:   livelink.sapmobileservices.com
Address: 74.117.14.245

/ # nslookup login.sapdigitalinterconnect.com
Server:         172.20.0.10
Address:        172.20.0.10#53

** server can't find login.sapdigitalinterconnect.com: NXDOMAIN				<<<<<<<<<<<<<<<<<<<<<<<<

/ # nslookup login.sapdigitalinterconnect.com
Server:         172.20.0.10
Address:        172.20.0.10#53

Non-authoritative answer:
Name:   login.sapdigitalinterconnect.com
Address: 74.117.14.240

/ # nslookup login.sapdigitalinterconnect.com
Server:         172.20.0.10
Address:        172.20.0.10#53

** server can't find login.sapdigitalinterconnect.com: NXDOMAIN				<<<<<<<<<<<<<<<<<<<<<<<<


--------------------------------------- 28 April 2020 -------------------------------------------------------

- Resolving the deployment reported by Pablo.
	https://bamboo.di-infra.sap.corp/deploy/viewDeploymentResult.action?deploymentResultId=151978104
	
- Worked on resolving the AAAA record of the EKS cluster.

/ # tcpdump -n port 53
tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes
05:54:08.790070 IP 10.1.8.164.46533 > 172.20.0.10.53: 33059+ A? login.sapdigitalinterconnect.com.mmg.svc.cluster.local. (72)
05:54:08.790560 IP 172.20.0.10.53 > 10.1.8.164.46533: 33059 NXDomain*- 0/1/0 (165)
05:54:08.791117 IP 10.1.8.164.47438 > 172.20.0.10.53: 18037+ A? login.sapdigitalinterconnect.com.svc.cluster.local. (68)
05:54:08.791951 IP 172.20.0.10.53 > 10.1.8.164.47438: 18037 NXDomain*- 0/1/0 (161)
05:54:08.792163 IP 10.1.8.164.34552 > 172.20.0.10.53: 89+ A? login.sapdigitalinterconnect.com.cluster.local. (64)
05:54:08.792978 IP 172.20.0.10.53 > 10.1.8.164.34552: 89 NXDomain*- 0/1/0 (157)
05:54:08.793230 IP 10.1.8.164.43482 > 172.20.0.10.53: 2265+ A? login.sapdigitalinterconnect.com.bd.trust. (59)
05:54:08.794447 IP 172.20.0.10.53 > 10.1.8.164.43482: 2265 NXDomain* 0/1/0 (145)
05:54:08.794842 IP 10.1.8.164.45404 > 172.20.0.10.53: 29122+ A? login.sapdigitalinterconnect.com.ec2.internal. (63)
05:54:08.797907 IP 172.20.0.10.53 > 10.1.8.164.45404: 29122 NXDomain 0/0/0 (63)
05:54:08.798286 IP 10.1.8.164.46131 > 172.20.0.10.53: 36116+ A? login.sapdigitalinterconnect.com.bd.trustec2.internal. (71)
05:54:10.802380 IP 172.20.0.10.53 > 10.1.8.164.46131: 36116 NXDomain 0/1/0 (146)
05:54:10.802932 IP 10.1.8.164.53707 > 172.20.0.10.53: 47417+ A? login.sapdigitalinterconnect.com. (50)
05:54:10.804302 IP 172.20.0.10.53 > 10.1.8.164.53707: 47417 1/0/0 A 74.117.14.240 (98)
05:54:10.805056 IP 10.1.8.164.56593 > 172.20.0.10.53: 46973+ AAAA? login.sapdigitalinterconnect.com. (50)
05:54:10.808348 IP 172.20.0.10.53 > 10.1.8.164.56593: 46973 NXDomain 0/1/0 (163)
05:56:20.908541 IP 10.1.8.164.57422 > 172.20.0.10.53: 62443+ A? livelink.sapmobileservices.com.mmg.svc.cluster.local. (70)
05:56:20.909748 IP 172.20.0.10.53 > 10.1.8.164.57422: 62443 NXDomain*- 0/1/0 (163)
05:56:20.910128 IP 10.1.8.164.32790 > 172.20.0.10.53: 50886+ A? livelink.sapmobileservices.com.svc.cluster.local. (66)
05:56:20.911060 IP 172.20.0.10.53 > 10.1.8.164.32790: 50886 NXDomain*- 0/1/0 (159)
05:56:20.911285 IP 10.1.8.164.58316 > 172.20.0.10.53: 51122+ A? livelink.sapmobileservices.com.cluster.local. (62)
05:56:20.911750 IP 172.20.0.10.53 > 10.1.8.164.58316: 51122 NXDomain*- 0/1/0 (155)
05:56:20.912095 IP 10.1.8.164.40227 > 172.20.0.10.53: 47950+ A? livelink.sapmobileservices.com.bd.trust. (57)
05:56:20.912911 IP 172.20.0.10.53 > 10.1.8.164.40227: 47950 NXDomain* 0/1/0 (143)
05:56:20.913401 IP 10.1.8.164.55832 > 172.20.0.10.53: 10674+ A? livelink.sapmobileservices.com.ec2.internal. (61)
05:56:20.920442 IP 172.20.0.10.53 > 10.1.8.164.55832: 10674 NXDomain 0/0/0 (61)
05:56:20.920891 IP 10.1.8.164.43787 > 172.20.0.10.53: 55925+ A? livelink.sapmobileservices.com.bd.trustec2.internal. (69)
05:56:20.926506 IP 172.20.0.10.53 > 10.1.8.164.43787: 55925 NXDomain 0/1/0 (144)
05:56:20.926788 IP 10.1.8.164.34418 > 172.20.0.10.53: 28384+ A? livelink.sapmobileservices.com. (48)
05:56:20.942033 IP 172.20.0.10.53 > 10.1.8.164.34418: 28384 1/0/0 A 74.117.14.245 (94)
05:56:20.942504 IP 10.1.8.164.38360 > 172.20.0.10.53: 15889+ AAAA? livelink.sapmobileservices.com. (48)
05:56:20.973595 IP 172.20.0.10.53 > 10.1.8.164.38360: 15889 0/1/0 (137)

---------------------- Analyse high response time for llk-frontend-reverse-proxy ----------------------

e04bd2283f03        dbs-docker-repository.docker.repositories.sapcdn.io/llk-frontend-reverse-proxy:2.0.2.8            "/bin/sh -c 'envsubs   2 weeks ago         Up 2 weeks          443/tcp, 0.0.0.0:9999->9999/tcp                  llk-frontend-reverse-proxy


1 Hour call with Pablo/Ravi.
1 Hour call with Khalid.

I would suggest 2 solutions over here.

1) Scheduler gets failed response code -> Scheduler will make a direct call to Core service but on a different API endpoint(you should be able to use the same endpoint as well with extra metadata like to notify success/failure). In which case you will have to make change in both Core and Scheduler service logic.
2) Scheduler makes a call to proxy internally(here you might need to add another endpoint in proxy) -> which inturn call to Core service on the same endpoint llk_callback.

Like you can make a call as https://mmg-ui.mmg.int.cloud.sap:19100/api_endpoint




-------------------- 29 April 2020 -----------------------------

Kumar, Ravi (external - Service) <ravi.kumar18@sap.com>


handling mmg-ui requests to create, manipulate and run campaigns

- Block the user based on MAC address.
- 


------------------- 30 April 2020 -------------------------------

nohup java -Dspring.config.additional-location=/opt/aatadm/aat_SpringBoot/conf/environment.properties -Dlog.execution-time -jar  /opt/aatadm/aat_SpringBoot/authentication-authority-api.jar &
aatadm@us4cidemo002 $ ps aux | grep -i aat
1000     18310  2.2  2.2 27494272 2258036 ?    Sl   Apr29  13:33 java -jar /opt/aatadm/authentication-authority-api.jar
root     31355  0.0  0.0 204124  3076 pts/13   S    06:38   0:00 sudo su - aatadm
root     31363  0.0  0.0 161508  1764 pts/13   S    06:38   0:00 su - aatadm
aatadm   31364  0.0  0.0 108356  1816 pts/13   S    06:38   0:00 -bash
aatadm   32175  0.0  0.0 122700  1284 pts/13   R+   06:40   0:00 ps aux
aatadm   32176  0.0  0.0 103328   916 pts/13   S+   06:40   0:00 grep -i aat
aatadm@us4cidemo002 $
aatadm@us4cidemo002 $ ls -l /opt/aatadm/authentication-authority-api.jar
ls: cannot access /opt/aatadm/authentication-authority-api.jar: No such file or directory

- Cleaning the llbpal53

docker rm

docker rm cae4bebf8c97
docker rm e99acef178cf
docker rm 63a055bbcde1
docker rm 2451934dcddf
docker rm c917ff9b0623
docker rm 2eb4453e5e67
docker rm 5118e8195232
docker rm 74e2704b8aeb
docker rm 678952f25546
docker rm 52a6f3656e25
docker rm 7e90b211bc68
docker rm 58f53164749e
docker rm 203f65011fb9
docker rm 3dd21e65d6ae
docker rm 82ae20e5eb86
docker rm 6c44d21dc5b2
docker rm 9856985257a4
docker rm 2900998f536b
docker rm 19cd8f18caee
docker rm 5c31a1806cad
docker rm 71831208a4cc
docker rm 71222aa5bb85
docker rm b176aea69df8
docker rm 5a1f8ab9ec6a
docker rm 61a273be057c

du -shx * | sort -k1 -n | egrep "M|G"

@daily /usr/bin/docker ps -q | xargs -Icont docker exec cont sh -c "find /var/log/llk/ -mtime +1 -type f -delete 2>/dev/null"

- Topics to discuss :
	1. Walk through of K8 deployment via script
	2. Where did we setup the DNS of K8
	3. How to configure the Dynatrace dashboard, just a walkthrough so that I can create it for rest. Just like we did it for CloudWatch for MMG.
		- Dynatrace
	4. Regarding WAN
	
	
parse @message "* * * * * [*] * *" as c1, c2, c3, c4, c5, c6, exceptionName, exceptionMessage
| filter @message like /ERROR/
| stats count() as exceptionCount by exceptionMessage
| sort exceptionCount desc
| display exceptionCount,exceptionMessage	
	
	
	
	
	
---------------------------------- 4 May 2020 ---------------------------------------

- Analysed all the mail related to issue happened on 2 May.

- Resolving the issue of Build,
		https://bamboo.di-infra.sap.corp/deploy/viewDeploymentResult.action?deploymentResultId=153845897
		
				Sachin Kesarkar  11:47 AM
				@here Any idea who made this change in the /etc/hosts of ansible tower server ?
				
				
				
				
				
				11:47
				[root@iad1ansible001 ~]# cat /etc/hosts | grep -i github
				#10.48.160.210 github.wdf.sap.corp
				10.67.76.21 github.wdf.sap.corp
				11:47
				I was getting this error because of the above configuration.
				11:48
				project_update failed for SAP Github with ID 127580
				11:48
				It took good amount of time for me to figure this out .... and making the below change it worked.
				11:48
				[root@iad1ansible001 ~]# cat /etc/hosts | grep -i github
				10.48.160.210 github.wdf.sap.corp
				#10.67.76.21 github.wdf.sap.corp
	
- Setting the profile in Bamboo server.

        <profile>
            <id>sclprofile</id>
            <repositories>
                <repository>
                    <id>snapshots.scl</id>
                    <url>http://172.24.227.60:8081/nexus/repository/snapshots/</url>
                </repository>
                <repository>
                    <id>internal.releases.scl</id>
                    <url>http://172.24.227.60:8081/nexus/repository/releases/</url>
                </repository>
            </repositories>
        </profile>

	clean deploy -P sclprofile -Dcvstimestamp=${bamboo.version}
	
	-Dmaven.repo.remote=http://172.24.227.60:8081/nexus/repository/releases/  ${bamboo.MAVEN_GOALS}
	
- Open BDTrust for MMG users

Desmond Chong		Singapore		desmond.chong@sap.com		- done	Chong, Desmond <desmond.chong@sap.com>		100173193
Jason Tan			Singapore		jas.tan@sap.com				- done	Tan, Jason <jas.tan@sap.com>				100173194
Chen Yu				Singapore		yu.chen03@sap.com
Meenal				US, San Ramon	
Steve				US, Virginia								- done Garcia, Steven <steven.garcia@sap.com>			100173197
Sam Tsang			UK, London		sam.tsang@sap.com			- done	Tsang, Sam <sam.tsang@sap.com>					100173195
Ujjal				India, Pune		ujjal.chatterjee@sap.com	- done	Chatterjee, Ujjal <ujjal.chatterjee@sap.com>	100173196
Meenal prasad			meenal.prasad@sap.com	Prasad, Meenal <meenal.prasad@sap.com>	100173198


Hi SysEng,

We need ReadOnly access to Prasad, Meenal <meenal.prasad@sap.com> on our BD TRUST domain.


Access Type:
Contractor
Change Request Type:
Add Access
Access duration. (Days):
365
Status:
Completed
Account type:
Linux
Record Type:
Production Account Request
Access Level:
Consumer Insight


Desmond Chong




------------------------------ 5 May 2020 -------------------------------------------------

cat application.log | grep -i unauthorized




------------------------------ 6 May 2020 -------------------------------------------------

- Had a discussion with Ravi regarding migration of SCL project.
- 


- Create a log group similar to RMS.
- Search query only for MMG.


fields @timestamp, @message
| filter kubernetes.container_name like /mmg-sch/
| fields strcontains(@message, "Response from SMS Client")
| sort @timestamp desc
| limit 20


------------------------------------ 7 May 2020 ------------------------------------------

- Had a 2 hour Showcase meeting




------------------------------------ 8 May 2020 ------------------------------------------

- Resolved Shikha issue of setting up the folder for sftp user.

- Resolved the reporting issue of US4CI, the file was not in zipped format. Hence zipping it.

-rw-r--r-- 1 root      root        4484419711 May  3 00:01 messages-2020-05-03.gz
-rw-r--r-- 1 root      root        4001978483 May  4 00:01 messages-2020-05-04.gz
-rw-r--r-- 1 root      root        4514148868 May  5 00:01 messages-2020-05-05.gz
-rw-r--r-- 1 root      root        4718492173 May  6 00:01 messages-2020-05-06.gz
-rw-r--r-- 1 root      root      117892552689 May  7 00:01 messages-2020-05-07
-rw-r--r-- 1 root      root        4406387465 May  8 00:01 messages-2020-05-08.gz

- Added ChenYu to the required Fisheye/Crucible group.

- Analysed the below error logs, development team is working on it.

May  8 09:10:01 us4ciweb01 docker/e04bd2283f03[32149]: 2020/05/08 09:10:01 [error] 22995#22995: *5966078 recv() failed (104: Connection reset by peer) while reading response header from upstream, client: 10.200.1.254, server: livelink.sapdigitalinterconnect.com, request: "GET /health/sms-channel HTTP/1.1", upstream: "http://10.200.2.19:15701/manager/health", host: "livelink.sapdigitalinterconnect.com"
May  8 09:10:01 us4ciweb01 docker/e04bd2283f03[32149]: 2020/05/08 09:10:01 [error] 22995#22995: *5966078 recv() failed (104: Connection reset by peer) while reading response header from upstream, client: 10.200.1.254, server: livelink.sapdigitalinterconnect.com, request: "GET /health/sms-channel HTTP/1.1", upstream: "http://10.200.2.19:15701/manager/health", host: "livelink.sapdigitalinterconnect.com"
May  8 09:10:01 us4ciweb01 docker/e04bd2283f03[32149]: 2020/05/08 09:10:01 [error] 22995#22995: *5966078 recv() failed (104: Connection reset by peer) while reading response header from upstream, client: 10.200.1.254, server: livelink.sapdigitalinterconnect.com, request: "GET /health/sms-channel HTTP/1.1", upstream: "http://10.200.2.19:15701/manager/health", host: "livelink.sapdigitalinterconnect.com"



------------------------------------ 11 May 2020 ------------------------------------------

- Provided following list of users to SysEng for which BDTrust needs to be created.

	- Desmond chong
	- Sam Tsang
	- 

- Provided list of server to SuhWei,

aws1mmgrabbitmq001	10.1.7.141
aws1mmgrabbitmq002	10.1.8.247
aws1mmgrabbitmq003	10.1.7.197
aws1mmgredis001		10.1.7.166
aws1mmgredis002		10.1.8.243
aws1mmgredis003		10.1.7.252

aws1-eks-encrypted-backend-encrypted-Node 10.1.7.204, 10.1.8.172

aws1asedb002-primary	10.1.6.116

- 1 Hour mike meeting
- 1 Hour Khalid meeting


curl -X POST \
  https://login.sapdigitalinterconnect.com/oauth/token \
  -H 'Accept: application/json' \
  -H 'Content-Type: application/x-www-form-urlencoded' \
  -H 'Postman-Token: 7063f376-fd94-4a71-800e-916124f850f7' \
  -H 'cache-control: no-cache' \
  -d 'grant_type=password&scope=prodaws1:mmg:ui&token_type=JWT&username=C5246840&password=April!@#45'
  


  
------------------------------------ 12 May 2020 ------------------------------------------

- Checked details of RMS failure,

https://apm.cf.us10.hana.ondemand.com/e/f95fd477-ba09-47d0-ba91-d6f35a4e2f22/#failureanalysis;sci=SERVICE-465CB4A0511708E5;pid=8629035236288900434_1589266680000V2;servicefilter=0%1E10%11SERVICE_METHOD_GROUP-EAE5D4109D057446%102%11500-599%103%110;timeframe=custom1589268540000to1589268840000;gf=all

Getting this error from Engagement API server.

Server returned HTTP response code: ***** for URL: https://ENGAGEMENTAPI-SP.SAPMOBILESERVICES.COM/MessageSendingConnectorService/MessageSendingConnector


- Responded to Dhal Chandra on mail regarding Network error from Paypal side.

- Had a call with Ravi for SCL-365

- Had a call with Pablo regarding WAF

- Had a call regarding WAF with Ryan


------------------------------------ 13 May 2020 ------------------------------------------

- Added changes to AWSDEV scheduler service

MAX_CONSUMER_PER_GROUP=10
INTERNAL_CALLBACK_URL=https://mmg-core-service.mmg.int.cloud.sap:19100/api/outbound_sms/acknowledgement/{{message-id}}


- Added Pod on production w.r.t to the following build for UI

https://bamboo.di-infra.sap.corp/browse/MMG-LLKPOR5-19


- Made changes to HPA on AWSDEV and AWSPRD environment.

[skesarkar@ip-10-1-12-98 mmg_chart]$ cd templates/
[skesarkar@ip-10-1-12-98 templates]$ ls
configmap_mmg-core-cm.yaml  mmg-core-deployment.yaml  mmg-sch-secret.yaml     NOTES.txt                  service_mmg-ui-svc.yaml
configmap_mmg-sch-cm.yaml   mmg-core-secret.yaml      mmg-ui-deployment.yaml  service_mmg-core-svc.yaml
configmap_mmg-ui-cm.yaml    mmg-sch-deployment.yaml   mmg-ui-secret.yaml      service_mmg-sch-svc.yaml
[skesarkar@ip-10-1-12-98 templates]$ vim hpa_mmg-ui-deployment.yaml
[skesarkar@ip-10-1-12-98 templates]$ vim hpa_mmg-sch-deployment.yaml
[skesarkar@ip-10-1-12-98 templates]$ vim hpa_mmg-core-deployment.yaml
[skesarkar@ip-10-1-12-98 templates]$ pwd
/var/home/skesarkar/GIT/devops/mmg/mmg_helm/mmg_chart/templates
[skesarkar@ip-10-1-12-98 templates]$ ls
configmap_mmg-core-cm.yaml  hpa_mmg-core-deployment.yaml  mmg-core-deployment.yaml  mmg-sch-secret.yaml     NOTES.txt                  service_mmg-ui-svc.yaml
configmap_mmg-sch-cm.yaml   hpa_mmg-sch-deployment.yaml   mmg-core-secret.yaml      mmg-ui-deployment.yaml  service_mmg-core-svc.yaml
configmap_mmg-ui-cm.yaml    hpa_mmg-ui-deployment.yaml    mmg-sch-deployment.yaml   mmg-ui-secret.yaml      service_mmg-sch-svc.yaml
[skesarkar@ip-10-1-12-98 templates]$ cd ..
[skesarkar@ip-10-1-12-98 mmg_chart]$ ls
Chart.yaml  templates  values.yaml  values.yaml.bkp.aws1prd  values.yaml.bkp.aws6stg  values.yaml.bkp.awsdev
[skesarkar@ip-10-1-12-98 mmg_chart]$ cd ..
[skesarkar@ip-10-1-12-98 mmg_helm]$ helm upgrade mmg mmg_chart
Release "mmg" has been upgraded. Happy Helming!
NAME: mmg
LAST DEPLOYED: Wed May 13 07:46:38 2020
NAMESPACE: mmg
STATUS: deployed
REVISION: 4
TEST SUITE: None
NOTES:
Application mmg_chart deployed successful.
Please check the functional Pod using 'kubectl get pods' command.

Thanks for using Helm templating for AAT, in case of doubts/problems, please contact the SAPDI DevOps team through our DL: DL_59BADC0B5F99B7EEC4000406@global.corp.sap (DL SAP PE Digital Interconnect Industry Services DevOps)

Have a nice day.


####################################################################
####################################################################
[skesarkar@ip-10-1-12-98 mmg_helm]$ kubectl get hpa -n mmg
NAME                      REFERENCE                        TARGETS         MINPODS   MAXPODS   REPLICAS   AGE
mmg-core-deployment-hpa   Deployment/mmg-core-deployment   <unknown>/80%   2         6         0          9s
mmg-sch-deployment-hpa    Deployment/mmg-sch-deployment    <unknown>/80%   2         6         0          9s
mmg-ui-deployment-hpa     Deployment/mmg-ui-deployment     <unknown>/80%   2         6         0          9s
[skesarkar@ip-10-1-12-98 mmg_helm]$ kubectl get pods -n mmg
NAME                                   READY   STATUS    RESTARTS   AGE
dnsutils                               1/1     Running   563        23d
mmg-core-deployment-8579d7b4f4-cnlvr   1/1     Running   0          13d
mmg-core-deployment-8579d7b4f4-z5vd5   1/1     Running   0          13d
mmg-sch-deployment-d9bc59697-s68bv     1/1     Running   0          13d
mmg-sch-deployment-d9bc59697-tz2kf     1/1     Running   0          13d
mmg-ui-deployment-7fd885c9c-5vd5j      1/1     Running   0          172m
mmg-ui-deployment-7fd885c9c-xj6hj      1/1     Running   0          172m
[skesarkar@ip-10-1-12-98 mmg_helm]$ vim mmg_chart/values.yaml
[skesarkar@ip-10-1-12-98 mmg_helm]$ helm upgrade mmg mmg_chart
Release "mmg" has been upgraded. Happy Helming!
NAME: mmg
LAST DEPLOYED: Wed May 13 07:47:41 2020
NAMESPACE: mmg
STATUS: deployed
REVISION: 5
TEST SUITE: None
NOTES:
Application mmg_chart deployed successful.
Please check the functional Pod using 'kubectl get pods' command.

Thanks for using Helm templating for AAT, in case of doubts/problems, please contact the SAPDI DevOps team through our DL: DL_59BADC0B5F99B7EEC4000406@global.corp.sap (DL SAP PE Digital Interconnect Industry Services DevOps)

Have a nice day.


####################################################################
####################################################################
[skesarkar@ip-10-1-12-98 mmg_helm]$ kubectl get pods -n mmg
NAME                                   READY   STATUS    RESTARTS   AGE
dnsutils                               1/1     Running   563        23d
mmg-core-deployment-8579d7b4f4-cnlvr   1/1     Running   0          13d
mmg-core-deployment-8579d7b4f4-z5vd5   1/1     Running   0          13d
mmg-sch-deployment-d9bc59697-s68bv     1/1     Running   0          13d
mmg-sch-deployment-d9bc59697-tz2kf     1/1     Running   0          13d
mmg-ui-deployment-7fd885c9c-5vd5j      1/1     Running   0          173m
mmg-ui-deployment-7fd885c9c-xj6hj      1/1     Running   0          173m
[skesarkar@ip-10-1-12-98 mmg_helm]$ kubectl get hpa -n mmg
NAME                      REFERENCE                        TARGETS         MINPODS   MAXPODS   REPLICAS   AGE
mmg-core-deployment-hpa   Deployment/mmg-core-deployment   <unknown>/80%   3         6         2          83s
mmg-sch-deployment-hpa    Deployment/mmg-sch-deployment    <unknown>/80%   3         6         2          83s
mmg-ui-deployment-hpa     Deployment/mmg-ui-deployment     <unknown>/80%   3         6         2          83s
[skesarkar@ip-10-1-12-98 mmg_helm]$ kubectl get hpa -n mmg
NAME                      REFERENCE                        TARGETS         MINPODS   MAXPODS   REPLICAS   AGE
mmg-core-deployment-hpa   Deployment/mmg-core-deployment   <unknown>/80%   3         6         3          90s
mmg-sch-deployment-hpa    Deployment/mmg-sch-deployment    <unknown>/80%   3         6         3          90s
mmg-ui-deployment-hpa     Deployment/mmg-ui-deployment     <unknown>/80%   3         6         3          90s
[skesarkar@ip-10-1-12-98 mmg_helm]$ kubectl get pods -n mmg
NAME                                   READY   STATUS    RESTARTS   AGE
dnsutils                               1/1     Running   563        23d
mmg-core-deployment-8579d7b4f4-cnlvr   1/1     Running   0          13d
mmg-core-deployment-8579d7b4f4-gp65w   0/1     Running   0          26s
mmg-core-deployment-8579d7b4f4-z5vd5   1/1     Running   0          13d
mmg-sch-deployment-d9bc59697-kgvhp     0/1     Running   0          26s
mmg-sch-deployment-d9bc59697-s68bv     1/1     Running   0          13d
mmg-sch-deployment-d9bc59697-tz2kf     1/1     Running   0          13d
mmg-ui-deployment-7fd885c9c-5vd5j      1/1     Running   0          173m
mmg-ui-deployment-7fd885c9c-xj6hj      1/1     Running   0          174m
mmg-ui-deployment-7fd885c9c-xzj85      1/1     Running   0          26s
[skesarkar@ip-10-1-12-98 mmg_helm]$ ls
aws1_prd_mmg_ingress.yaml  aws1_prd_mmg_regcred.yaml  mmg_chart
[skesarkar@ip-10-1-12-98 mmg_helm]$ vim mmg_chart/values.yaml
[skesarkar@ip-10-1-12-98 mmg_helm]$ helm upgrade mmg mmg_chart
Release "mmg" has been upgraded. Happy Helming!
NAME: mmg
LAST DEPLOYED: Wed May 13 07:53:18 2020
NAMESPACE: mmg
STATUS: deployed
REVISION: 6
TEST SUITE: None
NOTES:
Application mmg_chart deployed successful.
Please check the functional Pod using 'kubectl get pods' command.

Thanks for using Helm templating for AAT, in case of doubts/problems, please contact the SAPDI DevOps team through our DL: DL_59BADC0B5F99B7EEC4000406@global.corp.sap (DL SAP PE Digital Interconnect Industry Services DevOps)

Have a nice day.


####################################################################
####################################################################
[skesarkar@ip-10-1-12-98 mmg_helm]$ kubectl get hpa -n mmg
NAME                      REFERENCE                        TARGETS         MINPODS   MAXPODS   REPLICAS   AGE
mmg-core-deployment-hpa   Deployment/mmg-core-deployment   <unknown>/80%   2         6         2          6m51s
mmg-sch-deployment-hpa    Deployment/mmg-sch-deployment    <unknown>/80%   2         6         2          6m51s
mmg-ui-deployment-hpa     Deployment/mmg-ui-deployment     <unknown>/80%   2         6         2          6m51s
[skesarkar@ip-10-1-12-98 mmg_helm]$ kubectl get pods -n mmg
NAME                                   READY   STATUS        RESTARTS   AGE
dnsutils                               1/1     Running       563        23d
mmg-core-deployment-8579d7b4f4-cnlvr   1/1     Running       0          13d
mmg-core-deployment-8579d7b4f4-gp65w   1/1     Terminating   0          5m42s
mmg-core-deployment-8579d7b4f4-z5vd5   1/1     Running       0          13d
mmg-sch-deployment-d9bc59697-s68bv     1/1     Running       0          13d
mmg-sch-deployment-d9bc59697-tz2kf     1/1     Running       0          13d
mmg-ui-deployment-7fd885c9c-5vd5j      1/1     Running       0          179m
mmg-ui-deployment-7fd885c9c-xj6hj      1/1     Running       0          179m
mmg-ui-deployment-7fd885c9c-xzj85      1/1     Terminating   0          5m42s




------------------------------------ 15 May 2020 ------------------------------------------

- Helped Nikita to kill and start redis server on 6381 port on 55 and 51 for testin purpose.



Nikita Channawar  1:46 PM
Hi Sachin,
I have one question, if i kill redis process on 53 server then why it is again showing with different process id?
1:46
c5300646@llbpal53:~> ps aux | grep redis | grep 6381
redis    33638  0.1  0.0  36012  4384 ?        Ssl  May07  17:53 /opt/redis/bin/redis-server 0.0.0.0:26381 [sentinel]
root     70700  0.1  0.0  82456 40576 ?        Ssl  May11   7:45 redis-server 0.0.0.0:6381
c5300646@llbpal53:~>
c5300646@llbpal53:~>
c5300646@llbpal53:~> sudo kill 33638
c5300646@llbpal53:~> ps aux | grep redis | grep 6381
redis    29911  0.0  0.0  36012  3928 ?        Ssl  01:14   0:00 /opt/redis/bin/redis-server 0.0.0.0:26381 [sentinel]
root     70700  0.1  0.0  82456 40576 ?        Ssl  May11   7:45 redis-server 0.0.0.0:6381


- Applied terraform for RabbitMQ ALB

aws_alb_listener.front_end: Still creating... [4m40s elapsed]
aws_alb_listener.front_end: Still creating... [4m50s elapsed]

Error: error creating ELBv2 Listener: CertificateNotFound: Certificate 'arn:aws:acm:us-east-1:079549895456:certificate/ee515a47-b7ae-41c1-843d-c720ede96544' not found
        status code: 400, request id: 9415a88a-0d98-44e8-aa51-f3a9d421f454

  on alb.tf line 44, in resource "aws_alb_listener" "front_end":
  44: resource "aws_alb_listener" "front_end" {

------------------------------- 18 May 2020 --------------------------------------------------
- Had a call with Khalid almost 1 hour call
- Had a call with Pablo

------------------------------- 19 May 2020 --------------------------------------------------

- Worked with Nikita, Nishant.

- Installed Rabbitmq on AWSDEV.

  rabbitmq:
    host: rabbitmq-ha.shared-services
    port: 5672
    user: admin
    pwd: gv946uhFPY
    addresses: rabbitmq-ha.shared-services:5672
	
	
198.19.8.208 	awsdevmmgrabbitmq001
198.19.7.174	awsdevmmgrabbitmq002
198.19.8.204	awsdevmmgrabbitmq003

-------------------------------------------------------------

127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4 awsdevmmgrabbitmq003.bd.trust awsdevmmgrabbitmq003

awsdevmmgrabbitmq001 198.19.8.208
awsdevmmgrabbitmq002 198.19.7.174
awsdevmmgrabbitmq003 198.19.8.204

198.19.8.208

198.19.8.208:5672,198.19.7.174:5672,198.19.8.204:5672

Create rabbitMQ container on awsdevmmgrabbitmq001

docker run -d --restart=always --net=host --hostname awsdevmmgrabbitmq001 --name docker-rabbit --ulimit nofile=65535:65535 -p 4369:4369 -p 5672:5672 -p 15672:15672 -p 25672:25672 -p 35197:35197 -u rabbitmq -e RABBITMQ_DEFAULT_USER=admin -e RABBITMQ_DEFAULT_PASS=gv946uhFPY -e RABBITMQ_ERLANG_COOKIE='rabbitmqsecret' rabbitmq:3-management

Create rabbitMQ container on awsdevmmgrabbitmq002

docker run -d --restart=always --net=host --hostname awsdevmmgrabbitmq002 --name docker-rabbit --ulimit nofile=65535:65535 -p 4369:4369 -p 5672:5672 -p 15672:15672 -p 25672:25672 -p 35197:35197 -u rabbitmq -e RABBITMQ_DEFAULT_USER=admin -e RABBITMQ_DEFAULT_PASS=gv946uhFPY -e RABBITMQ_ERLANG_COOKIE='rabbitmqsecret' rabbitmq:3-management

Create rabbitMQ container on awsdevmmgrabbitmq003

docker run -d --restart=always --net=host --hostname awsdevmmgrabbitmq003 --name docker-rabbit --ulimit nofile=65535:65535 -p 4369:4369 -p 5672:5672 -p 15672:15672 -p 25672:25672 -p 35197:35197 -u rabbitmq -e RABBITMQ_DEFAULT_USER=admin -e RABBITMQ_DEFAULT_PASS=gv946uhFPY -e RABBITMQ_ERLANG_COOKIE='rabbitmqsecret' rabbitmq:3-management

On awsdevmmgrabbitmq002 and awsdevmmgrabbitmq003, join cluster rabbit@awsdevmmgrabbitmq001

docker exec docker-rabbit rabbitmqctl stop_app
docker exec docker-rabbit rabbitmqctl join_cluster rabbit@awsdevmmgrabbitmq001
docker exec docker-rabbit rabbitmqctl start_app
docker exec docker-rabbit rabbitmqctl set_policy ha-nodes "^nodes\." \
'{"ha-mode":"nodes","ha-params":["rabbit@awsdevmmgrabbitmq001", "rabbit@awsdevmmgrabbitmq002", "rabbit@awsdevmmgrabbitmq003"]}'





Enable shovel

docker exec docker-rabbit rabbitmq-plugins enable rabbitmq_shovel



------------------------------- 20 May 2020 --------------------------------------------------

15910 : give more descriptive name than, webserver name : _15910


cat messages | grep -i "May 20 05:" | grep -i "Exception"

cat application.log | grep -i "2020-05-20 05" | grep -i "Exception" | head

- Had 2 hours call with SuhWei.


------------------------------- 21 MAy 2020 ---------------------------------------------------

- Had a 2 hour call with SuhWei.

- Worked with Nidhi on bringing down redis and testing the application.

- Had a call with Khalid 1 hour call team meeting.

- Worked on Secret-bridge :

https://duo.com/labs/research/how-to-monitor-github-for-secrets

https://github.com/duo-labs/secret-bridge

https://github.com/Yelp/detect-secrets

https://pre-commit.com/

https://help.github.com/en/enterprise/2.20/admin/developer-workflow/creating-a-pre-receive-hook-script

https://github.com/awslabs/git-secrets

https://github.com/dxa4481/truffleHog

https://sweetcode.io/how-use-truffle-hog-git-secrets/

https://redblueteam.wordpress.com/2020/01/04/digging-secrets-from-git-repositories-by-using-trufflehog/



------------------------------- 25 May 2020 ---------------------------------------------------

- Resolved the issue of (deleted) file descriptor issue

- Worked with MMG team on Slack check the Chat


- Applied Redis for MMG on AWSSTG environment.

	----------------------------------------------------------
		
	Apply complete! Resources: 6 added, 0 changed, 0 destroyed.

	Outputs:

	azs = [
	  "us-east-1a",
	  "us-east-1b",
	]
	cluster_endpoint = master.awsstg-mmg-redis.prv1ld.use1.cache.amazonaws.com
	password = YyHQpRZQxyAvCZnu
	port = 6382
	redis_string_connection = rediss://YyHQpRZQxyAvCZnu@master.awsstg-mmg-redis.prv1ld.use1.cache.amazonaws.com:6382/0
	----------------------------------------------------------
	
	
	
	
------------------------------- 26 May 2020 ---------------------------------------------------

- Resolved the issue of RabbitMQ not responding. SMSChannel was not receiving the ACK's.

- Worked with SuhWei for RabbitMQ queuing issue for load testing

- Worked with Abhilekh on resolving the UI issue.

- Worked with Nidhi, check chat.

- Also please check mails.



------------------------------- 27 May 2020 ---------------------------------------------------

https://medium.com/better-programming/how-you-can-prevent-committing-secrets-and-credentials-into-git-repositories-adffc25c2ea2

https://blog.gitguardian.com/git-hooks-automated-secrets-detection/






------------------------------- 28 May 2020 ---------------------------------------------------



curl -k -X GET https://dbs-docker-repository.docker.repositories.sap.ondemand.com/v2/_catalog -w "\n%{http_code}\n" -H 'Content-Type: application/json' -H 'Authorization: Basic YWRtaW46c2VjcmV0' -d '{"new_password" : "SAPpoc!23"}'



------------------------------- 29 May 2020 ---------------------------------------------------

- Involved in load testing activity of MMG

- Created a repository for "rms-ui-test-automation".




--------------------------------1June2020-------------------------------------------------------

- arn:aws:eks:us-east-1:079549895456:cluster/awsstg-eks-encrypted


REDIS_SENTINEL_NODES=us4ciapi01.bd.trust:26385,us4ciapi03.bd.trust:26385,us4ciapi04.bd.trust:26385

- Reference : https://github.tools.sap/sapdi/devops/blob/AATPRD/helm/llk/values.yaml


isql -Usch_db_admin -S10.2.5.89:5002 -w5000 -PFZEPPnePZX

isql -Ummg_db_admin -S10.2.5.89:5002 -w5000 -Pw#pfzNP5ml



apiVersion: v1
data:
  .dockerconfigjson: eyJhdXRocyI6eyJodHRwczovL2Ricy1kb2NrZXItcmVwb3NpdG9yeS5kb2NrZXIucmVwb3NpdG9yaWVzLnNhcC5vbmRlbWFuZC5jb20iOnsidXNlcm5hbWUiOiJkYnMtZG9ja2VyLXVzZXIiLCJwYXNzd29yZCI6Im44MTNuWncyMmEyZVZqNE4iLCJlbWFpbCI6InNhY2hpbi5rZXNhcmthckBzYXAuY29tIiwiYXV0aCI6IlpHSnpMV1J2WTJ0bGNpMTFjMlZ5T200NE1UTnVXbmN5TW1FeVpWWnFORTQ9In19fQ==
kind: Secret
metadata:
  creationTimestamp: "2020-06-01T16:37:49Z"
  name: regcred
  namespace: mmg
  resourceVersion: "76789614"
  selfLink: /api/v1/namespaces/mmg/secrets/regcred
  uid: 3ad8f6ec-a426-11ea-b587-12238f87a887
type: kubernetes.io/dockerconfigjson

apiVersion: v1
data:
  .dockerconfigjson: eyJhdXRocyI6eyJodHRwczovL2Ricy1kb2NrZXItcmVwb3NpdG9yeS5kb2NrZXIucmVwb3NpdG9yaWVzLnNhcC5vbmRlbWFuZC5jb20iOnsidXNlcm5hbWUiOiJkYnMtZG9ja2VyLXVzZXIiLCJwYXNzd29yZCI6Im44MTNuWncyMmEyZVZqNE4iLCJlbWFpbCI6InNhY2hpbi5rZXNhcmthckBzYXAuY29tIiwiYXV0aCI6IlpHSnpMV1J2WTJ0bGNpMTFjMlZ5T200NE1UTnVXbmN5TW1FeVpWWnFORTQ9In19fQ==
kind: Secret
metadata:
  creationTimestamp: "2020-06-01T16:41:08Z"
  name: mmg-regcred
  namespace: mmg
  resourceVersion: "76790171"
  selfLink: /api/v1/namespaces/mmg/secrets/mmg-regcred
  uid: b16d5f1a-a426-11ea-bbf5-0ad132001557
type: kubernetes.io/dockerconfigjson


apiVersion: v1
data:
  .dockerconfigjson: eyJhdXRocyI6eyJkYnMtZG9ja2VyLXJlcG9zaXRvcnkuZG9ja2VyLnJlcG9zaXRvcmllcy5zYXAub25kZW1hbmQuY29tIjp7InVzZXJuYW1lIjoiZGJzLWRvY2tlci11c2VyIiwicGFzc3dvcmQiOiJuODEzblp3MjJhMmVWajROIiwiZW1haWwiOiJwYWJsby5zaWx2YTAxQHNhcC5jb20iLCJhdXRoIjoiWkdKekxXUnZZMnRsY2kxMWMyVnlPbTQ0TVROdVduY3lNbUV5WlZacU5FND0ifX19
kind: Secret
metadata:
  namespace: mmg
  name: mmg-regcred
type: kubernetes.io/dockerconfigjson


kubectl create secret docker-registry mmg-regcred -n mmg --docker-server=https://dbs-docker-repository.docker.repositories.sap.ondemand.com --docker-username=dbs-docker-user --docker-password=n813nZw22a2eVj4N --docker-email=sachin.kesarkar@sap.com




--------------------------------2June2020-------------------------------------------------------

- Followed with Amol on RMS queue.

- 

2020-06-02T05:20:29-07:00 llbpal52.pal.sap.corp mmg-core/7e23e083d103[4109]: 2020-06-02 12:20:29.858 DEBUG 7 --- [nnection closer] com.zaxxer.hikari.pool.PoolBase          : HikariPool-1 - Closing connection com.sybase.jdbc4.jdbc.SybConnection@5f097780: (connection has passed maxLifetime)


isql -SDTSCIDB -Usa -X -Y -PAdh3r3nt -w8000

isql -SQASCIDB -Ueng_admin_db_user -X -PNg2u8HvW -w9000


sybase@llbpal58:~> isql -SDTSCIDB -Ueng_admin_db_user -PNg2u8HvW -w8000
Msg 4002, Level 14, State 1:
Server 'DTSCIDB':
Login failed.
CT-LIBRARY error:
        ct_connect(): protocol specific layer: external error: The attempt to connect to the server failed.
sybase@llbpal58:~> ps aux | grep -i
Usage: grep [OPTION]... PATTERN [FILE]...
Try `grep --help' for more information.
sybase@llbpal58:~>
sybase@llbpal58:~>
sybase@llbpal58:~> isql -SQASCIDB -Ueng_admin_db_user -X -PNg2u8HvW -w9000
1> select convert(varchar(30),o.name) AS table_name
from sysobjects o
where type = 'U'
order by table_name2> 3> 4>
5> go
 table_name
 ------------------------------
 CampaignChecking
 DATABASECHANGELOG
 DATABASECHANGELOGLOCK
 campaign
 campaign_checking
 campaign_subscription
 organization
 organization_permission
 organization_sms_account
 outbound_sms
 parameter
 permission
 role
 role_permission
 sms_account
 sms_template
 subscriber
 task
 user_profile
 user_profile_sms_account

(20 rows affected)



[psilva@iad1bastion01 .ssh]$ ssh -i .ssh/awsstg_ps.pem ec2-user@10.2.7.29
Warning: Identity file .ssh/awsstg_ps.pem not accessible: No such file or directory.
Permission denied (publickey,gssapi-keyex,gssapi-with-mic).
[psilva@iad1bastion01 .ssh]$

isql -SAWSSTGRMSDB1:5002 -Usch_db_admin -X -w8000 -P'FZEPPnePZX'

helm install --name mmgsachin . --debug --tls




apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    alb.ingress.kubernetes.io/backend-protocol: HTTPS
    alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:us-east-1:079549895456:certificate/683660c7-0a4d-4853-8585-54fed4ff4958
    alb.ingress.kubernetes.io/healthcheck-interval-seconds: "10"
    alb.ingress.kubernetes.io/healthcheck-path: /
    alb.ingress.kubernetes.io/healthcheck-port: "32125"
    alb.ingress.kubernetes.io/healthcheck-protocol: HTTPS
    alb.ingress.kubernetes.io/healthcheck-timeout-seconds: "8"
    alb.ingress.kubernetes.io/healthy-threshold-count: "5"
    alb.ingress.kubernetes.io/listen-ports: '[{"HTTPS":443}]'
    alb.ingress.kubernetes.io/load-balancer-attributes: deletion_protection.enabled=true
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/security-groups: sg-0068e45a128711ec5
    alb.ingress.kubernetes.io/ssl-policy: ELBSecurityPolicy-TLS-1-2-Ext-2018-06
    alb.ingress.kubernetes.io/success-codes: 200-300,307,404
    alb.ingress.kubernetes.io/target-type: instance
    kubernetes.io/ingress.class: alb
    meta.helm.sh/release-name: aat
    meta.helm.sh/release-namespace: kube-system
  creationTimestamp: "2020-05-30T00:10:49Z"
  generation: 1
  labels:
    AppVersion: "1.0"
    ChartVersion: 0.1.0
    app: aat
    app.kubernetes.io/managed-by: Helm
    component: aat
    environment: awsstg
    managed-by: DevOps
  name: aat-ingress
  namespace: aat
  resourceVersion: "1417452"
  selfLink: /apis/extensions/v1beta1/namespaces/aat/ingresses/aat-ingress
  uid: 0439df71-a20a-11ea-880e-0a0e6bd5293f
spec:
  rules:
  - http:
      paths:
      - backend:
          serviceName: aat-reverse-proxy-svc
          servicePort: 14500
        path: /*
status:
  loadBalancer:
    ingress:
    - hostname: ed7e5296-aat-aatingress-837a-847944655.us-east-1.elb.amazonaws.com
[sachin@awsstgdevops001 mmg_helm]$ cat aws_stg_mmg_
aws_stg_mmg_ingress.yaml  aws_stg_mmg_regcred.yaml
[sachin@awsstgdevops001 mmg_helm]$ cat aws_stg_mmg_ingress.yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: mmg-ingress
  namespace: mmg
  annotations:
    kubernetes.io/ingress.class: alb
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/security-groups: sg-0c9e9babdfb2e247d
    alb.ingress.kubernetes.io/healthy-threshold-count: '5'
    alb.ingress.kubernetes.io/healthcheck-port: '30955'
    alb.ingress.kubernetes.io/healthcheck-protocol: HTTPS
    alb.ingress.kubernetes.io/listen-ports: '[{"HTTPS":443}]'
    alb.ingress.kubernetes.io/success-codes: 200-300,307,404
    alb.ingress.kubernetes.io/healthcheck-path: /
    alb.ingress.kubernetes.io/healthcheck-interval-seconds: '10'
    alb.ingress.kubernetes.io/healthcheck-timeout-seconds: '8'
    alb.ingress.kubernetes.io/ssl-policy: ELBSecurityPolicy-TLS-1-2-Ext-2018-06
    alb.ingress.kubernetes.io/certificate-arn: arn:aws:iam::079549895456:server-certificate/jira-self-signed-certificate
    alb.ingress.kubernetes.io/target-type: instance
    alb.ingress.kubernetes.io/backend-protocol: HTTPS
spec:
  rules:
    - http:
        paths:
          - path: /*
            backend:
              serviceName: mmg-ui
              servicePort: 15910
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    alb.ingress.kubernetes.io/backend-protocol: HTTPS
    alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:us-east-1:079549895456:certificate/683660c7-0a4d-4853-8585-54fed4ff4958
    alb.ingress.kubernetes.io/healthcheck-interval-seconds: "10"
    alb.ingress.kubernetes.io/healthcheck-path: /
    alb.ingress.kubernetes.io/healthcheck-port: "32125"
    alb.ingress.kubernetes.io/healthcheck-protocol: HTTPS
    alb.ingress.kubernetes.io/healthcheck-timeout-seconds: "8"
    alb.ingress.kubernetes.io/healthy-threshold-count: "5"
    alb.ingress.kubernetes.io/listen-ports: '[{"HTTPS":443}]'
    alb.ingress.kubernetes.io/load-balancer-attributes: deletion_protection.enabled=true
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/security-groups: sg-0068e45a128711ec5
    alb.ingress.kubernetes.io/ssl-policy: ELBSecurityPolicy-TLS-1-2-Ext-2018-06
    alb.ingress.kubernetes.io/success-codes: 200-300,307,404
    alb.ingress.kubernetes.io/target-type: instance
    kubernetes.io/ingress.class: alb
    meta.helm.sh/release-name: aat
    meta.helm.sh/release-namespace: kube-system
  creationTimestamp: "2020-05-30T00:10:49Z"
  generation: 1
  labels:
    AppVersion: "1.0"
    ChartVersion: 0.1.0
    app: aat
    app.kubernetes.io/managed-by: Helm
    component: aat
    environment: awsstg
    managed-by: DevOps
  name: aat-ingress
  namespace: aat
  resourceVersion: "1417452"
  selfLink: /apis/extensions/v1beta1/namespaces/aat/ingresses/aat-ingress
  uid: 0439df71-a20a-11ea-880e-0a0e6bd5293f
spec:
  rules:
  - http:
      paths:
      - backend:
          serviceName: aat-reverse-proxy-svc
          servicePort: 14500
        path: /*
status:
  loadBalancer:
    ingress:
    - hostname: ed7e5296-aat-aatingress-837a-847944655.us-east-1.elb.amazonaws.com
[sachin@awsstgdevops001 mmg_helm]$ cat aws_stg_mmg_
aws_stg_mmg_ingress.yaml  aws_stg_mmg_regcred.yaml
[sachin@awsstgdevops001 mmg_helm]$ cat aws_stg_mmg_ingress.yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: mmg-ingress
  namespace: mmg
  annotations:
    kubernetes.io/ingress.class: alb
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/security-groups: sg-0068e45a128711ec5
    alb.ingress.kubernetes.io/healthy-threshold-count: '5'
    alb.ingress.kubernetes.io/healthcheck-port: '30955'
    alb.ingress.kubernetes.io/healthcheck-protocol: HTTPS
    alb.ingress.kubernetes.io/listen-ports: '[{"HTTPS":443}]'
    alb.ingress.kubernetes.io/success-codes: 200-300,307,404
    alb.ingress.kubernetes.io/healthcheck-path: /
    alb.ingress.kubernetes.io/healthcheck-interval-seconds: '10'
    alb.ingress.kubernetes.io/healthcheck-timeout-seconds: '8'
    alb.ingress.kubernetes.io/ssl-policy: ELBSecurityPolicy-TLS-1-2-Ext-2018-06
    alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:us-east-1:079549895456:certificate/683660c7-0a4d-4853-8585-54fed4ff4958
	alb.ingress.kubernetes.io/load-balancer-attributes: deletion_protection.enabled=true
    alb.ingress.kubernetes.io/target-type: instance
    alb.ingress.kubernetes.io/backend-protocol: HTTPS
spec:
  rules:
    - http:
        paths:
          - path: /*
            backend:
              serviceName: mmg-ui
              servicePort: 15910



--------------------------------3June2020-------------------------------------------------------

mo >>>>> are on paypal_backup_mo	. Has to be pushed to main queue

acks >>>>> are on paypal_backup_default


1 - login on rabbitmq us4llkapi002
2 - go to queues
3 - go to paypal_backup_mo queue and scroll down
4 - on the session "Move", fill the field "Destination queue" with "callback_queue" and then click in move



--------------------------------4June2020-------------------------------------------------------
power outage
--------------------------------5June2020-------------------------------------------------------
power outage







--------------------------------8June2020-------------------------------------------------------


You can access the GIT using this URL, https://github.wdf.sap.corp/ . You can refer to following URL to generate the Token for GIT and use it to clone the repository, https://github.wdf.sap.corp/pages/github/sso-enablement .



------ Install rabbitmq for AWSSTG environment for MMG-LLKPOR5-19

10.2.7.198:5672, 10.2.8.85:5672, 10.2.7.201:5672

awsstgmmgrabbitmq001 10.2.7.198
awsstgmmgrabbitmq002 10.2.8.85
awsstgmmgrabbitmq003 10.2.7.201






Create rabbitMQ container on awsstgmmgrabbitmq001

docker run -d --restart=always --hostname awsstgmmgrabbitmq001 --name docker-rabbit --ulimit nofile=65535:65535 -p 4369:4369 -p 5672:5672 -p 15672:15672 -p 25672:25672 -p 35197:35197 -u rabbitmq -e RABBITMQ_DEFAULT_USER=admin -e RABBITMQ_DEFAULT_PASS=gv946uhFPY -e RABBITMQ_ERLANG_COOKIE='rabbitmqsecret' rabbitmq:3-management

docker run -d --restart=always --net=host --hostname awsstgmmgrabbitmq001 --name docker-rabbit --ulimit nofile=65535:65535 -p 4369:4369 -p 5672:5672 -p 15672:15672 -p 25672:25672 -p 35197:35197 -u rabbitmq -e RABBITMQ_DEFAULT_USER=admin -e RABBITMQ_DEFAULT_PASS=gv946uhFPY -e RABBITMQ_ERLANG_COOKIE='rabbitmqsecret' rabbitmq:3-management

Create rabbitMQ container on awsstgmmgrabbitmq002


docker run -d --restart=always --hostname awsstgmmgrabbitmq002 --name docker-rabbit --ulimit nofile=65535:65535 -p 4369:4369 -p 5672:5672 -p 15672:15672 -p 25672:25672 -p 35197:35197 -u rabbitmq -e RABBITMQ_DEFAULT_USER=admin -e RABBITMQ_DEFAULT_PASS=gv946uhFPY -e RABBITMQ_ERLANG_COOKIE='rabbitmqsecret' rabbitmq:3-management

docker run -d --restart=always --net=host --hostname awsstgmmgrabbitmq002 --name docker-rabbit --ulimit nofile=65535:65535 -p 4369:4369 -p 5672:5672 -p 15672:15672 -p 25672:25672 -p 35197:35197 -u rabbitmq -e RABBITMQ_DEFAULT_USER=admin -e RABBITMQ_DEFAULT_PASS=gv946uhFPY -e RABBITMQ_ERLANG_COOKIE='rabbitmqsecret' rabbitmq:3-management

Create rabbitMQ container on awsstgmmgrabbitmq003


docker run -d --restart=always --hostname awsstgmmgrabbitmq003 --name docker-rabbit --ulimit nofile=65535:65535 -p 4369:4369 -p 5672:5672 -p 15672:15672 -p 25672:25672 -p 35197:35197 -u rabbitmq -e RABBITMQ_DEFAULT_USER=admin -e RABBITMQ_DEFAULT_PASS=gv946uhFPY -e RABBITMQ_ERLANG_COOKIE='rabbitmqsecret' rabbitmq:3-management

docker run -d --restart=always --net=host --hostname awsstgmmgrabbitmq003 --name docker-rabbit --ulimit nofile=65535:65535 -p 4369:4369 -p 5672:5672 -p 15672:15672 -p 25672:25672 -p 35197:35197 -u rabbitmq -e RABBITMQ_DEFAULT_USER=admin -e RABBITMQ_DEFAULT_PASS=gv946uhFPY -e RABBITMQ_ERLANG_COOKIE='rabbitmqsecret' rabbitmq:3-management

On awsstgmmgrabbitmq002 and awsstgmmgrabbitmq003, join cluster rabbit@awsstgmmgrabbitmq001

docker exec docker-rabbit rabbitmqctl stop_app
docker exec docker-rabbit rabbitmqctl join_cluster rabbit@awsstgmmgrabbitmq001
docker exec docker-rabbit rabbitmqctl start_app
docker exec docker-rabbit rabbitmqctl set_policy ha-nodes "^nodes\." \
'{"ha-mode":"nodes","ha-params":["rabbit@awsstgmmgrabbitmq001", "rabbit@awsstgmmgrabbitmq002", "rabbit@awsstgmmgrabbitmq003"]}'

 10.2.7.198  awsstgmmgrabbitmq001
 10.2.8.85  awsstgmmgrabbitmq002
 10.2.7.201  awsstgmmgrabbitmq003

127.0.0.1       localhost
::1     localhost ip6-localhost ip6-loopback
fe00::0 ip6-localnet
ff00::0 ip6-mcastprefix
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters
172.17.0.2 awsstgmmgrabbitmq003
10.2.7.198 awsstgmmgrabbitmq001
10.2.8.85  awsstgmmgrabbitmq002



   89  sudo docker exec -u0 -ti docker-rabbit sh
   90  sudo docker exec -u0 -ti docker-rabbit bash




Enable shovel

docker exec docker-rabbit rabbitmq-plugins enable rabbitmq_shovel




/cmn/paypal_ll_<masked>/paypal_ll_<masked>



---------------------------------------- 09-June-2020 -----------------------------------------------------

curl -X POST https://login.sapdigitalinterconnect.com/oauth/token -H 'Authorization: Basic Q1Fac2liUXR6M1Zyb0dNYTpSSldFdWN5RHNRSVBOYUgxRVpUVXFOc1VkNjAySU1FUQ==' -d 'grant_type=client_credentials&token_type=jwt&scope=lle%3Aapi&undefined='




curl -k -X GET https://recipientmanagement.sapdigitalinterconnect.com/api/broadcast-channels/5 -H 'Authorization: Bearer eyJhbGciOiJSUzI1NiJ9.eyJjbGFpbWFudCI6eyJhcHBDb2RlIjoiQ1Fac2liUXR6M1Zyb0dNYSIsImFwcE5hbWUiOiJGZWRFeCBkZW1vIExMSyAyIiwiYXBwVG9rZW5EdXJhdGlvbiI6NjAsInJlZnJlc2hQZXJpb2QiOjE1LCJ0b2tlbkR1cmF0aW9uSW5NaWxsaXNlY29uZHMiOjM2MDAwMDAsInRva2VuUmVmcmVzaFBlcmlvZEluTWlsbGlzZWNvbmRzIjo5MDAwMDAsInJvbGVDb2RlcyI6WyJMSVZFTElOS19BUFAiXSwic2NvcGUiOiJsbGU6YXBpIn0sImlzcyI6IlNDSV9BVVRIRU5USUNBVElPTl9BVVRIT1JJVFkiLCJqdGkiOiJJdjlzaWpoaENEMmFKcGQ5S3RBRHVXYjJvYnZmazZXRSIsImV4cCI6MTU5MTY4OTc4Mn0.kO6TZI5bvrxCu0y2zPzuwc5uWNK-cyDYbOjmRr5VX06ii6xbWrQw6ihYMIfSKZjhpWmJS-RnyxUeWZ0HBIOzUpXil5YC_ot7VZwiaY4tTaW7d3PTwiVvSq0drZhflg5E9LBaj_V_VxWk0J6mPTRVVx1izTU2WoKqGzJZJtjK1RqiKvuMqnpbZvXHKCkHg-4PUhUBS4pSmg_WqWdsYUqq4wS2Et0V6J2Md1Ak4leaIFswl7QPzQAuaBE6DTRFXfTjlSMTtvxWBFyfjzuB0to4AvFwNqx9WyQ6u588WI7kFPZ203P3dXZhmujaiEZw__-hTEm57EyXQaQzcSicVql2IA' -H 'Content-Type: application/json' -H 'System: lle:api'







----------------------------------------- 12-June-2020 ------------------------------------------------------------

- Had a call with Nidhi, explained her the kubernetes funda.

- 

# kubectl exec -ti dnsutils -- nslookup kubernetes.default

{"httpStatusCode":"500 INTERNAL_SERVER_ERROR","apiErrorCode":1013,"userMessage":"Error occurred while processing request. Internal Server Error","debugMessage":"I/O error on POST request for \"https://aat-service.aat.int.cloud.sap:14500/api/app\": Operation timed out (Connection timed out); nested exception is java.net.ConnectException: Operation timed out (Connection timed out)"}

kubectl exec -it dnsutils sh -n aat
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl kubectl exec [POD] -- [COMMAND] instead.
/ # nslookup aat-service.aat.int.cloud.sap
Server:         172.20.0.10
Address:        172.20.0.10#53

Name:   aat-service.aat.svc.cluster.local
Address: 172.20.207.108

--------------------------------------------------------------------
[sachin@awsstgdevops001 tmp]$ kubectl get pods -n aat
NAME                                            READY   STATUS    RESTARTS   AGE
aat-api-deployment-88bb8b9ff-9wn9w              1/1     Running   0          38h
aat-reverse-proxy-deployment-5c4d596d6b-lcfwx   1/1     Running   0          2d9h
aat-ui-deployment-76b9fd8d46-lx5w9              1/1     Running   0          2d9h
dnsutils                                        1/1     Running   0          25m
[sachin@awsstgdevops001 tmp]$ kubectl get svc -n aat
NAME                    TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)               AGE
aat-reverse-proxy-svc   NodePort    172.20.141.40    <none>        14500:32125/TCP       13d
aat-service             ClusterIP   172.20.207.108   <none>        14550/TCP,14540/TCP   13d



[sachin@awsstgdevops001 tmp]$ kubectl exec -it aat-reverse-proxy-deployment-5c4d596d6b-lcfwx sh -n aat
/ $
/ $ netstat -ntulp
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 0.0.0.0:14500           0.0.0.0:*               LISTEN      1/nginx: master pro


[sachin@awsstgdevops001 tmp]$ kubectl exec -it aat-api-deployment-88bb8b9ff-9wn9w sh -n aat
/opt/aatadm $
/opt/aatadm $ netstat -ntulp
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 0.0.0.0:14550           0.0.0.0:*               LISTEN      6/java


{"httpStatusCode":"500 INTERNAL_SERVER_ERROR","apiErrorCode":1013,"userMessage":"Error occurred while processing request. Internal Server Error","debugMessage":"I/O error on POST request for \"https://aat-service.aat.int.cloud.sap:14500/api/app\": Operation timed out (Connection timed out); nested exception is java.net.ConnectException: Operation timed out (Connection timed out)"}


--------------------------------------------------------------------



- Setup ContainerInsights on kubernetes server.

curl https://raw.githubusercontent.com/aws-samples/amazon-cloudwatch-container-insights/latest/k8s-deployment-manifest-templates/deployment-mode/daemonset/container-insights-monitoring/quickstart/cwagent-fluentd-quickstart.yaml | sed "s/{{cluster_name}}/eks-dev-cluster/;s/{{region_name}}/us-east-1/" | kubectl apply -f -


[sachin@ip-198-19-11-212 ~]$ curl https://raw.githubusercontent.com/aws-samples/amazon-cloudwatch-container-insights/latest/k8s-deployment-manifest-templates/deployment-mode/daemonset/container-insights-monitoring/quickstart/cwagent-fluentd-quickstart.yaml | sed "s/{{cluster_name}}/eks-dev-cluster/;s/{{region_name}}/us-east-1/" | kubectl apply -f -
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 15552  100 15552    0     0  46566      0 --:--:-- --:--:-- --:--:-- 46702
namespace/amazon-cloudwatch unchanged
serviceaccount/cloudwatch-agent created
clusterrole.rbac.authorization.k8s.io/cloudwatch-agent-role created
clusterrolebinding.rbac.authorization.k8s.io/cloudwatch-agent-role-binding created
configmap/cwagentconfig created
daemonset.apps/cloudwatch-agent created
Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply
configmap/cluster-info configured
serviceaccount/fluentd unchanged
clusterrole.rbac.authorization.k8s.io/fluentd-role unchanged
clusterrolebinding.rbac.authorization.k8s.io/fluentd-role-binding unchanged
configmap/fluentd-config unchanged
daemonset.apps/fluentd-cloudwatch configured


- Done for staging.

[sachin@awsstgdevops001 ~]$ kubectl config current-context
arn:aws:eks:us-east-1:079549895456:cluster/awsstg-eks-encrypted
[sachin@awsstgdevops001 ~]$
[sachin@awsstgdevops001 ~]$
[sachin@awsstgdevops001 ~]$ curl https://raw.githubusercontent.com/aws-samples/amazon-cloudwatch-container-insights/latest/k8s-deployment-manifest-templates/deployment-mode/daemonset/container-insights-monitoring/quickstart/cwagent-fluentd-quickstart.yaml | sed "s/{{cluster_name}}/awsstg-eks-encrypted/;s/{{region_name}}/us-east-1/" | kubectl apply -f -
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 15552  100 15552    0     0   4882      0  0:00:03  0:00:03 --:--:--  4882
namespace/amazon-cloudwatch created
serviceaccount/cloudwatch-agent created
clusterrole.rbac.authorization.k8s.io/cloudwatch-agent-role created
clusterrolebinding.rbac.authorization.k8s.io/cloudwatch-agent-role-binding created
configmap/cwagentconfig created
daemonset.apps/cloudwatch-agent created
configmap/cluster-info created
serviceaccount/fluentd created
clusterrole.rbac.authorization.k8s.io/fluentd-role created
clusterrolebinding.rbac.authorization.k8s.io/fluentd-role-binding created
configmap/fluentd-config created
daemonset.apps/fluentd-cloudwatch created
[sachin@awsstgdevops001 ~]$
[sachin@awsstgdevops001 ~]$
[sachin@awsstgdevops001 ~]$
[sachin@awsstgdevops001 ~]$ kubectl get all -n amazon-cloudwatch
NAME                           READY   STATUS            RESTARTS   AGE
pod/cloudwatch-agent-blbts     1/1     Running           0          90s
pod/cloudwatch-agent-c6sfx     1/1     Running           0          90s
pod/cloudwatch-agent-hd9sb     1/1     Running           0          90s
pod/cloudwatch-agent-wxpqc     1/1     Running           0          90s
pod/fluentd-cloudwatch-6v8zc   0/1     PodInitializing   0          90s
pod/fluentd-cloudwatch-7f7gr   0/1     PodInitializing   0          90s
pod/fluentd-cloudwatch-rftgt   0/1     Init:1/2          0          90s
pod/fluentd-cloudwatch-zmzhf   0/1     PodInitializing   0          90s

NAME                                DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
daemonset.apps/cloudwatch-agent     4         4         4       4            4           <none>          90s
daemonset.apps/fluentd-cloudwatch   4         4         0       4            0           <none>          90s



------------------------------------------------------

Hey,

I just performed some of the troubleshooting as below

1) There are no relevant logs generated either at the aat-service or at aat-reverse-proxy side.

2) Further I checked the aat-service.aat.int.cloud.sap is resolving properly on the dns.

```kubectl exec -it dnsutils sh -n aat
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl kubectl exec [POD] -- [COMMAND] instead.
/ # nslookup aat-service.aat.int.cloud.sap
Server:         172.20.0.10
Address:        172.20.0.10#53

Name:   aat-service.aat.svc.cluster.local
Address: 172.20.207.108                 <<<<<<< This is resolving
```

3) After seeing the error I could see that we are getting this error while reaching out on port 14500 for aat-service.aat.int.cloud.sap.

{"httpStatusCode":"500 INTERNAL_SERVER_ERROR","apiErrorCode":1013,"userMessage":"Error occurred while processing request. Internal Server Error","debugMessage":"I/O error on POST request for \"https://aat-service.aat.int.cloud.sap:14500/api/app\": Operation timed out (Connection timed out); nested exception is java.net.ConnectException: Operation timed out (Connection timed out)"}

Where as this service is running at port 14550.

[sachin@awsstgdevops001 tmp]$ kubectl get svc -n aat
NAME                    TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)               AGE
aat-reverse-proxy-svc   NodePort    172.20.141.40    <none>        14500:32125/TCP       13d
aat-service             ClusterIP   172.20.207.108   <none>        14550/TCP,14540/TCP   13d                 <<<<<<< Running at 14550


Same I confirmed from the pod as well

[sachin@awsstgdevops001 tmp]$ kubectl exec -it aat-api-deployment-88bb8b9ff-9wn9w sh -n aat
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl kubectl exec [POD] -- [COMMAND] instead.
/opt/aatadm $
/opt/aatadm $ netstat -ntulp
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 0.0.0.0:14550           0.0.0.0:*               LISTEN      6/java                 <<<<<<< Running at 14550

4) Further I checked the llk-account-service configmap there is no option where I can set the call https://aat-service.aat.int.cloud.sap:14500/api/app to https://aat-service.aat.int.cloud.sap:14550/api/app


[sachin@awsstgdevops001 tmp]$ kubectl get cm llk-accounts-service-cm -n llk -o yaml | grep -i aat-service
  AAT_METADATA_URL: https://aat-service.aat:14500/auth/metadata
  AAT_TOKEN_GENERATE_URL: https://aat-service.aat:14500/oauth/token
  AAT_TOKEN_INVALIDATE_URL: https://aat-service.aat:14500/oauth/token/invalidate
  AAT_TOKEN_VALIDATE_URL: https://aat-service.aat:14500/oauth/token/validate

5) Also if I run the aat-service at 14500 then it will interrupt with the aat-reverse proxy service which is already running on port 14500.

[sachin@awsstgdevops001 tmp]$ kubectl get svc -n aat
NAME                    TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)               AGE
aat-reverse-proxy-svc   NodePort    172.20.141.40    <none>        14500:32125/TCP       13d
aat-service             ClusterIP   172.20.207.108   <none>        14550/TCP,14540/TCP   13d

6) So the conclusion is the service aat-service.aat.int.cloud.sap has to be made available/reachable on proper port by making appropriate changes in the llk-account-service service.


-------------- Had a call with Armory kubernetes team



------------------------------------------ 15June2020 ------------------------------------------

- Resolved Nitasha issue
- Resolved bamboo issue by restarting the container

- deployment of https://bamboo.di-infra.sap.corp/deploy/viewDeploymentResult.action?deploymentResultId=160956420 is done.

- provided access to ec2-user for awsstg environment 

[awsstg]
aws_access_key_id = AKIARFBMLCMQC22ZHUHT
aws_secret_access_key = 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

- HI @Sachin Kesarkar  cannot view the deployment screen on mmg-development and mmg-quality branch for admin portal
https://bamboo.di-infra.sap.corp/browse/MMG-MMGAD0

- Pankaj DBO query permissions

- Worked with Rajul

- Attended Team call

------------------------------------- 16June2020 ------------------------------------------------------


------------------------------------- 17June2020 ------------------------------------------------------
Donwntime startred at 8:51 PM CST
Issue reported at 2:45 AM CST
Issue resolved at 3:30 AM CST
Issue resolution confirmed to Account manager over the chat 3:30 AM CST
Issue resolution confirmed to Account Manager over the mail after testing application at 3:45 AM CST

Reason of Downtime :
- Application was not able to connect to DB giving "request timeout after 30000ms"
- The log segment of the rms_db database was full.

Resolution :
- The log segment was cleared by DBA team for the rms_db.


------------------

- Cleared SuhWeh doubts

 curl -X POST https://login.sapdigitalinterconnect.com/oauth/token -H 'Authorization: Basic Q1Fac2liUXR6M1Zyb0dNYTpSSldFdWN5RHNRSVBOYUgxRVpUVXFOc1VkNjAySU1FUQ==' -d 'grant_type=client_credentials&token_type=jwt&scope=lle%3Aapi&undefined='
{"access_token":"eyJhbGciOiJSUzI1NiJ9.eyJjbGFpbWFudCI6eyJhcHBDb2RlIjoiQ1Fac2liUXR6M1Zyb0dNYSIsImFwcE5hbWUiOiJGZWRFeCBkZW1vIExMSyAyIiwiYXBwVG9rZW5EdXJhdGlvbiI6NjAsInJlZnJlc2hQZXJpb2QiOjE1LCJ0b2tlbkR1cmF0aW9uSW5NaWxsaXNlY29uZHMiOjM2MDAwMDAsInRva2VuUmVmcmVzaFBlcmlvZEluTWlsbGlzZWNvbmRzIjo5MDAwMDAsInJvbGVDb2RlcyI6WyJMSVZFTElOS19BUFAiXSwic2NvcGUiOiJsbGU6YXBpIn0sImlzcyI6IlNDSV9BVVRIRU5USUNBVElPTl9BVVRIT1JJVFkiLCJqdGkiOiIzWUZoeVVPM2NxdDV1bjQzV3pMd2U4WXZhdU13TmhxRCIsImV4cCI6MTU5MjM5NzQ3NH0.s6sf7LAHQajw5zFh3NyxgfGYn9wtUlfB8utwKgogucb_03GjRDZ2zfJbelzaV9aJl9dlOjMK4BWQ6H-PKSbVbaaRuizHRui5tVVW8Ar4pswUe4KZ_gMfv00EbWM5YO0-uBD6JN0-xuk56oTxadhAuGgVYYrnglqVlcQ4Q6r0m96LRFchmDYr8LLj9cNj_56dvU8Ddz_WOMQW6fIHdJkPI-8x4zIrSQ6UvkVsAS5tsJS_CQZdl0-p8_JzibLoTNGPjDLA1br6Cx3LKMge2oSoQu-r1Ce6EUdnNuYQVBKyxPnP14enHrdjzLUMJTRVtIr_MMTdYWVCQRLVflggirrGXQ","token_type":"Bearer","expires_in":3600000,"refresh_token":"C2UkTWLa4ltcUg2l","scope":"lle:api","refresh_expires_in":5400000}

curl -k -X GET https://recipientmanagement.sapdigitalinterconnect.com/api/broadcast-channels/5 -H 'Authorization: Bearer eyJhbGciOiJSUzI1NiJ9.eyJjbGFpbWFudCI6eyJhcHBDb2RlIjoiQ1Fac2liUXR6M1Zyb0dNYSIsImFwcE5hbWUiOiJGZWRFeCBkZW1vIExMSyAyIiwiYXBwVG9rZW5EdXJhdGlvbiI6NjAsInJlZnJlc2hQZXJpb2QiOjE1LCJ0b2tlbkR1cmF0aW9uSW5NaWxsaXNlY29uZHMiOjM2MDAwMDAsInRva2VuUmVmcmVzaFBlcmlvZEluTWlsbGlzZWNvbmRzIjo5MDAwMDAsInJvbGVDb2RlcyI6WyJMSVZFTElOS19BUFAiXSwic2NvcGUiOiJsbGU6YXBpIn0sImlzcyI6IlNDSV9BVVRIRU5USUNBVElPTl9BVVRIT1JJVFkiLCJqdGkiOiIzWUZoeVVPM2NxdDV1bjQzV3pMd2U4WXZhdU13TmhxRCIsImV4cCI6MTU5MjM5NzQ3NH0.s6sf7LAHQajw5zFh3NyxgfGYn9wtUlfB8utwKgogucb_03GjRDZ2zfJbelzaV9aJl9dlOjMK4BWQ6H-PKSbVbaaRuizHRui5tVVW8Ar4pswUe4KZ_gMfv00EbWM5YO0-uBD6JN0-xuk56oTxadhAuGgVYYrnglqVlcQ4Q6r0m96LRFchmDYr8LLj9cNj_56dvU8Ddz_WOMQW6fIHdJkPI-8x4zIrSQ6UvkVsAS5tsJS_CQZdl0-p8_JzibLoTNGPjDLA1br6Cx3LKMge2oSoQu-r1Ce6EUdnNuYQVBKyxPnP14enHrdjzLUMJTRVtIr_MMTdYWVCQRLVflggirrGXQ' -H 'Content-Type: application/json' -H 'System: lle:api'
{"createdBy":"nQLT6U9LBk3miWUQ","createdDate":"2020-04-24T20:00:19.870Z","lastModifiedBy":"r88XVJxDWcyQGZrq","lastModifiedDate":"2020-06-01T16:59:52.833333333Z","id":5,"name":"Canadian Follow Mess","description":"Canadian Follow Mess","restricted":true,"llkAccount":272,"appKey":"a8FDH9LSMYWTxRuN","appSecret":"G6C0Tg4INVqVBpSIl0uPWMl7qbNNetjU","hubAccount":35157,"timeRestricted":true,"timeZoneId":"US/Eastern","messagesPerHour":null,"messagesToForward":"ALLBAROPTOUTS","messageExpiration":25,"enabled":true,"safeSendConfig":{"createdBy":"nQLT6U9LBk3miWUQ","createdDate":"2020-04-24T20:00:19.860Z","lastModifiedBy":"nQLT6U9LBk3miWUQ","lastModifiedDate":"2020-04-24T20:00:19.860Z","id":14,"suStartTime":"08:00:00","suEndTime":"23:00:00","moStartTime":"08:00:00","moEndTime":"23:00:00","tuStartTime":"08:00:00","tuEndTime":"23:00:00","weStartTime":"08:00:00","weEndTime":"23:00:00","thStartTime":"08:00:00","thEndTime":"23:00:00","frStartTime":"08:00:00","frEndTime":"23:00:00","saStartTime":"08:00:00","saEndTime":"23:00:00"},"originators":[{"createdBy":"nQLT6U9LBk3miWUQ","createdDate":"2020-04-24T20:00:19.886666666Z","lastModifiedBy":"nQLT6U9LBk3miWUQ","lastModifiedDate":"2020-04-24T20:00:19.886666666Z","id":4,"code":"38773","country":null,"type":null,"description":"Canadian Shortcode","optInDisabled":false,"optOutDisabled":false,"blacklistId":5,"defaultLocale":"CA","defaultMessages":null}],"company":{"createdBy":"nQLT6U9LBk3miWUQ","createdDate":"2020-04-24T20:00:19.746666666Z","lastModifiedBy":"nQLT6U9LBk3miWUQ","lastModifiedDate":"2020-04-24T20:00:19.746666666Z","id":1,"name":"FEDEX","description":"Main company used by FedEx testing","llkMainAccount":109,"inboundBlacklistId":null,"outboundBlacklistId":null,"originators":null}}


- 1 hour meeting with Barani

- 1 hour meeting with Khalid

- 1 hour meeting with Armory



------------------------------------- 18June2020 ------------------------------------------------------

- Configure Application LB for MMG AWSSTG

- Raise JIRA for, TCP protocols

Allow applications in AWSSTG to access FR1 hosts:
 
==========

Request: Allow applications in AWSSTG to access FR1 hosts:

-------------
 
SwiftMQ directory services and routers:
•	fr1directory001.bd.trust (10.150.7.37) port 4000
•	fr1directory002.bd.trust (10.150.8.37) port 4000
•	fr1mtinqueue001.bd.trust (10.150.7.49) port 4001 / 4002 
•	fr1mtinqueue002.bd.trust (10.150.8.49) port 4001 / 4002

- Allow communication between awsstg-app subnet and fr1directory001~002.bd.trust on port 4000

*VPC*: AWSSTG
*Source IP(s)*: 10.2.7.0/24, 10.2.8.0/24
*Destination IP(s)*: 10.150.7.37, 10.150.8.37
*Protocol (TCP, UDP, or ICMP)*: TCP
*Port*: 4000
*Description*: Communication between awsstg-app subnet (SwiftMQ routes) and fr1directory001~002.bd.trust


- Allow communication between awsstg-app subnet and fr1mtinqueue001~002.bd.trust on port 4001,4002
*VPC*: AWSSTG
*Source IP(s)*: 10.2.7.0/24, 10.2.8.0/24
*Destination IP(s)*: 10.150.7.49, 10.150.8.49
*Protocol (TCP, UDP, or ICMP)*: TCP
*Port*: 4001,4002
*Description*: Communication between awsstg-app subnet (SwiftMQ routes) and fr1mtinqueue001~002.bd.trust
-------------
 
SwiftMQ routers and application web services:
•	fr1jhttppush001.bd.trust (10.150.1.29) port 4001 / 4002 / 5001 / 5002 / 8000 / 8001
•	fr1regpush001.bd.trust (10.150.1.30) port 7000 / 8186

- Allow communication between awsstg-app subnet and fr1jhttppush001.bd.trust
*VPC*: AWSSTG
*Source IP(s)*: 10.2.7.0/24, 10.2.8.0/24
*Destination IP(s)*: 10.150.1.29
*Protocol (TCP, UDP, or ICMP)*: TCP
*Port*: 4001,4002,5001,5002,8000,8001
*Description*: Communication between awsstg-app subnet (SwiftMQ routes) and fr1jhttppush001.bd.trust


- Allow communication between awsstg-app subnet and fr1regpush001.bd.trust
*VPC*: AWSSTG
*Source IP(s)*: 10.2.7.0/24, 10.2.8.0/24
*Destination IP(s)*: 10.150.1.30
*Protocol (TCP, UDP, or ICMP)*: TCP
*Port*: 7000,8186
*Description*: Communication between awsstg-app subnet (SwiftMQ routes) and fr1regpush001.bd.trust

-------------
 
Applications
•	fr1orderid001.bd.trust (10.150.7.14), port 3458
•	fr1orderid002.bd.trust (10.150.8.14) port 3458

- Allow communication between awsstg-app subnet and fr1orderid001~002.bd.trust
*VPC*: AWSSTG
*Source IP(s)*: 10.2.7.0/24, 10.2.8.0/24
*Destination IP(s)*: 10.150.7.14, 10.150.8.14
*Protocol (TCP, UDP, or ICMP)*: TCP
*Port*: 3458
*Description*: Communication between awsstg-app subnet (SwiftMQ routes) and fr1orderid001~002.bd.trust
 
============

awsstg-app-1	10.2.7.0/24
awsstg-app-2	10.2.8.0/24

------------------------------------- 19June2020 ------------------------------------------------------

66643619

22214539

44429078

llk-sms-channel

31903, 55703

c5246840@llbpal55:~> ps aux | grep -i 55073
c5246840 33425  0.0  0.0  13328  2256 pts/11   S+   05:46   0:00 grep --color=auto -i 55073
root     55073  0.2 46.1 16871960 15111808 ?   Ssl  Jun05  48:17 redis-server-4.0.8 0.0.0.0:6381
c5246840@llbpal55:~>
c5246840@llbpal55:~> ps aux | grep -i 31903
root     31903 46.3 47.8 16872104 15662448 ?   D    05:44   1:12 redis-rdb-bgsave 0.0.0.0:6381
c5246840 33621  0.0  0.0  13332  2200 pts/11   S+   05:47   0:00 grep --color=auto -i 31903





------------------------------------- 22June2020 ------------------------------------------------------

- Can you please deploy below builds for LCM-4015 branch on aws staging
https://bamboo.di-infra.sap.corp/browse/LIV-LLKAC11
https://bamboo.di-infra.sap.corp/browse/LIV-LSC24
https://bamboo.di-infra.sap.corp/browse/LIV-LLKSMSEV6
https://bamboo.di-infra.sap.corp/browse/LIV-LLKSMSCHAN6

- Restarted the Bamboo server as it was not working.

- 1.5 hours : did a presentation with MMG operations team.



------------------------------------- 23June2020 ------------------------------------------------------

- Checking RMS status.

{"rmsOrderId":522155206,"content":{"destination":"19167526654","message":"FedEx: fedex.com/t/188573357372/en_US Unable to deliver, Track for details. Reply HELP for help. STOP to cancel.","originator":null,"broadcastChannelId":3,"profileId":null,"dateToSend":null,"deliveryStatusAddress":{"address":"https://ws.fedex.com:443/web-services/sms","username":"HAu74Ugn9U5fN9qx","password":"NdDIyRBDrXAnfrr33eo6FATQh"},"replyMessageId":null,"subscriptionId":"19167526654:188573357372","additionalData":"{\"MessageKey\":\"SMS|2020-06-23T02:59:55.323Z\",\"Attributes\":{\"notifCd\":\"1022\",\"trkNo\":\"188573357372\",\"clt\":\"INSIGHT\",\"recipient\":\"19167526654\",\"locale\":\"en_US\"}}","langLocale":"en_US"}}

{"rmsOrderId":720558163,"content":{"destination":"13613185030","message":"FedEx: fedex.com/t/393979245325/en_US Scheduled for delivery tomorrow 06/23 by 4:30P. Take control and manage your delivery. Reply ENROLL to learn more. Reply HELP for help. STOP to cancel.","originator":null,"broadcastChannelId":3,"profileId":null,"dateToSend":null,"deliveryStatusAddress":{"address":"https://ws.fedex.com:443/web-services/sms","username":"HAu74Ugn9U5fN9qx","password":"NdDIyRBDrXAnfrr33eo6FATQh"},"replyMessageId":null,"subscriptionId":"13613185030:393979245325","additionalData":"{\"MessageKey\":\"SMS|2020-06-23T02:59:58.887Z\",\"Attributes\":{\"notifCd\":\"1020\",\"trkNo\":\"393979245325\",\"clt\":\"INSIGHT\",\"recipient\":\"13613185030\",\"locale\":\"en_US\"}}","langLocale":"en_US"}}


{"rmsOrderId":739105683,"content":{"destination":"18609309273","message":"FedEx: fedex.com/t/394046541821/en_US Scheduled for delivery tomorrow 06/23. Take control and manage your delivery. Reply ENROLL to learn more. Reply HELP for help. STOP to cancel.","originator":null,"broadcastChannelId":3,"profileId":null,"dateToSend":null,"deliveryStatusAddress":{"address":"https://ws.fedex.com:443/web-services/sms","username":"HAu74Ugn9U5fN9qx","password":"NdDIyRBDrXAnfrr33eo6FATQh"},"replyMessageId":null,"subscriptionId":"18609309273:394046541821","additionalData":"{\"MessageKey\":\"SMS|2020-06-23T02:59:54.391Z\",\"Attributes\":{\"notifCd\":\"1020\",\"trkNo\":\"394046541821\",\"clt\":\"INSIGHT\",\"recipient\":\"18609309273\",\"locale\":\"en_US\"}}","langLocale":"en_US"}}


{"rmsOrderId":279395823,"content":{"destination":"13475436699","message":"FedEx: fedex.com/t/191382156593/en_US Picked up, scheduled for delivery 06/24. Take control and manage your delivery. Reply ENROLL to learn more. Reply HELP for help. STOP to cancel.","originator":null,"broadcastChannelId":3,"profileId":null,"dateToSend":null,"deliveryStatusAddress":{"address":"https://ws.fedex.com:443/web-services/sms","username":"HAu74Ugn9U5fN9qx","password":"NdDIyRBDrXAnfrr33eo6FATQh"},"replyMessageId":null,"subscriptionId":"13475436699:191382156593","additionalData":"{\"MessageKey\":\"SMS|2020-06-23T03:00:01.052Z\",\"Attributes\":{\"notifCd\":\"1029\",\"trkNo\":\"191382156593\",\"clt\":\"INSIGHT\",\"recipient\":\"13475436699\",\"locale\":\"en_US\"}}","langLocale":"en_US"}}


{"rmsOrderId":702349681,"content":{"destination":"13187945626","message":"FedEx: fedex.com/t/74890981775722167018/en_US In transit. Estimated delivery to ALEXANDRIA, LA US not available. STOP to cancel.","originator":null,"broadcastChannelId":3,"profileId":null,"dateToSend":null,"deliveryStatusAddress":{"address":"https://ws.fedex.com:443/web-services/sms","username":"HAu74Ugn9U5fN9qx","password":"NdDIyRBDrXAnfrr33eo6FATQh"},"replyMessageId":null,"subscriptionId":"13187945626:74890981775722167018","additionalData":"{\"MessageKey\":\"SMS|2020-06-22T22:00:03.706-05:00\",\"Attributes\":{\"notifCd\":\"3007\",\"trkNo\":\"74890981775722167018\",\"clt\":\"TRKC\",\"recipient\":\"13187945626\",\"locale\":\"en_US\"}}","langLocale":"en_US"}}


curl -k -X GET https://recipientmanagement.sapdigitalinterconnect.com/api/broadcast-channels/3 -H 'Authorization: Bearer eyJhbGciOiJSUzI1NiJ9.eyJjbGFpbWFudCI6eyJhcHBDb2RlIjoiQ1Fac2liUXR6M1Zyb0dNYSIsImFwcE5hbWUiOiJGZWRFeCBkZW1vIExMSyAyIiwiYXBwVG9rZW5EdXJhdGlvbiI6NjAsInJlZnJlc2hQZXJpb2QiOjE1LCJ0b2tlbkR1cmF0aW9uSW5NaWxsaXNlY29uZHMiOjM2MDAwMDAsInRva2VuUmVmcmVzaFBlcmlvZEluTWlsbGlzZWNvbmRzIjo5MDAwMDAsInJvbGVDb2RlcyI6WyJMSVZFTElOS19BUFAiXSwic2NvcGUiOiJsbGU6YXBpIn0sImlzcyI6IlNDSV9BVVRIRU5USUNBVElPTl9BVVRIT1JJVFkiLCJqdGkiOiJ1aTIxajJoUXNxMjBuZlJuUDFnMlZtbXBDTFpOM2dkOCIsImV4cCI6MTU5Mjg5MzY3M30.L_gGRVDE-f52bJldRYLxnoaH07lW7uR0XVNFgbm_04RC9CggO_nHCrTm_M_UT3hieHQRnM-wAP44njFvhDtYMwJIMJx_SVKZLztK6gDXKopBqvHOAr12C3nhUpP5khOodRIbeGUM9kigMhE8OuZvu2Zt_n5MNJI0mVYxHdy6Q4zJ7YjcjIBFSpzEOycWaF-D5ZDJbbEfCYWFu8ST0CUO0U66K0KfxBqS74kW5WTtq2OlEiO3V0QhgAGhfRle7ThYxFlLqbNhid6T-CpHJ8gzhMJKwIUredaCI4xvwrn9rZXsSVZ3z02PUMsp4XzRWjRWF91XNomjEkJHpDg2yVxR8g' -H 'Content-Type: application/json' -H 'System: lle:api'


[skesarkar@iad1bastion01 ~]$ curl -k -X GET https://recipientmanagement.sapdigitalinterconnect.com/api/broadcast-channels/3 -H 'Authorization: Bearer eyJhbGciOiJSUzI1NiJ9.eyJjbGFpbWFudCI6eyJhcHBDb2RlIjoiQ1Fac2liUXR6M1Zyb0dNYSIsImFwcE5hbWUiOiJGZWRFeCBkZW1vIExMSyAyIiwiYXBwVG9rZW5EdXJhdGlvbiI6NjAsInJlZnJlc2hQZXJpb2QiOjE1LCJ0b2tlbkR1cmF0aW9uSW5NaWxsaXNlY29uZHMiOjM2MDAwMDAsInRva2VuUmVmcmVzaFBlcmlvZEluTWlsbGlzZWNvbmRzIjo5MDAwMDAsInJvbGVDb2RlcyI6WyJMSVZFTElOS19BUFAiXSwic2NvcGUiOiJsbGU6YXBpIn0sImlzcyI6IlNDSV9BVVRIRU5USUNBVElPTl9BVVRIT1JJVFkiLCJqdGkiOiJ1aTIxajJoUXNxMjBuZlJuUDFnMlZtbXBDTFpOM2dkOCIsImV4cCI6MTU5Mjg5MzY3M30.L_gGRVDE-f52bJldRYLxnoaH07lW7uR0XVNFgbm_04RC9CggO_nHCrTm_M_UT3hieHQRnM-wAP44njFvhDtYMwJIMJx_SVKZLztK6gDXKopBqvHOAr12C3nhUpP5khOodRIbeGUM9kigMhE8OuZvu2Zt_n5MNJI0mVYxHdy6Q4zJ7YjcjIBFSpzEOycWaF-D5ZDJbbEfCYWFu8ST0CUO0U66K0KfxBqS74kW5WTtq2OlEiO3V0QhgAGhfRle7ThYxFlLqbNhid6T-CpHJ8gzhMJKwIUredaCI4xvwrn9rZXsSVZ3z02PUMsp4XzRWjRWF91XNomjEkJHpDg2yVxR8g' -H 'Content-Type: application/json' -H 'System: lle:api'

{"createdBy":"nQLT6U9LBk3miWUQ","createdDate":"2020-04-24T20:00:19.870Z","lastModifiedBy":"r88XVJxDWcyQGZrq","lastModifiedDate":"2020-06-16T17:10:35.576666666Z","id":3,"name":"US Follow Messages","description":"US Follow Messages","restricted":true,"llkAccount":272,"appKey":"a8FDH9LSMYWTxRuN","appSecret":"G6C0Tg4INVqVBpSIl0uPWMl7qbNNetjU","hubAccount":35157,"timeRestricted":true,"timeZoneId":"US/Eastern","messagesPerHour":null,"messagesToForward":"ALLBAROPTOUTS","messageExpiration":25,"enabled":true,"safeSendConfig":{"createdBy":"nQLT6U9LBk3miWUQ","createdDate":"2020-04-24T20:00:19.860Z","lastModifiedBy":"nQLT6U9LBk3miWUQ","lastModifiedDate":"2020-04-24T20:00:19.860Z","id":10,"suStartTime":"08:00:00","suEndTime":"23:00:00","moStartTime":"08:00:00","moEndTime":"23:00:00","tuStartTime":"08:00:00","tuEndTime":"23:00:00","weStartTime":"08:00:00","weEndTime":"23:00:00","thStartTime":"08:00:00","thEndTime":"23:00:00","frStartTime":"08:00:00","frEndTime":"23:00:00","saStartTime":"08:00:00","saEndTime":"23:00:00"},"originators":[{"createdBy":"nQLT6U9LBk3miWUQ","createdDate":"2020-04-24T20:00:19.886666666Z","lastModifiedBy":"nQLT6U9LBk3miWUQ","lastModifiedDate":"2020-04-24T20:00:19.886666666Z","id":3,"code":"48773","country":null,"type":null,"description":"US Shortcode","optInDisabled":false,"optOutDisabled":false,"blacklistId":4,"defaultLocale":"US","defaultMessages":null}],"company":{"createdBy":"nQLT6U9LBk3miWUQ","createdDate":"2020-04-24T20:00:19.746666666Z","lastModifiedBy":"nQLT6U9LBk3miWUQ","lastModifiedDate":"2020-04-24T20:00:19.746666666Z","id":1,"name":"FEDEX","description":"Main company used by FedEx testing","llkMainAccount":109,"inboundBlacklistId":null,"outboundBlacklistId":null,"originators":null}}[skesarkar@iad1bastion01 ~]$
[skesarkar@iad1bastion01 ~]$
[skesarkar@iad1bastion01 ~]$ date
Tue Jun 23 05:32:57 GMT 2020

"timeZoneId":"US/Eastern"
"weStartTime":"08:00:00"

- Attended meeting with Khalid.




----------------------------------- 24June2020 -------------------------------------------

[skesarkar@us4cipapi01 ~]$ docker exec -it 5f47c73a2f98 sh
/ $ env | grep -i ase
ARC_REDIS_DATABASE=0
ARC_LIQUIBASE_PASSWORD=Sgv04tG1
ARC_LIQUIBASE_USER=arc_admin_db_user
ARC_LIQUIBASE_URL=jdbc:sybase:Tds:us4cipapi01.bd.trust:4000/arc_db
ARC_DB_URL=jdbc:sybase:Tds:us4cidb05.bd.trust:5000/arc_db
ARC_LIQUIBASE_ENABLED=true

